{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.kl import KLSummarizer\n",
    "from sumy.summarizers.reduction import ReductionSummarizer\n",
    "from sumy.summarizers.edmundson import EdmundsonSummarizer\n",
    "from sumy.summarizers.random import RandomSummarizer\n",
    "from sumy.summarizers.sum_basic import SumBasicSummarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "import os\n",
    "import math\n",
    "import sentencepiece\n",
    "import evaluate\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textwrap\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import rouge\n",
    "from IPython.display import display, HTML\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "import sumy\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "import re\n",
    "import difflib\n",
    "import contractions  \n",
    "import gensim\n",
    "import pickle\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from requests_html import AsyncHTMLSession\n",
    "asession = AsyncHTMLSession()\n",
    "rouge = evaluate.load('rouge')\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "from vizualization import utils_split_sentences, display_string_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/Viktor_Shevchuk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Viktor_Shevchuk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/Viktor_Shevchuk/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/Viktor_Shevchuk/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_auto_generated_summaries(model_name, summary_list):\n",
    "    folder_name = model_name + \"_summarized\"\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "\n",
    "    for index, auto_generated_summary in enumerate(summary_list):\n",
    "        file_path = os.path.join(folder_name, str(index) + \".txt\")\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(auto_generated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summary(reference, predicted):\n",
    "    predictions = [predicted]\n",
    "    references = [reference]\n",
    "    results = rouge.compute(predictions=predictions, references=references, rouge_types=['rouge1', 'rouge2', 'rougeL'])\n",
    "    score_1 = round(results['rouge1'], 2)\n",
    "    score_2 = round(results['rouge2'], 2)\n",
    "    score_L = round(results['rougeL'], 2)\n",
    "\n",
    "    avg_rouge = round(np.mean([score_1, score_2, score_L]), 2)\n",
    "    print(\"rouge1:\", score_1, \"| rouge2:\", score_2, \"| rougeL:\", score_L, \"--> avg rouge:\", avg_rouge)\n",
    "\n",
    "    return { \"score_1\": score_1, \"score_2\": score_2, \"score_L\": score_L, \"avg_rouge\": avg_rouge }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL1 = \"https://zakon.rada.gov.ua/laws/show/en/1207-18/conv/print\"\n",
    "URL2 = \"https://zakon.rada.gov.ua/laws/show/en/1700-18/print\"\n",
    "URL3 = \"https://zakon.rada.gov.ua/laws/show/en/254%D0%BA/96-%D0%B2%D1%80/print\"\n",
    "page = requests.get(URL1)\n",
    "\n",
    "# print(page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = soup.select('.rvts0 > .rvps2')\n",
    "paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_stop_words = [\n",
    "    \"amended\", \"restated\", \"be stated\", \"supplemented\", \"section\", \"paragraph\", \"article\", \"as follows\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_sentence_with_stopWord(para):\n",
    "    for word in legal_stop_words:\n",
    "        sentence_with_stopWord = para.find(\n",
    "            text=lambda t: word in t.text.lower()\n",
    "        )\n",
    "\n",
    "        if sentence_with_stopWord:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_provisions(para):\n",
    "    pattern = r\"\\b(?=[MDCLXVIΙ])M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})([IΙ]X|[IΙ]V|V?[IΙ]{0,3})\\b\\.?\"\n",
    "    para_with_no_roman = re.sub(pattern, '', para.text.strip()).strip()\n",
    "\n",
    "    return para_with_no_roman == \"Final and transitional provisions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_article_ensuring(para):\n",
    "  return para.find(\"span\", class_=\"rvts37\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_article_title(para):\n",
    "    return para.find(\"span\", class_=\"rvts9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_closing_char(letter):\n",
    "  return letter == \".\" or letter == \")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discard_ordered_list_numbers(text):\n",
    "    is_first_char_numeric = text[0].isdigit()\n",
    "    is_second_char_numeric = text[1].isdigit()\n",
    "    is_third_char_numeric = text[2].isdigit()\n",
    "\n",
    "    if is_first_char_numeric and is_closing_char(text[1]):\n",
    "        return text[2:].strip()\n",
    "    elif is_first_char_numeric and is_second_char_numeric and is_closing_char(text[2]):\n",
    "        return text[3:].strip()\n",
    "    elif is_first_char_numeric and is_second_char_numeric and is_third_char_numeric and is_closing_char(text[3]):\n",
    "        return text[4:].strip()\n",
    "    elif re.search(r\"[a-z]\", text[0]) and is_closing_char(text[1]):\n",
    "        return text[2:].strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_sentence_end(sentence):\n",
    "    last_character = sentence[-1]\n",
    "    is_etc = sentence[-4:] == \"etc.\"\n",
    "\n",
    "    if last_character == \".\" and not is_etc:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(paragraphs):\n",
    "    cleaned = []\n",
    "    article_ensuring = False\n",
    "    concatenated_list = []\n",
    "    ordered_list_with_stop_word = False\n",
    "\n",
    "    for para in paragraphs:\n",
    "        if article_ensuring and not is_article_title(para):\n",
    "            continue\n",
    "        elif article_ensuring and is_article_title(para):\n",
    "            article_ensuring = False\n",
    "\n",
    "        if is_article_ensuring(para):\n",
    "            article_ensuring = True\n",
    "            continue\n",
    "\n",
    "        if is_provisions(para):\n",
    "            return cleaned\n",
    "\n",
    "        if is_article_title(para):\n",
    "            continue\n",
    "\n",
    "        text = para.text.strip()\n",
    "        last_character = text[-1]\n",
    "\n",
    "        if is_sentence_with_stopWord(para) and last_character == \":\":\n",
    "            ordered_list_with_stop_word = True\n",
    "        elif not is_sentence_with_stopWord(para) and ordered_list_with_stop_word and is_sentence_end(text):\n",
    "            ordered_list_with_stop_word = False\n",
    "        elif not is_sentence_with_stopWord(para) and not ordered_list_with_stop_word:\n",
    "            if last_character == \":\" and not len(concatenated_list):\n",
    "                concatenated_list.append(text)\n",
    "            elif len(concatenated_list) and not is_sentence_end(text):\n",
    "                concatenated_list.append(text)\n",
    "            elif is_sentence_end(text) and len(concatenated_list):\n",
    "                concatenated_list.append(text)\n",
    "                cleaned.append(\n",
    "                    discard_ordered_list_numbers(\" \".join(concatenated_list))\n",
    "                )\n",
    "                concatenated_list.clear()\n",
    "            elif not len(concatenated_list):\n",
    "                cleaned.append(discard_ordered_list_numbers(text))\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_paragraphs = sent_tokenize(' '.join(clean(paragraphs)))\n",
    "cleaned_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "type(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.add('shall')\n",
    "stop_words.add('.')\n",
    "stop_words.add(',')\n",
    "stop_words.add('(')\n",
    "stop_words.add(')')\n",
    "stop_words.add(';')\n",
    "stop_words.add(':')\n",
    "stop_words.add('—')\n",
    "stop_words.add('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = nltk.word_tokenize(\" \".join(cleaned_paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the words in word_tokens to lower case and then checks whether they are present in stop_words or not\n",
    "filtered_sentence = [w.lower()\n",
    "                     for w in word_tokens if not w.lower() in stop_words]\n",
    "# with lower case conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compute n-grams frequency with nltk tokenizer.\n",
    ":parameter\n",
    "    :param corpus: list - dtf[\"text\"]\n",
    "    :param ngrams: int or list - 1 for unigrams, 2 for bigrams, [1,2] for both\n",
    "    :param top: num - plot the top frequent words\n",
    ":return\n",
    "    dtf_count: dtf with word frequency\n",
    "'''\n",
    "\n",
    "\n",
    "def word_freq(corpus, ngrams=[1, 2, 3], top=10, figsize=(10, 7)):\n",
    "    lst_tokens = nltk.tokenize.word_tokenize(corpus)\n",
    "    ngrams = [ngrams] if type(ngrams) is int else ngrams\n",
    "\n",
    "    # calculate\n",
    "    dtf_freq = pd.DataFrame()\n",
    "    for n in ngrams:\n",
    "        dic_words_freq = nltk.FreqDist(nltk.ngrams(lst_tokens, n))\n",
    "        dtf_n = pd.DataFrame(dic_words_freq.most_common(),\n",
    "                             columns=[\"word\", \"freq\"])\n",
    "        dtf_n[\"ngrams\"] = n\n",
    "        dtf_freq = dtf_freq.append(dtf_n)\n",
    "    dtf_freq[\"word\"] = dtf_freq[\"word\"].apply(\n",
    "        lambda x: \" \".join(string for string in x))\n",
    "    dtf_freq = dtf_freq.sort_values(\n",
    "        [\"ngrams\", \"freq\"], ascending=[True, False])\n",
    "\n",
    "    # plot\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.barplot(x=\"freq\", y=\"word\", hue=\"ngrams\", dodge=False, ax=ax,\n",
    "                data=dtf_freq.groupby('ngrams')[\n",
    "                    \"ngrams\", \"freq\", \"word\"].head(top)\n",
    "                )\n",
    "    ax.set(xlabel=None, ylabel=None, title=\"Most frequent words\")\n",
    "    ax.grid(axis=\"x\")\n",
    "    plt.show()\n",
    "    return dtf_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most common words in text\n",
    "freq = word_freq(\n",
    "    corpus=\" \".join(filtered_sentence),\n",
    "    ngrams=[1],\n",
    "    top=45,\n",
    "    figsize=(10, 8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokened_sent = sent_tokenize(\" \".join(cleaned_paragraphs))\n",
    "\n",
    "main_dict = {}\n",
    "\n",
    "for item in tokened_sent:\n",
    "    item1 = list(item.split(\" \"))\n",
    "    item2 = [' '.join(item1)]\n",
    "    Length = []\n",
    "    Length.append(len(item1))\n",
    "    mydict = dict(zip(item2, Length))\n",
    "    main_dict.update(mydict)\n",
    "\n",
    "print('Maximum word amount in a sentence: ', max(main_dict.values()))\n",
    "print('Minimum word amount in a sentence: ', min(main_dict.values()))\n",
    "print('Average word amount in a sentence: ', mean(main_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = TfidfVectorizer(\n",
    "    stop_words=stopwords.words('english'), \n",
    "    # norm='l1'\n",
    ")\n",
    "# featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = featurizer.fit_transform(\n",
    "    cleaned_paragraphs\n",
    ")\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_score(tfidf_row):\n",
    "    # return the average of the non-zero values of the tf-idf vector representation of a sentence\n",
    "    x = tfidf_row[tfidf_row != 0]\n",
    "\n",
    "    return x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.zeros(len(cleaned_paragraphs))\n",
    "for i in range(len(cleaned_paragraphs)):\n",
    "    score = get_sentence_score(X[i, :])\n",
    "    scores[i] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_idx = np.argsort(-scores)\n",
    "# sort_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_idx[:20]:\n",
    "    print(\"%.2f: %s\" % (scores[i], cleaned_paragraphs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title = soup.find(\"span\", class_=\"rvts23\").text \n",
    "# title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_summarize(text):\n",
    "    X = featurizer.fit_transform(text)\n",
    "    scores = np.zeros(len(text))\n",
    "\n",
    "    for i in range(len(text)):\n",
    "        score = get_sentence_score(X[i, :])\n",
    "        scores[i] = score\n",
    "\n",
    "    sort_idx = np.argsort(-scores)\n",
    "  \n",
    "    summarized = \"\"\n",
    "\n",
    "    for i in sort_idx[:10]:\n",
    "        test = text[i]\n",
    "        summarized += text[i] + \" \"\n",
    "\n",
    "    return summarized.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_summarize(cleaned_paragraphs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LexRank Algorithm is an unsupervised approach to summarization and is inspired by the PageRank algorithm. LexRank uses an IDF-modified cosine similarity score to improve the Pagerank score for document summarization. It summarizes the text based on graph-based centrality scoring of sentences.\n",
    "\n",
    "If one sentence is very similar to other sentences in the text corpus, then that sentence is considered of great importance. Such sentences can be recommended to the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = cosine_similarity(X)\n",
    "# S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize similarity matrix\n",
    "S /= S.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniform transition matrix\n",
    "U = np.ones_like(S) / len(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothed similarity matrix\n",
    "factor = 0.15 \n",
    "S = (1 - factor) * S + factor * U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the limiting / stationary distribution\n",
    "eigenvals, eigenvecs = np.linalg.eig(S.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenvecs[:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenvecs[:, 0].dot(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = eigenvecs[:, 0] / eigenvecs[:, 0].sum()\n",
    "sort_idx = np.argsort(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sort_idx[:10]:\n",
    "    print(\"%.2f: %s\" % (scores[i], cleaned_paragraphs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textRankSummarize(text, factor=0.15):\n",
    "    X = featurizer.fit_transform(text)\n",
    "    S = cosine_similarity(X)  # compute similarity matrix\n",
    "    S /= S.sum(axis=1, keepdims=True)  # normalize similarity matrix\n",
    "    U = np.ones_like(S) / len(S)  # uniform transition matrix\n",
    "    S = (1 - factor) * S + factor * U  # smooth similarity matrix\n",
    "    # find the limiting / stationary distribution\n",
    "    eigenvals, eigenvecs = np.linalg.eig(S.T)\n",
    "    scores = eigenvecs[:, 0] / eigenvecs[:, 0].sum()  # compute scores\n",
    "    sort_idx = np.argsort(-scores)  # sort scores\n",
    "\n",
    "    summarized = \"\"\n",
    "\n",
    "    for i in sort_idx[:10]:\n",
    "        test = text[i]\n",
    "        summarized += text[i] + \" \"\n",
    "\n",
    "    return summarized.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textRankSummarize(cleaned_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.lsa import LsaSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PlaintextParser.from_string(cleaned_paragraphs, Tokenizer(\"english\"))\n",
    "parser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = LsaSummarizer()\n",
    "summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarizer(parser.document, sentences_count=10)\n",
    "# summary\n",
    "[str(sentence) for sentence in summary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luhn\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "# text: text to summarize\n",
    "# no_sentences: number of sentences in your summary,\n",
    "# lang: language of text\n",
    "\n",
    "\n",
    "def luhn_summary(text, no_sentences, lang):\n",
    "    parser = PlaintextParser(text, Tokenizer(lang))\n",
    "    luhn_sum = LuhnSummarizer()\n",
    "    summary = luhn_sum(parser.document, no_sentences)\n",
    "    return [str(sentence) for sentence in summary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "luhn_summary(cleaned_paragraphs, 10, \"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KL\n",
    "from sumy.summarizers.kl import KLSummarizer\n",
    "# text: text to summarize\n",
    "# no_sentences: number of sentences in your summary,\n",
    "# lang: language of text\n",
    "\n",
    "\n",
    "def kl_summary(text, no_sentences, lang):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(lang))\n",
    "    kl_summarizer = KLSummarizer()\n",
    "    summary = kl_summarizer(parser.document, sentences_count=no_sentences)\n",
    "    return [str(sentence) for sentence in summary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_summary(cleaned_paragraphs, 10, \"english\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextRank vs Seq2Seq vs BART"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hardest NLP tasks are the ones where the output isn’t a single label or value (like Classification and Regression), but a full new text (like Translation, Summarization and Conversation)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text summarization is the problem of reducing the number of sentences and words of a document without changing its meaning. There are different techniques to extract information from raw text data and use it for a summarization model, overall they can be categorized as Extractive and Abstractive. Extractive methods select the most important sentences within a text (without necessarily understanding the meaning), therefore the result summary is just a subset of the full text. On the contrary, Abstractive models understand the semantics of the text and generate a meaningful summary. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I’ll keep On Ensuring Civil Rights and Freedoms, and the Legal Regime on the Temporarily Occupied Territory of Ukraine summary on dentons to compare different models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we evaluate the results? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_summary(\n",
    "    human_summary,\n",
    "    tf_idf_summarize(cleaned_paragraphs)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that 22% of unigrams (ROUGE-1) and 7% of bigrams (ROUGE-2) are present in both summaries, while the longest common subsequences (ROUGE-L) match by 7%. Overall, the average score is 16%. ROUGE scores don’t measure how fluent the summary is. For that experts recomend using the good old human eye. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_summary(\n",
    "    human_summary,\n",
    "    textRankSummarize(cleaned_paragraphs)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization: I will display 2 texts, i.e. the summary and the original text, or the predicted summary and the real summary, and highlight the matching parts. It highlights the matching substrings of two texts. It can be used on word-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Find the matching substrings in 2 strings.\n",
    ":parameter\n",
    "    :param a: string - raw text\n",
    "    :param b: string - raw text\n",
    ":return\n",
    "    2 lists used in to display matches\n",
    "'''\n",
    "\n",
    "\n",
    "def utils_split_sentences(a, b):\n",
    "    # find clean matches\n",
    "    match = difflib.SequenceMatcher(isjunk=None, a=a, b=b, autojunk=True)\n",
    "    lst_match = [block for block in match.get_matching_blocks()\n",
    "                 if block.size > 20]\n",
    "\n",
    "    # difflib didn't find any match\n",
    "    if len(lst_match) == 0:\n",
    "        lst_a, lst_b = sent_tokenize(a), sent_tokenize(b)\n",
    "\n",
    "    # work with matches\n",
    "    else:\n",
    "        first_m, last_m = lst_match[0], lst_match[-1]\n",
    "\n",
    "        # a\n",
    "        string = a[0: first_m.a]\n",
    "        lst_a = [t for t in sent_tokenize(string)]\n",
    "        for n in range(len(lst_match)):\n",
    "            m = lst_match[n]\n",
    "            string = a[m.a: m.a+m.size]\n",
    "            lst_a.append(string)\n",
    "            if n+1 < len(lst_match):\n",
    "                next_m = lst_match[n+1]\n",
    "                string = a[m.a+m.size: next_m.a]\n",
    "                lst_a = lst_a + [t for t in sent_tokenize(string)]\n",
    "            else:\n",
    "                break\n",
    "        string = a[last_m.a+last_m.size:]\n",
    "        lst_a = lst_a + [t for t in sent_tokenize(string)]\n",
    "\n",
    "        # b\n",
    "        string = b[0: first_m.b]\n",
    "        lst_b = [t for t in sent_tokenize(string)]\n",
    "        for n in range(len(lst_match)):\n",
    "            m = lst_match[n]\n",
    "            string = b[m.b: m.b+m.size]\n",
    "            lst_b.append(string)\n",
    "            if n+1 < len(lst_match):\n",
    "                next_m = lst_match[n+1]\n",
    "                string = b[m.b+m.size: next_m.b]\n",
    "                lst_b = lst_b + [t for t in sent_tokenize(string)]\n",
    "            else:\n",
    "                break\n",
    "        string = b[last_m.b+last_m.size:]\n",
    "        lst_b = lst_b + [t for t in sent_tokenize(string)]\n",
    "\n",
    "    return lst_a, lst_b\n",
    "\n",
    "'''\n",
    "Highlights the matched strings in text.\n",
    ":parameter\n",
    "    :param a: string - raw text\n",
    "    :param b: string - raw text\n",
    "    :param both: bool - search a in b and, if True, viceversa\n",
    "    :param sentences: bool - if False matches single words\n",
    ":return\n",
    "    text html, it can be visualized on notebook with display(HTML(text))\n",
    "'''\n",
    "\n",
    "\n",
    "def display_string_matching(a, b, both=True, sentences=True, titles=[]):\n",
    "    if sentences is True:\n",
    "        lst_a, lst_b = utils_split_sentences(a, b)\n",
    "    else:\n",
    "        lst_a, lst_b = a.split(), b.split()\n",
    "\n",
    "    # highlight a\n",
    "    first_text = []\n",
    "    for i in lst_a:\n",
    "        if re.sub(r'[^\\w\\s]', '', i.lower()) in [re.sub(r'[^\\w\\s]', '', z.lower()) for z in lst_b]:\n",
    "            first_text.append(\n",
    "                '<span style=\"background-color:rgba(255,215,0,0.3);\">' + i + '</span>')\n",
    "        else:\n",
    "            first_text.append(i)\n",
    "    first_text = ' '.join(first_text)\n",
    "\n",
    "    # highlight b\n",
    "    second_text = []\n",
    "    if both is True:\n",
    "        for i in lst_b:\n",
    "            if re.sub(r'[^\\w\\s]', '', i.lower()) in [re.sub(r'[^\\w\\s]', '', z.lower()) for z in lst_a]:\n",
    "                second_text.append(\n",
    "                    '<span style=\"background-color:rgba(255,215,0,0.3);\">' + i + '</span>')\n",
    "            else:\n",
    "                second_text.append(i)\n",
    "    else:\n",
    "        second_text.append(b)\n",
    "    second_text = ' '.join(second_text)\n",
    "\n",
    "    # concatenate\n",
    "    if len(titles) > 0:\n",
    "        first_text = \"<strong>\"+titles[0]+\"</strong><br>\"+first_text\n",
    "    if len(titles) > 1:\n",
    "        second_text = \"<strong>\"+titles[1]+\"</strong><br>\"+second_text\n",
    "    else:\n",
    "        second_text = \"---\"*65+\"<br><br>\"+second_text\n",
    "    final_text = first_text + '<br><br>' + second_text\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = display_string_matching(\n",
    "    human_summary,\n",
    "    tf_idf_summarize(cleaned_paragraphs),\n",
    "    both=True,\n",
    "    sentences=False,\n",
    "    titles=[\"Real Summary\", \"tfIDF Predicted Summary\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(HTML(match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = display_string_matching(\n",
    "    human_summary,\n",
    "    textRankSummarize(cleaned_paragraphs),\n",
    "    both=True,\n",
    "    sentences=False,\n",
    "    titles=[\"Real Summary\", \"Text Rank Predicted Summary\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(HTML(match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = display_string_matching(\n",
    "    ' '.join(cleaned_paragraphs),\n",
    "    tf_idf_summarize(cleaned_paragraphs),\n",
    "    both=True,\n",
    "    titles=[\"Full Text\", \"tfIDF Predicted Summary\"]\n",
    ")\n",
    "\n",
    "# display(HTML(match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = display_string_matching(\n",
    "    ' '.join(cleaned_paragraphs),\n",
    "    textRankSummarize(cleaned_paragraphs),\n",
    "    both=True,\n",
    "    titles=[\"Full Text\", \"Text Rank Predicted Summary\"]\n",
    ")\n",
    "\n",
    "# display(HTML(match))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction has most of the information mentioned in the original summary. As expected from an Extractive algorithms, the predicted summary is fully contained in the text: the model considers those the most important sentences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes in this chapter, we can\n",
    "also use the tokenizers from NLTK to build a simple text summarizer that is based on\n",
    "Luhn's method that he wrote about in The automatic creation of literature abstracts. This\n",
    "basic, extractive program will first tokenize each sentence in the text sample, then\n",
    "choose which words occur most frequently while excluding unimportant words,\n",
    "called stopwords, and finally it will find the sentence or sentences that include the\n",
    "important words.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our program will include one other subtle effect. We will construct a score for each\n",
    "sentence based on the aggregated scores of the words inside it. For example, suppose\n",
    "the word cat appears in a text 10 times, and the word hat appears three times. The\n",
    "sentence The cat wears a hat will score a 13, or the score for cat plus the score for hat.\n",
    "The sentence His hat is different than her hat will only score 3, since the score for hat is\n",
    "counted only once. This scoring system has the advantage of privileging sentences\n",
    "that have a variety of important words in them, while minimizing the effect of\n",
    "sentences that have fewer important words, even if those words are repeated\n",
    "multiple times. The rationale for this scoring system is that sentences with a variety\n",
    "of important words are more likely to be topic sentences or main ideas, and topic\n",
    "sentences are more relevant for building a summary.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from collections import OrderedDict\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(cleaned_paragraphs)\n",
    "summary_sentences = []\n",
    "candidate_sentences = {}\n",
    "candidate_sentence_counts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(text)\n",
    "# words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase_words = [word.lower() for word in words\n",
    "                   if word not in stopwords.words() and word.isalpha()]\n",
    "lowercase_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = FreqDist(lowercase_words)\n",
    "most_frequent_words = FreqDist(lowercase_words).most_common(25)\n",
    "most_frequent_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    candidate_sentences[sentence] = sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for long, short in candidate_sentences.items():\n",
    "    count = 0\n",
    "    for freq_word, frequency_score in most_frequent_words:\n",
    "        if freq_word in short:\n",
    "            count += frequency_score\n",
    "            candidate_sentence_counts[long] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sentences = OrderedDict(sorted(\n",
    "    candidate_sentence_counts.items(),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True)[:4])\n",
    "print(sorted_sentences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extractive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"english\"\n",
    "stemmer = Stemmer(LANGUAGE)\n",
    "SENTENCES_COUNT = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sum_basic_summarization():\n",
    "    summarized = []\n",
    "    summarizer = SumBasicSummarizer()\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        summarized.append(sentence)\n",
    "\n",
    "    return ' '.join(str(v) for v in summarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_extractively(law_list, summarizer):\n",
    "    folder_name = summarizer.__class__.__name__ + \"_summarized\"\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "    for index, law in enumerate(law_list):\n",
    "        file_path = os.path.join(folder_name, str(index) + \".txt\")\n",
    "\n",
    "        summarized = []\n",
    "        law_text = \" \".join(law[\"cleaned_law\"])\n",
    "        parser = PlaintextParser.from_string(law_text, Tokenizer(LANGUAGE))\n",
    "        print(index, law[\"url\"])\n",
    "\n",
    "        for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "            summarized.append(sentence)\n",
    "            \n",
    "        summarized_text = ' '.join(str(v) for v in summarized)\n",
    "        with open(file_path, 'w') as file:\n",
    "          file.write(summarized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 https://zakon.rada.gov.ua/laws/show/en/1669-18\n",
      "1 https://zakon.rada.gov.ua/laws/show/en/1644-18\n",
      "2 https://zakon.rada.gov.ua/laws/show/en/1556-18\n",
      "3 https://zakon.rada.gov.ua/laws/show/en/1207-18\n",
      "4 https://zakon.rada.gov.ua/laws/show/en/5403-17\n",
      "5 https://zakon.rada.gov.ua/laws/show/en/5293-17\n",
      "6 https://zakon.rada.gov.ua/laws/show/en/5073-17\n",
      "7 https://zakon.rada.gov.ua/laws/show/en/5080-17\n",
      "8 https://zakon.rada.gov.ua/laws/show/en/5018-17\n",
      "9 https://zakon.rada.gov.ua/laws/show/en/4901-17\n",
      "10 https://zakon.rada.gov.ua/laws/show/en/4709-17\n",
      "11 https://zakon.rada.gov.ua/laws/show/en/4651-17\n",
      "12 https://zakon.rada.gov.ua/laws/show/en/4572-17\n",
      "13 https://zakon.rada.gov.ua/laws/show/en/4495-17\n",
      "14 https://zakon.rada.gov.ua/laws/show/en/3773-17\n",
      "15 https://zakon.rada.gov.ua/laws/show/en/3715-17\n",
      "16 https://zakon.rada.gov.ua/laws/show/en/3687-17\n",
      "17 https://zakon.rada.gov.ua/laws/show/en/3671-17\n",
      "18 https://zakon.rada.gov.ua/laws/show/en/3392-17\n",
      "19 https://zakon.rada.gov.ua/laws/show/en/3268-17\n",
      "20 https://zakon.rada.gov.ua/laws/show/en/3038-17\n",
      "21 https://zakon.rada.gov.ua/laws/show/en/2939-17\n",
      "22 https://zakon.rada.gov.ua/laws/show/en/2778-17\n",
      "23 https://zakon.rada.gov.ua/laws/show/en/2735-17\n",
      "24 https://zakon.rada.gov.ua/laws/show/en/2456-17\n",
      "25 https://zakon.rada.gov.ua/laws/show/en/2404-17\n",
      "26 https://zakon.rada.gov.ua/laws/show/en/1704-17\n",
      "27 https://zakon.rada.gov.ua/laws/show/en/877-16\n",
      "28 https://zakon.rada.gov.ua/laws/show/en/143-16\n",
      "29 https://zakon.rada.gov.ua/laws/show/en/2860-15\n",
      "30 https://zakon.rada.gov.ua/laws/show/en/2709-15\n",
      "31 https://zakon.rada.gov.ua/laws/show/en/2662-15\n",
      "32 https://zakon.rada.gov.ua/laws/show/en/2633-15\n",
      "33 https://zakon.rada.gov.ua/laws/show/en/2604-15\n",
      "34 https://zakon.rada.gov.ua/laws/show/en/2509-15\n",
      "35 https://zakon.rada.gov.ua/laws/show/en/1955-15\n",
      "36 https://zakon.rada.gov.ua/laws/show/en/1906-15\n",
      "37 https://zakon.rada.gov.ua/laws/show/en/1914-15\n",
      "38 https://zakon.rada.gov.ua/laws/show/en/1864-15\n",
      "39 https://zakon.rada.gov.ua/laws/show/en/1862-15\n",
      "40 https://zakon.rada.gov.ua/laws/show/en/1861-15\n",
      "41 https://zakon.rada.gov.ua/laws/show/en/1618-15\n",
      "42 https://zakon.rada.gov.ua/laws/show/en/1629-15\n",
      "43 https://zakon.rada.gov.ua/laws/show/en/1382-15\n",
      "44 https://zakon.rada.gov.ua/laws/show/en/1129-15\n",
      "45 https://zakon.rada.gov.ua/laws/show/en/755-15\n",
      "46 https://zakon.rada.gov.ua/laws/show/en/555-15\n",
      "47 https://zakon.rada.gov.ua/laws/show/en/549-15\n",
      "48 https://zakon.rada.gov.ua/laws/show/en/436-15\n",
      "49 https://zakon.rada.gov.ua/laws/show/en/228-15\n",
      "50 https://zakon.rada.gov.ua/laws/show/en/40-15\n",
      "51 https://zakon.rada.gov.ua/laws/show/en/37-15\n",
      "52 https://zakon.rada.gov.ua/laws/show/en/2947-14\n",
      "53 https://zakon.rada.gov.ua/laws/show/en/2918-14\n",
      "54 https://zakon.rada.gov.ua/laws/show/en/2768-14\n",
      "55 https://zakon.rada.gov.ua/laws/show/en/2665-14\n",
      "56 https://zakon.rada.gov.ua/laws/show/en/2664-14\n",
      "57 https://zakon.rada.gov.ua/laws/show/en/2623-14\n",
      "58 https://zakon.rada.gov.ua/laws/show/en/2491-14\n",
      "59 https://zakon.rada.gov.ua/laws/show/en/2344-14\n",
      "60 https://zakon.rada.gov.ua/laws/show/en/2235-14\n",
      "61 https://zakon.rada.gov.ua/laws/show/en/1644-14\n",
      "62 https://zakon.rada.gov.ua/laws/show/en/1391-14\n",
      "63 https://zakon.rada.gov.ua/laws/show/en/1370-14\n",
      "64 https://zakon.rada.gov.ua/laws/show/en/1172-14\n",
      "65 https://zakon.rada.gov.ua/laws/show/en/752-14\n",
      "66 https://zakon.rada.gov.ua/laws/show/en/687-14\n",
      "67 https://zakon.rada.gov.ua/laws/show/en/351-14\n",
      "68 https://zakon.rada.gov.ua/laws/show/en/309-14\n",
      "69 https://zakon.rada.gov.ua/laws/show/en/161-14\n",
      "70 https://zakon.rada.gov.ua/laws/show/en/9/98-%D0%B2%D1%80\n",
      "71 https://zakon.rada.gov.ua/laws/show/en/771/97-%D0%B2%D1%80\n",
      "72 https://zakon.rada.gov.ua/laws/show/en/468/97-%D0%B2%D1%80\n",
      "73 https://zakon.rada.gov.ua/laws/show/en/393/96-%D0%B2%D1%80\n",
      "74 https://zakon.rada.gov.ua/laws/show/en/273/96-%D0%B2%D1%80\n",
      "75 https://zakon.rada.gov.ua/laws/show/en/270/96-%D0%B2%D1%80\n",
      "76 https://zakon.rada.gov.ua/laws/show/en/254%D0%BA/96-%D0%B2%D1%80\n",
      "77 https://zakon.rada.gov.ua/laws/show/en/236/96-%D0%B2%D1%80\n",
      "78 https://zakon.rada.gov.ua/laws/show/en/192/96-%D0%B2%D1%80\n",
      "79 https://zakon.rada.gov.ua/laws/show/en/123/96-%D0%B2%D1%80\n",
      "80 https://zakon.rada.gov.ua/laws/show/en/255/95-%D0%B2%D1%80\n",
      "81 https://zakon.rada.gov.ua/laws/show/en/213/95-%D0%B2%D1%80\n",
      "82 https://zakon.rada.gov.ua/laws/show/en/176/95-%D0%B2%D1%80\n",
      "83 https://zakon.rada.gov.ua/laws/show/en/86/95-%D0%B2%D1%80\n",
      "84 https://zakon.rada.gov.ua/laws/show/en/232/94-%D0%B2%D1%80\n",
      "85 https://zakon.rada.gov.ua/laws/show/en/132/94-%D0%B2%D1%80\n",
      "86 https://zakon.rada.gov.ua/laws/show/en/4002-12\n",
      "87 https://zakon.rada.gov.ua/laws/show/en/3808-12\n",
      "88 https://zakon.rada.gov.ua/laws/show/en/3687-12\n",
      "89 https://zakon.rada.gov.ua/laws/show/en/3688-12\n",
      "90 https://zakon.rada.gov.ua/laws/show/en/3689-12\n",
      "91 https://zakon.rada.gov.ua/laws/show/en/3353-12\n",
      "92 https://zakon.rada.gov.ua/laws/show/en/3322-12\n",
      "93 https://zakon.rada.gov.ua/laws/show/en/3116-12\n",
      "94 https://zakon.rada.gov.ua/laws/show/en/2801-12\n",
      "95 https://zakon.rada.gov.ua/laws/show/en/2657-12\n",
      "96 https://zakon.rada.gov.ua/laws/show/en/1798-12\n",
      "97 https://zakon.rada.gov.ua/laws/show/en/1023-12\n",
      "98 https://zakon.rada.gov.ua/laws/show/en/987-12\n",
      "99 https://zakon.rada.gov.ua/laws/show/en/959-12\n",
      "100 https://zakon.rada.gov.ua/laws/show/en/322-08\n"
     ]
    }
   ],
   "source": [
    "# summarize_extractively(cleaned_law_list, RandomSummarizer(stemmer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarizer = EdmundsonSummarizer(stemmer)\n",
    "# summarizer.bonus_words = ('foo')\n",
    "# summarizer.stigma_words = ('foo')\n",
    "# summarizer.null_words = ('foo')\n",
    "# summarize_extractively(cleaned_law_list, summarizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize_extractively(cleaned_law_list, LuhnSummarizer(stemmer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize_extractively(cleaned_law_list, ReductionSummarizer(stemmer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 https://zakon.rada.gov.ua/laws/show/en/1798-12\n",
      "100 https://zakon.rada.gov.ua/laws/show/en/322-08\n"
     ]
    }
   ],
   "source": [
    "# summarize_extractively(cleaned_law_list, KLSummarizer(stemmer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize_extractively(cleaned_law_list, LexRankSummarizer(stemmer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize_extractively(cleaned_law_list, TextRankSummarizer(stemmer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize_extractively(cleaned_law_list, LsaSummarizer(stemmer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize_extractively(cleaned_law_list, SumBasicSummarizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_extractive_methods(human_abstract_list, summarizer):\n",
    "    method_name = summarizer.__class__.__name__\n",
    "    path = method_name + \"_summarized\"\n",
    "    score_1_list = []\n",
    "    score_2_list = []\n",
    "    score_L_list = []\n",
    "    avg_rouge_list = []\n",
    "\n",
    "    for index, human_abstract in enumerate(human_abstract_list):\n",
    "        file_path = path + \"/\" + str(index) + \".txt\"\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "          continue\n",
    "\n",
    "        f = open(file_path, \"r\")\n",
    "        auto_summarized_text = f.read()\n",
    "        evaluation = evaluate_summary(\n",
    "            \" \".join(human_abstract[\"cleaned_abstract\"]),\n",
    "            auto_summarized_text\n",
    "        )\n",
    "        score_1_list.append(evaluation[\"score_1\"])\n",
    "        score_2_list.append(evaluation[\"score_2\"])\n",
    "        score_L_list.append(evaluation[\"score_L\"])\n",
    "        avg_rouge_list.append(evaluation[\"avg_rouge\"])\n",
    "    \n",
    "    return {\n",
    "        \"score_1_averaged\": round(np.mean(score_1_list), 2),\n",
    "        \"score_2_averaged\": round(np.mean(score_2_list), 2),\n",
    "        \"score_3_averaged\": round(np.mean(score_L_list), 2),\n",
    "        \"avg_rouge_averaged\": round(np.mean(avg_rouge_list), 2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: 0.21 | rouge2: 0.08 | rougeL: 0.13 --> avg rouge: 0.14\n",
      "rouge1: 0.67 | rouge2: 0.37 | rougeL: 0.51 --> avg rouge: 0.52\n",
      "rouge1: 0.05 | rouge2: 0.03 | rougeL: 0.04 --> avg rouge: 0.04\n",
      "rouge1: 0.19 | rouge2: 0.12 | rougeL: 0.15 --> avg rouge: 0.15\n",
      "rouge1: 0.29 | rouge2: 0.11 | rougeL: 0.13 --> avg rouge: 0.18\n",
      "rouge1: 0.26 | rouge2: 0.08 | rougeL: 0.15 --> avg rouge: 0.16\n",
      "rouge1: 0.19 | rouge2: 0.08 | rougeL: 0.12 --> avg rouge: 0.13\n",
      "rouge1: 0.06 | rouge2: 0.03 | rougeL: 0.05 --> avg rouge: 0.05\n",
      "rouge1: 0.0 | rouge2: 0.0 | rougeL: 0.0 --> avg rouge: 0.0\n",
      "rouge1: 0.41 | rouge2: 0.15 | rougeL: 0.22 --> avg rouge: 0.26\n",
      "rouge1: 0.11 | rouge2: 0.04 | rougeL: 0.07 --> avg rouge: 0.07\n",
      "rouge1: 0.05 | rouge2: 0.02 | rougeL: 0.03 --> avg rouge: 0.03\n",
      "rouge1: 0.04 | rouge2: 0.02 | rougeL: 0.03 --> avg rouge: 0.03\n",
      "rouge1: 0.07 | rouge2: 0.02 | rougeL: 0.05 --> avg rouge: 0.05\n",
      "rouge1: 0.19 | rouge2: 0.05 | rougeL: 0.12 --> avg rouge: 0.12\n",
      "rouge1: 0.39 | rouge2: 0.23 | rougeL: 0.28 --> avg rouge: 0.3\n",
      "rouge1: 0.19 | rouge2: 0.1 | rougeL: 0.14 --> avg rouge: 0.14\n",
      "rouge1: 0.25 | rouge2: 0.14 | rougeL: 0.16 --> avg rouge: 0.18\n",
      "rouge1: 0.4 | rouge2: 0.17 | rougeL: 0.28 --> avg rouge: 0.28\n",
      "rouge1: 0.28 | rouge2: 0.06 | rougeL: 0.15 --> avg rouge: 0.16\n",
      "rouge1: 0.39 | rouge2: 0.09 | rougeL: 0.18 --> avg rouge: 0.22\n",
      "rouge1: 0.0 | rouge2: 0.0 | rougeL: 0.0 --> avg rouge: 0.0\n",
      "rouge1: 0.44 | rouge2: 0.14 | rougeL: 0.2 --> avg rouge: 0.26\n",
      "rouge1: 0.39 | rouge2: 0.12 | rougeL: 0.2 --> avg rouge: 0.24\n",
      "rouge1: 0.24 | rouge2: 0.12 | rougeL: 0.14 --> avg rouge: 0.17\n",
      "rouge1: 0.45 | rouge2: 0.18 | rougeL: 0.26 --> avg rouge: 0.3\n",
      "rouge1: 0.12 | rouge2: 0.05 | rougeL: 0.1 --> avg rouge: 0.09\n",
      "rouge1: 0.29 | rouge2: 0.17 | rougeL: 0.2 --> avg rouge: 0.22\n",
      "rouge1: 0.17 | rouge2: 0.08 | rougeL: 0.12 --> avg rouge: 0.12\n",
      "rouge1: 0.46 | rouge2: 0.16 | rougeL: 0.26 --> avg rouge: 0.29\n",
      "rouge1: 0.1 | rouge2: 0.04 | rougeL: 0.07 --> avg rouge: 0.07\n",
      "rouge1: 0.2 | rouge2: 0.09 | rougeL: 0.14 --> avg rouge: 0.14\n",
      "rouge1: 0.45 | rouge2: 0.15 | rougeL: 0.24 --> avg rouge: 0.28\n",
      "rouge1: 0.32 | rouge2: 0.12 | rougeL: 0.2 --> avg rouge: 0.21\n",
      "rouge1: 0.48 | rouge2: 0.21 | rougeL: 0.24 --> avg rouge: 0.31\n",
      "rouge1: 0.43 | rouge2: 0.15 | rougeL: 0.23 --> avg rouge: 0.27\n",
      "rouge1: 0.29 | rouge2: 0.14 | rougeL: 0.19 --> avg rouge: 0.21\n",
      "rouge1: 0.16 | rouge2: 0.08 | rougeL: 0.13 --> avg rouge: 0.12\n",
      "rouge1: 0.0 | rouge2: 0.0 | rougeL: 0.0 --> avg rouge: 0.0\n",
      "rouge1: 0.4 | rouge2: 0.17 | rougeL: 0.24 --> avg rouge: 0.27\n",
      "rouge1: 0.41 | rouge2: 0.15 | rougeL: 0.24 --> avg rouge: 0.27\n",
      "rouge1: 0.11 | rouge2: 0.03 | rougeL: 0.08 --> avg rouge: 0.07\n",
      "rouge1: 0.54 | rouge2: 0.2 | rougeL: 0.29 --> avg rouge: 0.34\n",
      "rouge1: 0.31 | rouge2: 0.12 | rougeL: 0.18 --> avg rouge: 0.2\n",
      "rouge1: 0.11 | rouge2: 0.03 | rougeL: 0.07 --> avg rouge: 0.07\n",
      "rouge1: 0.4 | rouge2: 0.11 | rougeL: 0.21 --> avg rouge: 0.24\n",
      "rouge1: 0.29 | rouge2: 0.11 | rougeL: 0.19 --> avg rouge: 0.2\n",
      "rouge1: 0.48 | rouge2: 0.16 | rougeL: 0.21 --> avg rouge: 0.28\n",
      "rouge1: 0.17 | rouge2: 0.04 | rougeL: 0.09 --> avg rouge: 0.1\n",
      "rouge1: 0.57 | rouge2: 0.25 | rougeL: 0.34 --> avg rouge: 0.39\n",
      "rouge1: 0.57 | rouge2: 0.36 | rougeL: 0.41 --> avg rouge: 0.45\n",
      "rouge1: 0.29 | rouge2: 0.05 | rougeL: 0.17 --> avg rouge: 0.17\n",
      "rouge1: 0.12 | rouge2: 0.03 | rougeL: 0.08 --> avg rouge: 0.08\n",
      "rouge1: 0.11 | rouge2: 0.04 | rougeL: 0.08 --> avg rouge: 0.08\n",
      "rouge1: 0.27 | rouge2: 0.1 | rougeL: 0.14 --> avg rouge: 0.17\n",
      "rouge1: 0.22 | rouge2: 0.12 | rougeL: 0.16 --> avg rouge: 0.17\n",
      "rouge1: 0.39 | rouge2: 0.12 | rougeL: 0.2 --> avg rouge: 0.24\n",
      "rouge1: 0.55 | rouge2: 0.32 | rougeL: 0.33 --> avg rouge: 0.4\n",
      "rouge1: 0.23 | rouge2: 0.08 | rougeL: 0.14 --> avg rouge: 0.15\n",
      "rouge1: 0.26 | rouge2: 0.08 | rougeL: 0.14 --> avg rouge: 0.16\n",
      "rouge1: 0.29 | rouge2: 0.13 | rougeL: 0.19 --> avg rouge: 0.2\n",
      "rouge1: 0.36 | rouge2: 0.14 | rougeL: 0.24 --> avg rouge: 0.25\n",
      "rouge1: 0.49 | rouge2: 0.2 | rougeL: 0.24 --> avg rouge: 0.31\n",
      "rouge1: 0.42 | rouge2: 0.13 | rougeL: 0.24 --> avg rouge: 0.26\n",
      "rouge1: 0.34 | rouge2: 0.13 | rougeL: 0.2 --> avg rouge: 0.22\n",
      "rouge1: 0.22 | rouge2: 0.09 | rougeL: 0.11 --> avg rouge: 0.14\n",
      "rouge1: 0.25 | rouge2: 0.07 | rougeL: 0.15 --> avg rouge: 0.16\n",
      "rouge1: 0.27 | rouge2: 0.12 | rougeL: 0.17 --> avg rouge: 0.19\n",
      "rouge1: 0.19 | rouge2: 0.08 | rougeL: 0.14 --> avg rouge: 0.14\n",
      "rouge1: 0.28 | rouge2: 0.1 | rougeL: 0.16 --> avg rouge: 0.18\n",
      "rouge1: 0.39 | rouge2: 0.12 | rougeL: 0.21 --> avg rouge: 0.24\n",
      "rouge1: 0.28 | rouge2: 0.07 | rougeL: 0.17 --> avg rouge: 0.17\n",
      "rouge1: 0.18 | rouge2: 0.04 | rougeL: 0.11 --> avg rouge: 0.11\n",
      "rouge1: 0.34 | rouge2: 0.06 | rougeL: 0.15 --> avg rouge: 0.18\n",
      "rouge1: 0.16 | rouge2: 0.08 | rougeL: 0.12 --> avg rouge: 0.12\n",
      "rouge1: 0.05 | rouge2: 0.02 | rougeL: 0.05 --> avg rouge: 0.04\n",
      "rouge1: 0.14 | rouge2: 0.08 | rougeL: 0.1 --> avg rouge: 0.11\n",
      "rouge1: 0.32 | rouge2: 0.11 | rougeL: 0.19 --> avg rouge: 0.21\n",
      "rouge1: 0.28 | rouge2: 0.1 | rougeL: 0.17 --> avg rouge: 0.18\n",
      "rouge1: 0.39 | rouge2: 0.17 | rougeL: 0.2 --> avg rouge: 0.25\n",
      "rouge1: 0.34 | rouge2: 0.14 | rougeL: 0.21 --> avg rouge: 0.23\n",
      "rouge1: 0.23 | rouge2: 0.1 | rougeL: 0.14 --> avg rouge: 0.16\n",
      "rouge1: 0.11 | rouge2: 0.03 | rougeL: 0.07 --> avg rouge: 0.07\n",
      "rouge1: 0.19 | rouge2: 0.1 | rougeL: 0.14 --> avg rouge: 0.14\n",
      "rouge1: 0.37 | rouge2: 0.08 | rougeL: 0.22 --> avg rouge: 0.22\n",
      "rouge1: 0.16 | rouge2: 0.06 | rougeL: 0.1 --> avg rouge: 0.11\n",
      "rouge1: 0.39 | rouge2: 0.1 | rougeL: 0.2 --> avg rouge: 0.23\n",
      "rouge1: 0.26 | rouge2: 0.11 | rougeL: 0.16 --> avg rouge: 0.18\n",
      "rouge1: 0.39 | rouge2: 0.11 | rougeL: 0.2 --> avg rouge: 0.23\n",
      "rouge1: 0.4 | rouge2: 0.06 | rougeL: 0.19 --> avg rouge: 0.22\n",
      "rouge1: 0.48 | rouge2: 0.11 | rougeL: 0.2 --> avg rouge: 0.26\n",
      "rouge1: 0.31 | rouge2: 0.09 | rougeL: 0.16 --> avg rouge: 0.19\n",
      "rouge1: 0.19 | rouge2: 0.11 | rougeL: 0.14 --> avg rouge: 0.15\n",
      "rouge1: 0.38 | rouge2: 0.15 | rougeL: 0.22 --> avg rouge: 0.25\n",
      "rouge1: 0.06 | rouge2: 0.02 | rougeL: 0.04 --> avg rouge: 0.04\n",
      "rouge1: 0.1 | rouge2: 0.02 | rougeL: 0.07 --> avg rouge: 0.06\n",
      "rouge1: 0.3 | rouge2: 0.03 | rougeL: 0.16 --> avg rouge: 0.16\n",
      "rouge1: 0.32 | rouge2: 0.09 | rougeL: 0.16 --> avg rouge: 0.19\n",
      "rouge1: 0.29 | rouge2: 0.04 | rougeL: 0.15 --> avg rouge: 0.16\n",
      "rouge1: 0.32 | rouge2: 0.09 | rougeL: 0.17 --> avg rouge: 0.19\n",
      "rouge1: 0.2 | rouge2: 0.06 | rougeL: 0.11 --> avg rouge: 0.12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score_1_averaged': 0.28,\n",
       " 'score_2_averaged': 0.1,\n",
       " 'score_3_averaged': 0.16,\n",
       " 'avg_rouge_averaged': 0.18}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_evaluation = evaluate_extractive_methods(cleaned_abstract_list, RandomSummarizer(stemmer))\n",
    "random_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: 0.36 | rouge2: 0.25 | rougeL: 0.26 --> avg rouge: 0.29\n",
      "rouge1: 0.36 | rouge2: 0.19 | rougeL: 0.17 --> avg rouge: 0.24\n",
      "rouge1: 0.17 | rouge2: 0.07 | rougeL: 0.09 --> avg rouge: 0.11\n",
      "rouge1: 0.14 | rouge2: 0.06 | rougeL: 0.1 --> avg rouge: 0.1\n",
      "rouge1: 0.05 | rouge2: 0.03 | rougeL: 0.03 --> avg rouge: 0.04\n",
      "rouge1: 0.5 | rouge2: 0.16 | rougeL: 0.22 --> avg rouge: 0.29\n",
      "rouge1: 0.35 | rouge2: 0.14 | rougeL: 0.19 --> avg rouge: 0.23\n",
      "rouge1: 0.31 | rouge2: 0.17 | rougeL: 0.17 --> avg rouge: 0.22\n",
      "rouge1: 0.37 | rouge2: 0.17 | rougeL: 0.21 --> avg rouge: 0.25\n",
      "rouge1: 0.17 | rouge2: 0.08 | rougeL: 0.12 --> avg rouge: 0.12\n",
      "rouge1: 0.29 | rouge2: 0.08 | rougeL: 0.14 --> avg rouge: 0.17\n",
      "rouge1: 0.02 | rouge2: 0.01 | rougeL: 0.01 --> avg rouge: 0.01\n",
      "rouge1: 0.09 | rouge2: 0.05 | rougeL: 0.07 --> avg rouge: 0.07\n",
      "rouge1: 0.05 | rouge2: 0.03 | rougeL: 0.04 --> avg rouge: 0.04\n",
      "rouge1: 0.59 | rouge2: 0.27 | rougeL: 0.26 --> avg rouge: 0.37\n",
      "rouge1: 0.24 | rouge2: 0.08 | rougeL: 0.15 --> avg rouge: 0.16\n",
      "rouge1: 0.11 | rouge2: 0.06 | rougeL: 0.07 --> avg rouge: 0.08\n",
      "rouge1: 0.47 | rouge2: 0.22 | rougeL: 0.24 --> avg rouge: 0.31\n",
      "rouge1: 0.61 | rouge2: 0.34 | rougeL: 0.44 --> avg rouge: 0.46\n",
      "rouge1: 0.25 | rouge2: 0.1 | rougeL: 0.17 --> avg rouge: 0.17\n",
      "rouge1: 0.47 | rouge2: 0.15 | rougeL: 0.21 --> avg rouge: 0.28\n",
      "rouge1: 0.25 | rouge2: 0.14 | rougeL: 0.19 --> avg rouge: 0.19\n",
      "rouge1: 0.43 | rouge2: 0.14 | rougeL: 0.21 --> avg rouge: 0.26\n",
      "rouge1: 0.42 | rouge2: 0.15 | rougeL: 0.22 --> avg rouge: 0.26\n",
      "rouge1: 0.09 | rouge2: 0.06 | rougeL: 0.07 --> avg rouge: 0.07\n",
      "rouge1: 0.52 | rouge2: 0.2 | rougeL: 0.3 --> avg rouge: 0.34\n",
      "rouge1: 0.46 | rouge2: 0.14 | rougeL: 0.26 --> avg rouge: 0.29\n",
      "rouge1: 0.24 | rouge2: 0.12 | rougeL: 0.17 --> avg rouge: 0.18\n",
      "rouge1: 0.46 | rouge2: 0.13 | rougeL: 0.22 --> avg rouge: 0.27\n",
      "rouge1: 0.35 | rouge2: 0.1 | rougeL: 0.19 --> avg rouge: 0.21\n",
      "rouge1: 0.39 | rouge2: 0.18 | rougeL: 0.22 --> avg rouge: 0.26\n",
      "rouge1: 0.32 | rouge2: 0.09 | rougeL: 0.16 --> avg rouge: 0.19\n",
      "rouge1: 0.43 | rouge2: 0.17 | rougeL: 0.21 --> avg rouge: 0.27\n",
      "rouge1: 0.19 | rouge2: 0.09 | rougeL: 0.11 --> avg rouge: 0.13\n",
      "rouge1: 0.37 | rouge2: 0.16 | rougeL: 0.21 --> avg rouge: 0.25\n",
      "rouge1: 0.38 | rouge2: 0.11 | rougeL: 0.2 --> avg rouge: 0.23\n",
      "rouge1: 0.3 | rouge2: 0.1 | rougeL: 0.18 --> avg rouge: 0.19\n",
      "rouge1: 0.36 | rouge2: 0.13 | rougeL: 0.2 --> avg rouge: 0.23\n",
      "rouge1: 0.35 | rouge2: 0.11 | rougeL: 0.19 --> avg rouge: 0.22\n",
      "rouge1: 0.28 | rouge2: 0.14 | rougeL: 0.17 --> avg rouge: 0.2\n",
      "rouge1: 0.49 | rouge2: 0.23 | rougeL: 0.27 --> avg rouge: 0.33\n",
      "rouge1: 0.08 | rouge2: 0.03 | rougeL: 0.06 --> avg rouge: 0.06\n",
      "rouge1: 0.41 | rouge2: 0.2 | rougeL: 0.24 --> avg rouge: 0.28\n",
      "rouge1: 0.39 | rouge2: 0.28 | rougeL: 0.28 --> avg rouge: 0.32\n",
      "rouge1: 0.1 | rouge2: 0.06 | rougeL: 0.07 --> avg rouge: 0.08\n",
      "rouge1: 0.2 | rouge2: 0.08 | rougeL: 0.12 --> avg rouge: 0.13\n",
      "rouge1: 0.41 | rouge2: 0.14 | rougeL: 0.2 --> avg rouge: 0.25\n",
      "rouge1: 0.4 | rouge2: 0.14 | rougeL: 0.2 --> avg rouge: 0.25\n",
      "rouge1: 0.17 | rouge2: 0.1 | rougeL: 0.11 --> avg rouge: 0.13\n",
      "rouge1: 0.39 | rouge2: 0.24 | rougeL: 0.29 --> avg rouge: 0.31\n",
      "rouge1: 0.39 | rouge2: 0.13 | rougeL: 0.2 --> avg rouge: 0.24\n",
      "rouge1: 0.46 | rouge2: 0.15 | rougeL: 0.25 --> avg rouge: 0.29\n",
      "rouge1: 0.18 | rouge2: 0.08 | rougeL: 0.11 --> avg rouge: 0.12\n",
      "rouge1: 0.52 | rouge2: 0.2 | rougeL: 0.26 --> avg rouge: 0.33\n",
      "rouge1: 0.04 | rouge2: 0.01 | rougeL: 0.03 --> avg rouge: 0.03\n",
      "rouge1: 0.52 | rouge2: 0.22 | rougeL: 0.28 --> avg rouge: 0.34\n",
      "rouge1: 0.35 | rouge2: 0.15 | rougeL: 0.18 --> avg rouge: 0.23\n",
      "rouge1: 0.41 | rouge2: 0.18 | rougeL: 0.26 --> avg rouge: 0.28\n",
      "rouge1: 0.31 | rouge2: 0.13 | rougeL: 0.18 --> avg rouge: 0.21\n",
      "rouge1: 0.41 | rouge2: 0.12 | rougeL: 0.18 --> avg rouge: 0.24\n",
      "rouge1: 0.54 | rouge2: 0.21 | rougeL: 0.26 --> avg rouge: 0.34\n",
      "rouge1: 0.45 | rouge2: 0.13 | rougeL: 0.24 --> avg rouge: 0.27\n",
      "rouge1: 0.47 | rouge2: 0.21 | rougeL: 0.25 --> avg rouge: 0.31\n",
      "rouge1: 0.38 | rouge2: 0.16 | rougeL: 0.24 --> avg rouge: 0.26\n",
      "rouge1: 0.51 | rouge2: 0.2 | rougeL: 0.29 --> avg rouge: 0.33\n",
      "rouge1: 0.41 | rouge2: 0.13 | rougeL: 0.2 --> avg rouge: 0.25\n",
      "rouge1: 0.36 | rouge2: 0.09 | rougeL: 0.2 --> avg rouge: 0.22\n",
      "rouge1: 0.48 | rouge2: 0.23 | rougeL: 0.31 --> avg rouge: 0.34\n",
      "rouge1: 0.42 | rouge2: 0.15 | rougeL: 0.26 --> avg rouge: 0.28\n",
      "rouge1: 0.22 | rouge2: 0.11 | rougeL: 0.15 --> avg rouge: 0.16\n",
      "rouge1: 0.25 | rouge2: 0.09 | rougeL: 0.16 --> avg rouge: 0.17\n",
      "rouge1: 0.21 | rouge2: 0.03 | rougeL: 0.12 --> avg rouge: 0.12\n",
      "rouge1: 0.47 | rouge2: 0.18 | rougeL: 0.23 --> avg rouge: 0.29\n",
      "rouge1: 0.44 | rouge2: 0.14 | rougeL: 0.23 --> avg rouge: 0.27\n",
      "rouge1: 0.33 | rouge2: 0.14 | rougeL: 0.19 --> avg rouge: 0.22\n",
      "rouge1: 0.45 | rouge2: 0.09 | rougeL: 0.19 --> avg rouge: 0.24\n",
      "rouge1: 0.16 | rouge2: 0.06 | rougeL: 0.1 --> avg rouge: 0.11\n",
      "rouge1: 0.34 | rouge2: 0.12 | rougeL: 0.2 --> avg rouge: 0.22\n",
      "rouge1: 0.3 | rouge2: 0.08 | rougeL: 0.16 --> avg rouge: 0.18\n",
      "rouge1: 0.38 | rouge2: 0.17 | rougeL: 0.24 --> avg rouge: 0.26\n",
      "rouge1: 0.41 | rouge2: 0.14 | rougeL: 0.22 --> avg rouge: 0.26\n",
      "rouge1: 0.16 | rouge2: 0.07 | rougeL: 0.1 --> avg rouge: 0.11\n",
      "rouge1: 0.11 | rouge2: 0.05 | rougeL: 0.07 --> avg rouge: 0.08\n",
      "rouge1: 0.37 | rouge2: 0.13 | rougeL: 0.21 --> avg rouge: 0.24\n",
      "rouge1: 0.33 | rouge2: 0.1 | rougeL: 0.25 --> avg rouge: 0.23\n",
      "rouge1: 0.06 | rouge2: 0.04 | rougeL: 0.05 --> avg rouge: 0.05\n",
      "rouge1: 0.34 | rouge2: 0.11 | rougeL: 0.19 --> avg rouge: 0.21\n",
      "rouge1: 0.39 | rouge2: 0.16 | rougeL: 0.21 --> avg rouge: 0.25\n",
      "rouge1: 0.22 | rouge2: 0.06 | rougeL: 0.13 --> avg rouge: 0.14\n",
      "rouge1: 0.28 | rouge2: 0.08 | rougeL: 0.16 --> avg rouge: 0.17\n",
      "rouge1: 0.29 | rouge2: 0.07 | rougeL: 0.15 --> avg rouge: 0.17\n",
      "rouge1: 0.17 | rouge2: 0.08 | rougeL: 0.12 --> avg rouge: 0.12\n",
      "rouge1: 0.13 | rouge2: 0.08 | rougeL: 0.09 --> avg rouge: 0.1\n",
      "rouge1: 0.35 | rouge2: 0.12 | rougeL: 0.21 --> avg rouge: 0.23\n",
      "rouge1: 0.07 | rouge2: 0.02 | rougeL: 0.05 --> avg rouge: 0.05\n",
      "rouge1: 0.16 | rouge2: 0.03 | rougeL: 0.11 --> avg rouge: 0.1\n",
      "rouge1: 0.2 | rouge2: 0.05 | rougeL: 0.14 --> avg rouge: 0.13\n",
      "rouge1: 0.37 | rouge2: 0.09 | rougeL: 0.15 --> avg rouge: 0.2\n",
      "rouge1: 0.31 | rouge2: 0.06 | rougeL: 0.16 --> avg rouge: 0.18\n",
      "rouge1: 0.19 | rouge2: 0.07 | rougeL: 0.11 --> avg rouge: 0.12\n",
      "rouge1: 0.22 | rouge2: 0.06 | rougeL: 0.14 --> avg rouge: 0.14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score_1_averaged': 0.32,\n",
       " 'score_2_averaged': 0.12,\n",
       " 'score_3_averaged': 0.18,\n",
       " 'avg_rouge_averaged': 0.21}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edmundson_evaluation = evaluate_extractive_methods(cleaned_abstract_list, EdmundsonSummarizer(stemmer))\n",
    "edmundson_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: 0.54 | rouge2: 0.24 | rougeL: 0.29 --> avg rouge: 0.36\n",
      "rouge1: 0.59 | rouge2: 0.32 | rougeL: 0.34 --> avg rouge: 0.42\n",
      "rouge1: 0.44 | rouge2: 0.2 | rougeL: 0.22 --> avg rouge: 0.29\n",
      "rouge1: 0.55 | rouge2: 0.24 | rougeL: 0.23 --> avg rouge: 0.34\n",
      "rouge1: 0.51 | rouge2: 0.17 | rougeL: 0.19 --> avg rouge: 0.29\n",
      "rouge1: 0.48 | rouge2: 0.18 | rougeL: 0.23 --> avg rouge: 0.3\n",
      "rouge1: 0.49 | rouge2: 0.2 | rougeL: 0.26 --> avg rouge: 0.32\n",
      "rouge1: 0.56 | rouge2: 0.29 | rougeL: 0.27 --> avg rouge: 0.37\n",
      "rouge1: 0.57 | rouge2: 0.24 | rougeL: 0.29 --> avg rouge: 0.37\n",
      "rouge1: 0.5 | rouge2: 0.18 | rougeL: 0.27 --> avg rouge: 0.32\n",
      "rouge1: 0.52 | rouge2: 0.18 | rougeL: 0.23 --> avg rouge: 0.31\n",
      "rouge1: 0.41 | rouge2: 0.17 | rougeL: 0.18 --> avg rouge: 0.25\n",
      "rouge1: 0.44 | rouge2: 0.21 | rougeL: 0.29 --> avg rouge: 0.31\n",
      "rouge1: 0.51 | rouge2: 0.21 | rougeL: 0.21 --> avg rouge: 0.31\n",
      "rouge1: 0.4 | rouge2: 0.22 | rougeL: 0.24 --> avg rouge: 0.29\n",
      "rouge1: 0.51 | rouge2: 0.26 | rougeL: 0.3 --> avg rouge: 0.36\n",
      "rouge1: 0.5 | rouge2: 0.22 | rougeL: 0.3 --> avg rouge: 0.34\n",
      "rouge1: 0.56 | rouge2: 0.26 | rougeL: 0.27 --> avg rouge: 0.36\n",
      "rouge1: 0.44 | rouge2: 0.25 | rougeL: 0.35 --> avg rouge: 0.35\n",
      "rouge1: 0.16 | rouge2: 0.03 | rougeL: 0.1 --> avg rouge: 0.1\n",
      "rouge1: 0.21 | rouge2: 0.12 | rougeL: 0.15 --> avg rouge: 0.16\n",
      "rouge1: 0.48 | rouge2: 0.17 | rougeL: 0.21 --> avg rouge: 0.29\n",
      "rouge1: 0.32 | rouge2: 0.13 | rougeL: 0.17 --> avg rouge: 0.21\n",
      "rouge1: 0.26 | rouge2: 0.11 | rougeL: 0.14 --> avg rouge: 0.17\n",
      "rouge1: 0.34 | rouge2: 0.14 | rougeL: 0.17 --> avg rouge: 0.22\n",
      "rouge1: 0.38 | rouge2: 0.18 | rougeL: 0.22 --> avg rouge: 0.26\n",
      "rouge1: 0.48 | rouge2: 0.18 | rougeL: 0.28 --> avg rouge: 0.31\n",
      "rouge1: 0.52 | rouge2: 0.25 | rougeL: 0.27 --> avg rouge: 0.35\n",
      "rouge1: 0.49 | rouge2: 0.16 | rougeL: 0.23 --> avg rouge: 0.29\n",
      "rouge1: 0.4 | rouge2: 0.12 | rougeL: 0.23 --> avg rouge: 0.25\n",
      "rouge1: 0.61 | rouge2: 0.27 | rougeL: 0.27 --> avg rouge: 0.38\n",
      "rouge1: 0.49 | rouge2: 0.15 | rougeL: 0.2 --> avg rouge: 0.28\n",
      "rouge1: 0.3 | rouge2: 0.14 | rougeL: 0.16 --> avg rouge: 0.2\n",
      "rouge1: 0.52 | rouge2: 0.21 | rougeL: 0.31 --> avg rouge: 0.35\n",
      "rouge1: 0.51 | rouge2: 0.2 | rougeL: 0.26 --> avg rouge: 0.32\n",
      "rouge1: 0.47 | rouge2: 0.15 | rougeL: 0.24 --> avg rouge: 0.29\n",
      "rouge1: 0.59 | rouge2: 0.27 | rougeL: 0.34 --> avg rouge: 0.4\n",
      "rouge1: 0.54 | rouge2: 0.2 | rougeL: 0.25 --> avg rouge: 0.33\n",
      "rouge1: 0.37 | rouge2: 0.17 | rougeL: 0.25 --> avg rouge: 0.26\n",
      "rouge1: 0.54 | rouge2: 0.21 | rougeL: 0.28 --> avg rouge: 0.34\n",
      "rouge1: 0.33 | rouge2: 0.16 | rougeL: 0.21 --> avg rouge: 0.23\n",
      "rouge1: 0.36 | rouge2: 0.12 | rougeL: 0.16 --> avg rouge: 0.21\n",
      "rouge1: 0.45 | rouge2: 0.21 | rougeL: 0.25 --> avg rouge: 0.3\n",
      "rouge1: 0.56 | rouge2: 0.22 | rougeL: 0.24 --> avg rouge: 0.34\n",
      "rouge1: 0.41 | rouge2: 0.12 | rougeL: 0.17 --> avg rouge: 0.23\n",
      "rouge1: 0.09 | rouge2: 0.04 | rougeL: 0.06 --> avg rouge: 0.06\n",
      "rouge1: 0.27 | rouge2: 0.09 | rougeL: 0.15 --> avg rouge: 0.17\n",
      "rouge1: 0.3 | rouge2: 0.13 | rougeL: 0.15 --> avg rouge: 0.19\n",
      "rouge1: 0.45 | rouge2: 0.15 | rougeL: 0.18 --> avg rouge: 0.26\n",
      "rouge1: 0.36 | rouge2: 0.2 | rougeL: 0.23 --> avg rouge: 0.26\n",
      "rouge1: 0.38 | rouge2: 0.17 | rougeL: 0.22 --> avg rouge: 0.26\n",
      "rouge1: 0.35 | rouge2: 0.11 | rougeL: 0.19 --> avg rouge: 0.22\n",
      "rouge1: 0.37 | rouge2: 0.06 | rougeL: 0.16 --> avg rouge: 0.2\n",
      "rouge1: 0.47 | rouge2: 0.23 | rougeL: 0.26 --> avg rouge: 0.32\n",
      "rouge1: 0.44 | rouge2: 0.14 | rougeL: 0.17 --> avg rouge: 0.25\n",
      "rouge1: 0.54 | rouge2: 0.27 | rougeL: 0.3 --> avg rouge: 0.37\n",
      "rouge1: 0.19 | rouge2: 0.08 | rougeL: 0.12 --> avg rouge: 0.13\n",
      "rouge1: 0.59 | rouge2: 0.3 | rougeL: 0.32 --> avg rouge: 0.4\n",
      "rouge1: 0.49 | rouge2: 0.17 | rougeL: 0.23 --> avg rouge: 0.3\n",
      "rouge1: 0.34 | rouge2: 0.11 | rougeL: 0.16 --> avg rouge: 0.2\n",
      "rouge1: 0.46 | rouge2: 0.22 | rougeL: 0.23 --> avg rouge: 0.3\n",
      "rouge1: 0.34 | rouge2: 0.14 | rougeL: 0.2 --> avg rouge: 0.23\n",
      "rouge1: 0.58 | rouge2: 0.31 | rougeL: 0.32 --> avg rouge: 0.4\n",
      "rouge1: 0.23 | rouge2: 0.1 | rougeL: 0.14 --> avg rouge: 0.16\n",
      "rouge1: 0.48 | rouge2: 0.2 | rougeL: 0.29 --> avg rouge: 0.32\n",
      "rouge1: 0.49 | rouge2: 0.15 | rougeL: 0.22 --> avg rouge: 0.29\n",
      "rouge1: 0.32 | rouge2: 0.11 | rougeL: 0.15 --> avg rouge: 0.19\n",
      "rouge1: 0.49 | rouge2: 0.21 | rougeL: 0.25 --> avg rouge: 0.32\n",
      "rouge1: 0.54 | rouge2: 0.22 | rougeL: 0.26 --> avg rouge: 0.34\n",
      "rouge1: 0.53 | rouge2: 0.21 | rougeL: 0.26 --> avg rouge: 0.33\n",
      "rouge1: 0.26 | rouge2: 0.08 | rougeL: 0.13 --> avg rouge: 0.16\n",
      "rouge1: 0.15 | rouge2: 0.03 | rougeL: 0.09 --> avg rouge: 0.09\n",
      "rouge1: 0.47 | rouge2: 0.19 | rougeL: 0.26 --> avg rouge: 0.31\n",
      "rouge1: 0.41 | rouge2: 0.12 | rougeL: 0.17 --> avg rouge: 0.23\n",
      "rouge1: 0.43 | rouge2: 0.16 | rougeL: 0.2 --> avg rouge: 0.26\n",
      "rouge1: 0.35 | rouge2: 0.15 | rougeL: 0.22 --> avg rouge: 0.24\n",
      "rouge1: 0.46 | rouge2: 0.22 | rougeL: 0.23 --> avg rouge: 0.3\n",
      "rouge1: 0.35 | rouge2: 0.12 | rougeL: 0.18 --> avg rouge: 0.22\n",
      "rouge1: 0.53 | rouge2: 0.2 | rougeL: 0.24 --> avg rouge: 0.32\n",
      "rouge1: 0.12 | rouge2: 0.07 | rougeL: 0.09 --> avg rouge: 0.09\n",
      "rouge1: 0.53 | rouge2: 0.18 | rougeL: 0.27 --> avg rouge: 0.33\n",
      "rouge1: 0.34 | rouge2: 0.15 | rougeL: 0.16 --> avg rouge: 0.22\n",
      "rouge1: 0.42 | rouge2: 0.1 | rougeL: 0.19 --> avg rouge: 0.24\n",
      "rouge1: 0.56 | rouge2: 0.24 | rougeL: 0.27 --> avg rouge: 0.36\n",
      "rouge1: 0.18 | rouge2: 0.04 | rougeL: 0.12 --> avg rouge: 0.11\n",
      "rouge1: 0.45 | rouge2: 0.19 | rougeL: 0.22 --> avg rouge: 0.29\n",
      "rouge1: 0.39 | rouge2: 0.12 | rougeL: 0.18 --> avg rouge: 0.23\n",
      "rouge1: 0.37 | rouge2: 0.14 | rougeL: 0.22 --> avg rouge: 0.24\n",
      "rouge1: 0.15 | rouge2: 0.05 | rougeL: 0.09 --> avg rouge: 0.1\n",
      "rouge1: 0.19 | rouge2: 0.05 | rougeL: 0.11 --> avg rouge: 0.12\n",
      "rouge1: 0.18 | rouge2: 0.05 | rougeL: 0.1 --> avg rouge: 0.11\n",
      "rouge1: 0.38 | rouge2: 0.13 | rougeL: 0.18 --> avg rouge: 0.23\n",
      "rouge1: 0.67 | rouge2: 0.38 | rougeL: 0.44 --> avg rouge: 0.5\n",
      "rouge1: 0.15 | rouge2: 0.07 | rougeL: 0.12 --> avg rouge: 0.11\n",
      "rouge1: 0.36 | rouge2: 0.14 | rougeL: 0.19 --> avg rouge: 0.23\n",
      "rouge1: 0.43 | rouge2: 0.12 | rougeL: 0.19 --> avg rouge: 0.25\n",
      "rouge1: 0.1 | rouge2: 0.03 | rougeL: 0.06 --> avg rouge: 0.06\n",
      "rouge1: 0.22 | rouge2: 0.06 | rougeL: 0.11 --> avg rouge: 0.13\n",
      "rouge1: 0.34 | rouge2: 0.09 | rougeL: 0.16 --> avg rouge: 0.2\n",
      "rouge1: 0.14 | rouge2: 0.06 | rougeL: 0.08 --> avg rouge: 0.09\n",
      "rouge1: 0.39 | rouge2: 0.12 | rougeL: 0.17 --> avg rouge: 0.23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score_1_averaged': 0.32,\n",
       " 'score_2_averaged': 0.12,\n",
       " 'score_3_averaged': 0.18,\n",
       " 'avg_rouge_averaged': 0.21}"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luhn_evaluation = evaluate_extractive_methods(cleaned_abstract_list, LuhnSummarizer(stemmer))\n",
    "edmundson_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: 0.58 | rouge2: 0.28 | rougeL: 0.3 --> avg rouge: 0.39\n",
      "rouge1: 0.66 | rouge2: 0.4 | rougeL: 0.5 --> avg rouge: 0.52\n",
      "rouge1: 0.44 | rouge2: 0.18 | rougeL: 0.19 --> avg rouge: 0.27\n",
      "rouge1: 0.55 | rouge2: 0.24 | rougeL: 0.24 --> avg rouge: 0.34\n",
      "rouge1: 0.54 | rouge2: 0.19 | rougeL: 0.21 --> avg rouge: 0.31\n",
      "rouge1: 0.45 | rouge2: 0.18 | rougeL: 0.22 --> avg rouge: 0.28\n",
      "rouge1: 0.47 | rouge2: 0.17 | rougeL: 0.22 --> avg rouge: 0.29\n",
      "rouge1: 0.6 | rouge2: 0.31 | rougeL: 0.28 --> avg rouge: 0.4\n",
      "rouge1: 0.6 | rouge2: 0.28 | rougeL: 0.3 --> avg rouge: 0.39\n",
      "rouge1: 0.46 | rouge2: 0.16 | rougeL: 0.21 --> avg rouge: 0.28\n",
      "rouge1: 0.55 | rouge2: 0.2 | rougeL: 0.23 --> avg rouge: 0.33\n",
      "rouge1: 0.44 | rouge2: 0.19 | rougeL: 0.19 --> avg rouge: 0.27\n",
      "rouge1: 0.45 | rouge2: 0.2 | rougeL: 0.28 --> avg rouge: 0.31\n",
      "rouge1: 0.54 | rouge2: 0.22 | rougeL: 0.23 --> avg rouge: 0.33\n",
      "rouge1: 0.42 | rouge2: 0.25 | rougeL: 0.27 --> avg rouge: 0.31\n",
      "rouge1: 0.52 | rouge2: 0.27 | rougeL: 0.33 --> avg rouge: 0.37\n",
      "rouge1: 0.52 | rouge2: 0.24 | rougeL: 0.32 --> avg rouge: 0.36\n",
      "rouge1: 0.55 | rouge2: 0.27 | rougeL: 0.26 --> avg rouge: 0.36\n",
      "rouge1: 0.35 | rouge2: 0.13 | rougeL: 0.24 --> avg rouge: 0.24\n",
      "rouge1: 0.17 | rouge2: 0.04 | rougeL: 0.1 --> avg rouge: 0.1\n",
      "rouge1: 0.17 | rouge2: 0.06 | rougeL: 0.1 --> avg rouge: 0.11\n",
      "rouge1: 0.52 | rouge2: 0.2 | rougeL: 0.24 --> avg rouge: 0.32\n",
      "rouge1: 0.32 | rouge2: 0.14 | rougeL: 0.17 --> avg rouge: 0.21\n",
      "rouge1: 0.25 | rouge2: 0.1 | rougeL: 0.14 --> avg rouge: 0.16\n",
      "rouge1: 0.38 | rouge2: 0.21 | rougeL: 0.2 --> avg rouge: 0.26\n",
      "rouge1: 0.34 | rouge2: 0.15 | rougeL: 0.2 --> avg rouge: 0.23\n",
      "rouge1: 0.42 | rouge2: 0.16 | rougeL: 0.24 --> avg rouge: 0.27\n",
      "rouge1: 0.52 | rouge2: 0.26 | rougeL: 0.28 --> avg rouge: 0.35\n",
      "rouge1: 0.48 | rouge2: 0.18 | rougeL: 0.25 --> avg rouge: 0.3\n",
      "rouge1: 0.42 | rouge2: 0.13 | rougeL: 0.24 --> avg rouge: 0.26\n",
      "rouge1: 0.63 | rouge2: 0.29 | rougeL: 0.3 --> avg rouge: 0.41\n",
      "rouge1: 0.49 | rouge2: 0.14 | rougeL: 0.2 --> avg rouge: 0.28\n",
      "rouge1: 0.28 | rouge2: 0.14 | rougeL: 0.16 --> avg rouge: 0.19\n",
      "rouge1: 0.54 | rouge2: 0.27 | rougeL: 0.34 --> avg rouge: 0.38\n",
      "rouge1: 0.55 | rouge2: 0.25 | rougeL: 0.31 --> avg rouge: 0.37\n",
      "rouge1: 0.46 | rouge2: 0.16 | rougeL: 0.27 --> avg rouge: 0.3\n",
      "rouge1: 0.61 | rouge2: 0.3 | rougeL: 0.34 --> avg rouge: 0.42\n",
      "rouge1: 0.54 | rouge2: 0.21 | rougeL: 0.25 --> avg rouge: 0.33\n",
      "rouge1: 0.3 | rouge2: 0.13 | rougeL: 0.21 --> avg rouge: 0.21\n",
      "rouge1: 0.48 | rouge2: 0.18 | rougeL: 0.24 --> avg rouge: 0.3\n",
      "rouge1: 0.31 | rouge2: 0.15 | rougeL: 0.19 --> avg rouge: 0.22\n",
      "rouge1: 0.33 | rouge2: 0.11 | rougeL: 0.15 --> avg rouge: 0.2\n",
      "rouge1: 0.45 | rouge2: 0.23 | rougeL: 0.29 --> avg rouge: 0.32\n",
      "rouge1: 0.53 | rouge2: 0.21 | rougeL: 0.25 --> avg rouge: 0.33\n",
      "rouge1: 0.3 | rouge2: 0.09 | rougeL: 0.14 --> avg rouge: 0.18\n",
      "rouge1: 0.08 | rouge2: 0.03 | rougeL: 0.05 --> avg rouge: 0.05\n",
      "rouge1: 0.28 | rouge2: 0.13 | rougeL: 0.15 --> avg rouge: 0.19\n",
      "rouge1: 0.24 | rouge2: 0.1 | rougeL: 0.13 --> avg rouge: 0.16\n",
      "rouge1: 0.35 | rouge2: 0.09 | rougeL: 0.16 --> avg rouge: 0.2\n",
      "rouge1: 0.3 | rouge2: 0.17 | rougeL: 0.2 --> avg rouge: 0.22\n",
      "rouge1: 0.42 | rouge2: 0.24 | rougeL: 0.29 --> avg rouge: 0.32\n",
      "rouge1: 0.34 | rouge2: 0.1 | rougeL: 0.19 --> avg rouge: 0.21\n",
      "rouge1: 0.4 | rouge2: 0.1 | rougeL: 0.16 --> avg rouge: 0.22\n",
      "rouge1: 0.44 | rouge2: 0.22 | rougeL: 0.25 --> avg rouge: 0.3\n",
      "rouge1: 0.45 | rouge2: 0.13 | rougeL: 0.17 --> avg rouge: 0.25\n",
      "rouge1: 0.46 | rouge2: 0.21 | rougeL: 0.25 --> avg rouge: 0.31\n",
      "rouge1: 0.19 | rouge2: 0.08 | rougeL: 0.12 --> avg rouge: 0.13\n",
      "rouge1: 0.59 | rouge2: 0.31 | rougeL: 0.32 --> avg rouge: 0.41\n",
      "rouge1: 0.5 | rouge2: 0.2 | rougeL: 0.24 --> avg rouge: 0.31\n",
      "rouge1: 0.34 | rouge2: 0.11 | rougeL: 0.16 --> avg rouge: 0.2\n",
      "rouge1: 0.47 | rouge2: 0.24 | rougeL: 0.28 --> avg rouge: 0.33\n",
      "rouge1: 0.34 | rouge2: 0.15 | rougeL: 0.2 --> avg rouge: 0.23\n",
      "rouge1: 0.59 | rouge2: 0.31 | rougeL: 0.31 --> avg rouge: 0.4\n",
      "rouge1: 0.21 | rouge2: 0.09 | rougeL: 0.14 --> avg rouge: 0.15\n",
      "rouge1: 0.46 | rouge2: 0.2 | rougeL: 0.26 --> avg rouge: 0.31\n",
      "rouge1: 0.54 | rouge2: 0.18 | rougeL: 0.23 --> avg rouge: 0.32\n",
      "rouge1: 0.3 | rouge2: 0.09 | rougeL: 0.16 --> avg rouge: 0.18\n",
      "rouge1: 0.47 | rouge2: 0.21 | rougeL: 0.25 --> avg rouge: 0.31\n",
      "rouge1: 0.5 | rouge2: 0.21 | rougeL: 0.27 --> avg rouge: 0.33\n",
      "rouge1: 0.49 | rouge2: 0.18 | rougeL: 0.24 --> avg rouge: 0.3\n",
      "rouge1: 0.23 | rouge2: 0.08 | rougeL: 0.12 --> avg rouge: 0.14\n",
      "rouge1: 0.13 | rouge2: 0.03 | rougeL: 0.08 --> avg rouge: 0.08\n",
      "rouge1: 0.43 | rouge2: 0.17 | rougeL: 0.21 --> avg rouge: 0.27\n",
      "rouge1: 0.41 | rouge2: 0.11 | rougeL: 0.17 --> avg rouge: 0.23\n",
      "rouge1: 0.43 | rouge2: 0.18 | rougeL: 0.2 --> avg rouge: 0.27\n",
      "rouge1: 0.38 | rouge2: 0.15 | rougeL: 0.24 --> avg rouge: 0.26\n",
      "rouge1: 0.49 | rouge2: 0.23 | rougeL: 0.25 --> avg rouge: 0.32\n",
      "rouge1: 0.38 | rouge2: 0.14 | rougeL: 0.21 --> avg rouge: 0.24\n",
      "rouge1: 0.54 | rouge2: 0.22 | rougeL: 0.25 --> avg rouge: 0.34\n",
      "rouge1: 0.14 | rouge2: 0.08 | rougeL: 0.1 --> avg rouge: 0.11\n",
      "rouge1: 0.51 | rouge2: 0.19 | rougeL: 0.23 --> avg rouge: 0.31\n",
      "rouge1: 0.33 | rouge2: 0.14 | rougeL: 0.16 --> avg rouge: 0.21\n",
      "rouge1: 0.39 | rouge2: 0.09 | rougeL: 0.18 --> avg rouge: 0.22\n",
      "rouge1: 0.57 | rouge2: 0.26 | rougeL: 0.28 --> avg rouge: 0.37\n",
      "rouge1: 0.17 | rouge2: 0.05 | rougeL: 0.11 --> avg rouge: 0.11\n",
      "rouge1: 0.45 | rouge2: 0.19 | rougeL: 0.21 --> avg rouge: 0.28\n",
      "rouge1: 0.41 | rouge2: 0.15 | rougeL: 0.21 --> avg rouge: 0.26\n",
      "rouge1: 0.34 | rouge2: 0.14 | rougeL: 0.19 --> avg rouge: 0.22\n",
      "rouge1: 0.16 | rouge2: 0.05 | rougeL: 0.09 --> avg rouge: 0.1\n",
      "rouge1: 0.19 | rouge2: 0.06 | rougeL: 0.11 --> avg rouge: 0.12\n",
      "rouge1: 0.15 | rouge2: 0.06 | rougeL: 0.09 --> avg rouge: 0.1\n",
      "rouge1: 0.38 | rouge2: 0.13 | rougeL: 0.17 --> avg rouge: 0.23\n",
      "rouge1: 0.66 | rouge2: 0.37 | rougeL: 0.44 --> avg rouge: 0.49\n",
      "rouge1: 0.15 | rouge2: 0.07 | rougeL: 0.1 --> avg rouge: 0.11\n",
      "rouge1: 0.41 | rouge2: 0.14 | rougeL: 0.17 --> avg rouge: 0.24\n",
      "rouge1: 0.41 | rouge2: 0.09 | rougeL: 0.18 --> avg rouge: 0.23\n",
      "rouge1: 0.09 | rouge2: 0.03 | rougeL: 0.06 --> avg rouge: 0.06\n",
      "rouge1: 0.2 | rouge2: 0.06 | rougeL: 0.1 --> avg rouge: 0.12\n",
      "rouge1: 0.03 | rouge2: 0.0 | rougeL: 0.03 --> avg rouge: 0.02\n",
      "rouge1: 0.14 | rouge2: 0.06 | rougeL: 0.08 --> avg rouge: 0.09\n",
      "rouge1: 0.35 | rouge2: 0.12 | rougeL: 0.17 --> avg rouge: 0.21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score_1_averaged': 0.4,\n",
       " 'score_2_averaged': 0.17,\n",
       " 'score_3_averaged': 0.21,\n",
       " 'avg_rouge_averaged': 0.26}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduction_evaluation = evaluate_extractive_methods(cleaned_abstract_list, ReductionSummarizer(stemmer))\n",
    "reduction_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: 0.27 | rouge2: 0.05 | rougeL: 0.15 --> avg rouge: 0.16\n",
      "rouge1: 0.35 | rouge2: 0.11 | rougeL: 0.18 --> avg rouge: 0.21\n",
      "rouge1: 0.14 | rouge2: 0.04 | rougeL: 0.08 --> avg rouge: 0.09\n",
      "rouge1: 0.2 | rouge2: 0.06 | rougeL: 0.13 --> avg rouge: 0.13\n",
      "rouge1: 0.04 | rouge2: 0.01 | rougeL: 0.03 --> avg rouge: 0.03\n",
      "rouge1: 0.23 | rouge2: 0.07 | rougeL: 0.14 --> avg rouge: 0.15\n",
      "rouge1: 0.25 | rouge2: 0.03 | rougeL: 0.16 --> avg rouge: 0.15\n",
      "rouge1: 0.03 | rouge2: 0.01 | rougeL: 0.02 --> avg rouge: 0.02\n",
      "rouge1: 0.14 | rouge2: 0.02 | rougeL: 0.09 --> avg rouge: 0.08\n",
      "rouge1: 0.41 | rouge2: 0.1 | rougeL: 0.22 --> avg rouge: 0.24\n",
      "rouge1: 0.21 | rouge2: 0.05 | rougeL: 0.12 --> avg rouge: 0.13\n",
      "rouge1: 0.07 | rouge2: 0.02 | rougeL: 0.05 --> avg rouge: 0.05\n",
      "rouge1: 0.25 | rouge2: 0.05 | rougeL: 0.14 --> avg rouge: 0.15\n",
      "rouge1: 0.27 | rouge2: 0.08 | rougeL: 0.16 --> avg rouge: 0.17\n",
      "rouge1: 0.3 | rouge2: 0.07 | rougeL: 0.16 --> avg rouge: 0.18\n",
      "rouge1: 0.31 | rouge2: 0.07 | rougeL: 0.19 --> avg rouge: 0.19\n",
      "rouge1: 0.12 | rouge2: 0.03 | rougeL: 0.08 --> avg rouge: 0.08\n",
      "rouge1: 0.24 | rouge2: 0.05 | rougeL: 0.14 --> avg rouge: 0.14\n",
      "rouge1: 0.24 | rouge2: 0.02 | rougeL: 0.16 --> avg rouge: 0.14\n",
      "rouge1: 0.19 | rouge2: 0.03 | rougeL: 0.14 --> avg rouge: 0.12\n",
      "rouge1: 0.28 | rouge2: 0.05 | rougeL: 0.16 --> avg rouge: 0.16\n",
      "rouge1: 0.25 | rouge2: 0.03 | rougeL: 0.15 --> avg rouge: 0.14\n",
      "rouge1: 0.28 | rouge2: 0.05 | rougeL: 0.2 --> avg rouge: 0.18\n",
      "rouge1: 0.32 | rouge2: 0.06 | rougeL: 0.17 --> avg rouge: 0.18\n",
      "rouge1: 0.21 | rouge2: 0.05 | rougeL: 0.14 --> avg rouge: 0.13\n",
      "rouge1: 0.22 | rouge2: 0.09 | rougeL: 0.17 --> avg rouge: 0.16\n",
      "rouge1: 0.37 | rouge2: 0.08 | rougeL: 0.23 --> avg rouge: 0.23\n",
      "rouge1: 0.22 | rouge2: 0.07 | rougeL: 0.15 --> avg rouge: 0.15\n",
      "rouge1: 0.16 | rouge2: 0.04 | rougeL: 0.11 --> avg rouge: 0.1\n",
      "rouge1: 0.31 | rouge2: 0.06 | rougeL: 0.18 --> avg rouge: 0.18\n",
      "rouge1: 0.43 | rouge2: 0.11 | rougeL: 0.22 --> avg rouge: 0.25\n",
      "rouge1: 0.19 | rouge2: 0.04 | rougeL: 0.13 --> avg rouge: 0.12\n",
      "rouge1: 0.3 | rouge2: 0.09 | rougeL: 0.16 --> avg rouge: 0.18\n",
      "rouge1: 0.26 | rouge2: 0.05 | rougeL: 0.14 --> avg rouge: 0.15\n",
      "rouge1: 0.29 | rouge2: 0.07 | rougeL: 0.17 --> avg rouge: 0.18\n",
      "rouge1: 0.25 | rouge2: 0.04 | rougeL: 0.12 --> avg rouge: 0.14\n",
      "rouge1: 0.15 | rouge2: 0.04 | rougeL: 0.11 --> avg rouge: 0.1\n",
      "rouge1: 0.33 | rouge2: 0.08 | rougeL: 0.17 --> avg rouge: 0.19\n",
      "rouge1: 0.26 | rouge2: 0.07 | rougeL: 0.13 --> avg rouge: 0.15\n",
      "rouge1: 0.33 | rouge2: 0.07 | rougeL: 0.19 --> avg rouge: 0.2\n",
      "rouge1: 0.25 | rouge2: 0.05 | rougeL: 0.15 --> avg rouge: 0.15\n",
      "rouge1: 0.27 | rouge2: 0.05 | rougeL: 0.14 --> avg rouge: 0.15\n",
      "rouge1: 0.27 | rouge2: 0.04 | rougeL: 0.17 --> avg rouge: 0.16\n",
      "rouge1: 0.41 | rouge2: 0.09 | rougeL: 0.21 --> avg rouge: 0.24\n",
      "rouge1: 0.22 | rouge2: 0.05 | rougeL: 0.14 --> avg rouge: 0.14\n",
      "rouge1: 0.33 | rouge2: 0.07 | rougeL: 0.19 --> avg rouge: 0.2\n",
      "rouge1: 0.1 | rouge2: 0.02 | rougeL: 0.08 --> avg rouge: 0.07\n",
      "rouge1: 0.27 | rouge2: 0.04 | rougeL: 0.14 --> avg rouge: 0.15\n",
      "rouge1: 0.3 | rouge2: 0.06 | rougeL: 0.18 --> avg rouge: 0.18\n",
      "rouge1: 0.17 | rouge2: 0.07 | rougeL: 0.14 --> avg rouge: 0.13\n",
      "rouge1: 0.29 | rouge2: 0.05 | rougeL: 0.18 --> avg rouge: 0.17\n",
      "rouge1: 0.34 | rouge2: 0.04 | rougeL: 0.19 --> avg rouge: 0.19\n",
      "rouge1: 0.29 | rouge2: 0.05 | rougeL: 0.16 --> avg rouge: 0.17\n",
      "rouge1: 0.21 | rouge2: 0.04 | rougeL: 0.12 --> avg rouge: 0.12\n",
      "rouge1: 0.22 | rouge2: 0.04 | rougeL: 0.13 --> avg rouge: 0.13\n",
      "rouge1: 0.18 | rouge2: 0.03 | rougeL: 0.1 --> avg rouge: 0.1\n",
      "rouge1: 0.24 | rouge2: 0.04 | rougeL: 0.16 --> avg rouge: 0.15\n",
      "rouge1: 0.3 | rouge2: 0.06 | rougeL: 0.2 --> avg rouge: 0.19\n",
      "rouge1: 0.35 | rouge2: 0.08 | rougeL: 0.19 --> avg rouge: 0.21\n",
      "rouge1: 0.29 | rouge2: 0.09 | rougeL: 0.17 --> avg rouge: 0.18\n",
      "rouge1: 0.34 | rouge2: 0.05 | rougeL: 0.17 --> avg rouge: 0.19\n",
      "rouge1: 0.21 | rouge2: 0.04 | rougeL: 0.13 --> avg rouge: 0.13\n",
      "rouge1: 0.26 | rouge2: 0.05 | rougeL: 0.16 --> avg rouge: 0.16\n",
      "rouge1: 0.28 | rouge2: 0.05 | rougeL: 0.17 --> avg rouge: 0.17\n",
      "rouge1: 0.32 | rouge2: 0.05 | rougeL: 0.19 --> avg rouge: 0.19\n",
      "rouge1: 0.2 | rouge2: 0.05 | rougeL: 0.12 --> avg rouge: 0.12\n",
      "rouge1: 0.32 | rouge2: 0.06 | rougeL: 0.16 --> avg rouge: 0.18\n",
      "rouge1: 0.28 | rouge2: 0.06 | rougeL: 0.16 --> avg rouge: 0.17\n",
      "rouge1: 0.31 | rouge2: 0.07 | rougeL: 0.19 --> avg rouge: 0.19\n",
      "rouge1: 0.3 | rouge2: 0.05 | rougeL: 0.18 --> avg rouge: 0.18\n",
      "rouge1: 0.24 | rouge2: 0.06 | rougeL: 0.17 --> avg rouge: 0.16\n",
      "rouge1: 0.16 | rouge2: 0.01 | rougeL: 0.11 --> avg rouge: 0.09\n",
      "rouge1: 0.25 | rouge2: 0.03 | rougeL: 0.17 --> avg rouge: 0.15\n",
      "rouge1: 0.29 | rouge2: 0.04 | rougeL: 0.16 --> avg rouge: 0.16\n",
      "rouge1: 0.22 | rouge2: 0.05 | rougeL: 0.16 --> avg rouge: 0.14\n",
      "rouge1: 0.25 | rouge2: 0.03 | rougeL: 0.13 --> avg rouge: 0.14\n",
      "rouge1: 0.22 | rouge2: 0.04 | rougeL: 0.15 --> avg rouge: 0.14\n",
      "rouge1: 0.26 | rouge2: 0.04 | rougeL: 0.18 --> avg rouge: 0.16\n",
      "rouge1: 0.3 | rouge2: 0.07 | rougeL: 0.18 --> avg rouge: 0.18\n",
      "rouge1: 0.4 | rouge2: 0.07 | rougeL: 0.22 --> avg rouge: 0.23\n",
      "rouge1: 0.35 | rouge2: 0.08 | rougeL: 0.18 --> avg rouge: 0.2\n",
      "rouge1: 0.32 | rouge2: 0.06 | rougeL: 0.18 --> avg rouge: 0.19\n",
      "rouge1: 0.28 | rouge2: 0.07 | rougeL: 0.17 --> avg rouge: 0.17\n",
      "rouge1: 0.28 | rouge2: 0.07 | rougeL: 0.14 --> avg rouge: 0.16\n",
      "rouge1: 0.31 | rouge2: 0.04 | rougeL: 0.18 --> avg rouge: 0.18\n",
      "rouge1: 0.29 | rouge2: 0.05 | rougeL: 0.17 --> avg rouge: 0.17\n",
      "rouge1: 0.4 | rouge2: 0.08 | rougeL: 0.19 --> avg rouge: 0.22\n",
      "rouge1: 0.31 | rouge2: 0.05 | rougeL: 0.18 --> avg rouge: 0.18\n",
      "rouge1: 0.21 | rouge2: 0.06 | rougeL: 0.14 --> avg rouge: 0.14\n",
      "rouge1: 0.26 | rouge2: 0.03 | rougeL: 0.15 --> avg rouge: 0.15\n",
      "rouge1: 0.37 | rouge2: 0.09 | rougeL: 0.21 --> avg rouge: 0.22\n",
      "rouge1: 0.47 | rouge2: 0.15 | rougeL: 0.2 --> avg rouge: 0.27\n",
      "rouge1: 0.47 | rouge2: 0.29 | rougeL: 0.35 --> avg rouge: 0.37\n",
      "rouge1: 0.29 | rouge2: 0.09 | rougeL: 0.19 --> avg rouge: 0.19\n",
      "rouge1: 0.13 | rouge2: 0.04 | rougeL: 0.08 --> avg rouge: 0.08\n",
      "rouge1: 0.32 | rouge2: 0.08 | rougeL: 0.18 --> avg rouge: 0.19\n",
      "rouge1: 0.26 | rouge2: 0.04 | rougeL: 0.14 --> avg rouge: 0.15\n",
      "rouge1: 0.4 | rouge2: 0.07 | rougeL: 0.17 --> avg rouge: 0.21\n",
      "rouge1: 0.31 | rouge2: 0.03 | rougeL: 0.18 --> avg rouge: 0.17\n",
      "rouge1: 0.33 | rouge2: 0.12 | rougeL: 0.2 --> avg rouge: 0.22\n",
      "rouge1: 0.38 | rouge2: 0.12 | rougeL: 0.2 --> avg rouge: 0.23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score_1_averaged': 0.27,\n",
       " 'score_2_averaged': 0.06,\n",
       " 'score_3_averaged': 0.16,\n",
       " 'avg_rouge_averaged': 0.16}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_evaluation = evaluate_extractive_methods(cleaned_abstract_list, KLSummarizer(stemmer))\n",
    "kl_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: 0.43 | rouge2: 0.17 | rougeL: 0.21 --> avg rouge: 0.27\n",
      "rouge1: 0.49 | rouge2: 0.18 | rougeL: 0.23 --> avg rouge: 0.3\n",
      "rouge1: 0.38 | rouge2: 0.17 | rougeL: 0.18 --> avg rouge: 0.24\n",
      "rouge1: 0.37 | rouge2: 0.2 | rougeL: 0.21 --> avg rouge: 0.26\n",
      "rouge1: 0.5 | rouge2: 0.17 | rougeL: 0.2 --> avg rouge: 0.29\n",
      "rouge1: 0.5 | rouge2: 0.18 | rougeL: 0.21 --> avg rouge: 0.3\n",
      "rouge1: 0.33 | rouge2: 0.13 | rougeL: 0.2 --> avg rouge: 0.22\n",
      "rouge1: 0.44 | rouge2: 0.26 | rougeL: 0.22 --> avg rouge: 0.31\n",
      "rouge1: 0.58 | rouge2: 0.26 | rougeL: 0.31 --> avg rouge: 0.38\n",
      "rouge1: 0.38 | rouge2: 0.12 | rougeL: 0.2 --> avg rouge: 0.23\n",
      "rouge1: 0.47 | rouge2: 0.15 | rougeL: 0.22 --> avg rouge: 0.28\n",
      "rouge1: 0.3 | rouge2: 0.14 | rougeL: 0.15 --> avg rouge: 0.2\n",
      "rouge1: 0.37 | rouge2: 0.18 | rougeL: 0.24 --> avg rouge: 0.26\n",
      "rouge1: 0.56 | rouge2: 0.23 | rougeL: 0.24 --> avg rouge: 0.34\n",
      "rouge1: 0.47 | rouge2: 0.23 | rougeL: 0.23 --> avg rouge: 0.31\n",
      "rouge1: 0.5 | rouge2: 0.24 | rougeL: 0.26 --> avg rouge: 0.33\n",
      "rouge1: 0.28 | rouge2: 0.12 | rougeL: 0.17 --> avg rouge: 0.19\n",
      "rouge1: 0.52 | rouge2: 0.24 | rougeL: 0.26 --> avg rouge: 0.34\n",
      "rouge1: 0.4 | rouge2: 0.17 | rougeL: 0.26 --> avg rouge: 0.28\n",
      "rouge1: 0.2 | rouge2: 0.05 | rougeL: 0.11 --> avg rouge: 0.12\n",
      "rouge1: 0.19 | rouge2: 0.08 | rougeL: 0.11 --> avg rouge: 0.13\n",
      "rouge1: 0.55 | rouge2: 0.24 | rougeL: 0.27 --> avg rouge: 0.35\n",
      "rouge1: 0.36 | rouge2: 0.14 | rougeL: 0.2 --> avg rouge: 0.23\n",
      "rouge1: 0.29 | rouge2: 0.12 | rougeL: 0.16 --> avg rouge: 0.19\n",
      "rouge1: 0.62 | rouge2: 0.3 | rougeL: 0.27 --> avg rouge: 0.4\n",
      "rouge1: 0.48 | rouge2: 0.22 | rougeL: 0.26 --> avg rouge: 0.32\n",
      "rouge1: 0.46 | rouge2: 0.18 | rougeL: 0.26 --> avg rouge: 0.3\n",
      "rouge1: 0.55 | rouge2: 0.26 | rougeL: 0.27 --> avg rouge: 0.36\n",
      "rouge1: 0.51 | rouge2: 0.17 | rougeL: 0.24 --> avg rouge: 0.31\n",
      "rouge1: 0.47 | rouge2: 0.16 | rougeL: 0.28 --> avg rouge: 0.3\n",
      "rouge1: 0.49 | rouge2: 0.23 | rougeL: 0.23 --> avg rouge: 0.32\n",
      "rouge1: 0.47 | rouge2: 0.14 | rougeL: 0.2 --> avg rouge: 0.27\n",
      "rouge1: 0.32 | rouge2: 0.15 | rougeL: 0.18 --> avg rouge: 0.22\n",
      "rouge1: 0.51 | rouge2: 0.27 | rougeL: 0.34 --> avg rouge: 0.37\n",
      "rouge1: 0.35 | rouge2: 0.16 | rougeL: 0.22 --> avg rouge: 0.24\n",
      "rouge1: 0.48 | rouge2: 0.14 | rougeL: 0.24 --> avg rouge: 0.29\n",
      "rouge1: 0.37 | rouge2: 0.13 | rougeL: 0.2 --> avg rouge: 0.23\n",
      "rouge1: 0.51 | rouge2: 0.21 | rougeL: 0.26 --> avg rouge: 0.33\n",
      "rouge1: 0.31 | rouge2: 0.12 | rougeL: 0.21 --> avg rouge: 0.21\n",
      "rouge1: 0.5 | rouge2: 0.18 | rougeL: 0.23 --> avg rouge: 0.3\n",
      "rouge1: 0.35 | rouge2: 0.16 | rougeL: 0.22 --> avg rouge: 0.24\n",
      "rouge1: 0.41 | rouge2: 0.1 | rougeL: 0.18 --> avg rouge: 0.23\n",
      "rouge1: 0.48 | rouge2: 0.23 | rougeL: 0.27 --> avg rouge: 0.33\n",
      "rouge1: 0.53 | rouge2: 0.18 | rougeL: 0.25 --> avg rouge: 0.32\n",
      "rouge1: 0.27 | rouge2: 0.09 | rougeL: 0.13 --> avg rouge: 0.16\n",
      "rouge1: 0.1 | rouge2: 0.05 | rougeL: 0.07 --> avg rouge: 0.07\n",
      "rouge1: 0.36 | rouge2: 0.12 | rougeL: 0.18 --> avg rouge: 0.22\n",
      "rouge1: 0.35 | rouge2: 0.13 | rougeL: 0.18 --> avg rouge: 0.22\n",
      "rouge1: 0.31 | rouge2: 0.14 | rougeL: 0.16 --> avg rouge: 0.2\n",
      "rouge1: 0.31 | rouge2: 0.17 | rougeL: 0.2 --> avg rouge: 0.23\n",
      "rouge1: 0.49 | rouge2: 0.18 | rougeL: 0.27 --> avg rouge: 0.31\n",
      "rouge1: 0.38 | rouge2: 0.09 | rougeL: 0.22 --> avg rouge: 0.23\n",
      "rouge1: 0.2 | rouge2: 0.05 | rougeL: 0.11 --> avg rouge: 0.12\n",
      "rouge1: 0.49 | rouge2: 0.26 | rougeL: 0.29 --> avg rouge: 0.35\n",
      "rouge1: 0.48 | rouge2: 0.14 | rougeL: 0.18 --> avg rouge: 0.27\n",
      "rouge1: 0.52 | rouge2: 0.23 | rougeL: 0.3 --> avg rouge: 0.35\n",
      "rouge1: 0.28 | rouge2: 0.11 | rougeL: 0.16 --> avg rouge: 0.18\n",
      "rouge1: 0.63 | rouge2: 0.34 | rougeL: 0.35 --> avg rouge: 0.44\n",
      "rouge1: 0.51 | rouge2: 0.16 | rougeL: 0.25 --> avg rouge: 0.31\n",
      "rouge1: 0.38 | rouge2: 0.12 | rougeL: 0.17 --> avg rouge: 0.22\n",
      "rouge1: 0.59 | rouge2: 0.25 | rougeL: 0.27 --> avg rouge: 0.37\n",
      "rouge1: 0.46 | rouge2: 0.19 | rougeL: 0.25 --> avg rouge: 0.3\n",
      "rouge1: 0.61 | rouge2: 0.31 | rougeL: 0.3 --> avg rouge: 0.41\n",
      "rouge1: 0.28 | rouge2: 0.11 | rougeL: 0.17 --> avg rouge: 0.19\n",
      "rouge1: 0.45 | rouge2: 0.2 | rougeL: 0.25 --> avg rouge: 0.3\n",
      "rouge1: 0.36 | rouge2: 0.13 | rougeL: 0.18 --> avg rouge: 0.22\n",
      "rouge1: 0.34 | rouge2: 0.09 | rougeL: 0.17 --> avg rouge: 0.2\n",
      "rouge1: 0.5 | rouge2: 0.21 | rougeL: 0.25 --> avg rouge: 0.32\n",
      "rouge1: 0.55 | rouge2: 0.24 | rougeL: 0.25 --> avg rouge: 0.35\n",
      "rouge1: 0.55 | rouge2: 0.2 | rougeL: 0.25 --> avg rouge: 0.33\n",
      "rouge1: 0.28 | rouge2: 0.1 | rougeL: 0.14 --> avg rouge: 0.17\n",
      "rouge1: 0.23 | rouge2: 0.04 | rougeL: 0.13 --> avg rouge: 0.13\n",
      "rouge1: 0.44 | rouge2: 0.13 | rougeL: 0.22 --> avg rouge: 0.26\n",
      "rouge1: 0.41 | rouge2: 0.09 | rougeL: 0.18 --> avg rouge: 0.23\n",
      "rouge1: 0.38 | rouge2: 0.16 | rougeL: 0.19 --> avg rouge: 0.24\n",
      "rouge1: 0.43 | rouge2: 0.17 | rougeL: 0.27 --> avg rouge: 0.29\n",
      "rouge1: 0.5 | rouge2: 0.25 | rougeL: 0.26 --> avg rouge: 0.34\n",
      "rouge1: 0.47 | rouge2: 0.18 | rougeL: 0.26 --> avg rouge: 0.3\n",
      "rouge1: 0.52 | rouge2: 0.2 | rougeL: 0.24 --> avg rouge: 0.32\n",
      "rouge1: 0.2 | rouge2: 0.11 | rougeL: 0.15 --> avg rouge: 0.15\n",
      "rouge1: 0.54 | rouge2: 0.2 | rougeL: 0.25 --> avg rouge: 0.33\n",
      "rouge1: 0.34 | rouge2: 0.15 | rougeL: 0.16 --> avg rouge: 0.22\n",
      "rouge1: 0.39 | rouge2: 0.08 | rougeL: 0.18 --> avg rouge: 0.22\n",
      "rouge1: 0.62 | rouge2: 0.29 | rougeL: 0.32 --> avg rouge: 0.41\n",
      "rouge1: 0.21 | rouge2: 0.05 | rougeL: 0.13 --> avg rouge: 0.13\n",
      "rouge1: 0.51 | rouge2: 0.2 | rougeL: 0.27 --> avg rouge: 0.33\n",
      "rouge1: 0.48 | rouge2: 0.15 | rougeL: 0.23 --> avg rouge: 0.29\n",
      "rouge1: 0.43 | rouge2: 0.15 | rougeL: 0.24 --> avg rouge: 0.27\n",
      "rouge1: 0.19 | rouge2: 0.05 | rougeL: 0.11 --> avg rouge: 0.12\n",
      "rouge1: 0.27 | rouge2: 0.1 | rougeL: 0.16 --> avg rouge: 0.18\n",
      "rouge1: 0.24 | rouge2: 0.08 | rougeL: 0.12 --> avg rouge: 0.15\n",
      "rouge1: 0.44 | rouge2: 0.15 | rougeL: 0.2 --> avg rouge: 0.26\n",
      "rouge1: 0.59 | rouge2: 0.35 | rougeL: 0.38 --> avg rouge: 0.44\n",
      "rouge1: 0.22 | rouge2: 0.09 | rougeL: 0.14 --> avg rouge: 0.15\n",
      "rouge1: 0.33 | rouge2: 0.13 | rougeL: 0.15 --> avg rouge: 0.2\n",
      "rouge1: 0.33 | rouge2: 0.1 | rougeL: 0.18 --> avg rouge: 0.2\n",
      "rouge1: 0.14 | rouge2: 0.04 | rougeL: 0.09 --> avg rouge: 0.09\n",
      "rouge1: 0.26 | rouge2: 0.06 | rougeL: 0.13 --> avg rouge: 0.15\n",
      "rouge1: 0.41 | rouge2: 0.1 | rougeL: 0.23 --> avg rouge: 0.25\n",
      "rouge1: 0.17 | rouge2: 0.06 | rougeL: 0.09 --> avg rouge: 0.11\n",
      "rouge1: 0.48 | rouge2: 0.12 | rougeL: 0.19 --> avg rouge: 0.26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score_1_averaged': 0.41,\n",
       " 'score_2_averaged': 0.16,\n",
       " 'score_3_averaged': 0.21,\n",
       " 'avg_rouge_averaged': 0.26}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex_rank_evaluation = evaluate_extractive_methods(cleaned_abstract_list, LexRankSummarizer(stemmer))\n",
    "lex_rank_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: 0.56 | rouge2: 0.24 | rougeL: 0.27 --> avg rouge: 0.36\n",
      "rouge1: 0.66 | rouge2: 0.4 | rougeL: 0.5 --> avg rouge: 0.52\n",
      "rouge1: 0.44 | rouge2: 0.18 | rougeL: 0.19 --> avg rouge: 0.27\n",
      "rouge1: 0.55 | rouge2: 0.24 | rougeL: 0.24 --> avg rouge: 0.34\n",
      "rouge1: 0.53 | rouge2: 0.18 | rougeL: 0.2 --> avg rouge: 0.3\n",
      "rouge1: 0.45 | rouge2: 0.18 | rougeL: 0.22 --> avg rouge: 0.28\n",
      "rouge1: 0.47 | rouge2: 0.17 | rougeL: 0.22 --> avg rouge: 0.29\n",
      "rouge1: 0.6 | rouge2: 0.31 | rougeL: 0.28 --> avg rouge: 0.4\n",
      "rouge1: 0.6 | rouge2: 0.28 | rougeL: 0.3 --> avg rouge: 0.39\n",
      "rouge1: 0.46 | rouge2: 0.16 | rougeL: 0.21 --> avg rouge: 0.28\n",
      "rouge1: 0.54 | rouge2: 0.19 | rougeL: 0.21 --> avg rouge: 0.31\n",
      "rouge1: 0.44 | rouge2: 0.19 | rougeL: 0.19 --> avg rouge: 0.27\n",
      "rouge1: 0.46 | rouge2: 0.21 | rougeL: 0.3 --> avg rouge: 0.32\n",
      "rouge1: 0.53 | rouge2: 0.22 | rougeL: 0.23 --> avg rouge: 0.33\n",
      "rouge1: 0.4 | rouge2: 0.22 | rougeL: 0.24 --> avg rouge: 0.29\n",
      "rouge1: 0.52 | rouge2: 0.27 | rougeL: 0.33 --> avg rouge: 0.37\n",
      "rouge1: 0.53 | rouge2: 0.25 | rougeL: 0.33 --> avg rouge: 0.37\n",
      "rouge1: 0.55 | rouge2: 0.27 | rougeL: 0.26 --> avg rouge: 0.36\n",
      "rouge1: 0.43 | rouge2: 0.23 | rougeL: 0.34 --> avg rouge: 0.33\n",
      "rouge1: 0.17 | rouge2: 0.04 | rougeL: 0.1 --> avg rouge: 0.1\n",
      "rouge1: 0.17 | rouge2: 0.06 | rougeL: 0.1 --> avg rouge: 0.11\n",
      "rouge1: 0.52 | rouge2: 0.2 | rougeL: 0.24 --> avg rouge: 0.32\n",
      "rouge1: 0.32 | rouge2: 0.14 | rougeL: 0.17 --> avg rouge: 0.21\n",
      "rouge1: 0.25 | rouge2: 0.1 | rougeL: 0.14 --> avg rouge: 0.16\n",
      "rouge1: 0.38 | rouge2: 0.21 | rougeL: 0.2 --> avg rouge: 0.26\n",
      "rouge1: 0.34 | rouge2: 0.15 | rougeL: 0.2 --> avg rouge: 0.23\n",
      "rouge1: 0.41 | rouge2: 0.16 | rougeL: 0.23 --> avg rouge: 0.27\n",
      "rouge1: 0.52 | rouge2: 0.26 | rougeL: 0.28 --> avg rouge: 0.35\n",
      "rouge1: 0.48 | rouge2: 0.18 | rougeL: 0.25 --> avg rouge: 0.3\n",
      "rouge1: 0.42 | rouge2: 0.13 | rougeL: 0.24 --> avg rouge: 0.26\n",
      "rouge1: 0.63 | rouge2: 0.29 | rougeL: 0.3 --> avg rouge: 0.41\n",
      "rouge1: 0.49 | rouge2: 0.14 | rougeL: 0.2 --> avg rouge: 0.28\n",
      "rouge1: 0.28 | rouge2: 0.14 | rougeL: 0.16 --> avg rouge: 0.19\n",
      "rouge1: 0.54 | rouge2: 0.27 | rougeL: 0.34 --> avg rouge: 0.38\n",
      "rouge1: 0.53 | rouge2: 0.22 | rougeL: 0.29 --> avg rouge: 0.35\n",
      "rouge1: 0.46 | rouge2: 0.16 | rougeL: 0.27 --> avg rouge: 0.3\n",
      "rouge1: 0.61 | rouge2: 0.3 | rougeL: 0.34 --> avg rouge: 0.42\n",
      "rouge1: 0.54 | rouge2: 0.21 | rougeL: 0.25 --> avg rouge: 0.33\n",
      "rouge1: 0.3 | rouge2: 0.13 | rougeL: 0.21 --> avg rouge: 0.21\n",
      "rouge1: 0.48 | rouge2: 0.18 | rougeL: 0.24 --> avg rouge: 0.3\n",
      "rouge1: 0.31 | rouge2: 0.15 | rougeL: 0.19 --> avg rouge: 0.22\n",
      "rouge1: 0.33 | rouge2: 0.11 | rougeL: 0.15 --> avg rouge: 0.2\n",
      "rouge1: 0.42 | rouge2: 0.2 | rougeL: 0.24 --> avg rouge: 0.29\n",
      "rouge1: 0.53 | rouge2: 0.21 | rougeL: 0.25 --> avg rouge: 0.33\n",
      "rouge1: 0.36 | rouge2: 0.1 | rougeL: 0.15 --> avg rouge: 0.2\n",
      "rouge1: 0.08 | rouge2: 0.03 | rougeL: 0.05 --> avg rouge: 0.05\n",
      "rouge1: 0.28 | rouge2: 0.13 | rougeL: 0.15 --> avg rouge: 0.19\n",
      "rouge1: 0.24 | rouge2: 0.1 | rougeL: 0.13 --> avg rouge: 0.16\n",
      "rouge1: 0.35 | rouge2: 0.09 | rougeL: 0.16 --> avg rouge: 0.2\n",
      "rouge1: 0.3 | rouge2: 0.17 | rougeL: 0.2 --> avg rouge: 0.22\n",
      "rouge1: 0.42 | rouge2: 0.24 | rougeL: 0.29 --> avg rouge: 0.32\n",
      "rouge1: 0.34 | rouge2: 0.1 | rougeL: 0.19 --> avg rouge: 0.21\n",
      "rouge1: 0.4 | rouge2: 0.1 | rougeL: 0.16 --> avg rouge: 0.22\n",
      "rouge1: 0.44 | rouge2: 0.22 | rougeL: 0.25 --> avg rouge: 0.3\n",
      "rouge1: 0.45 | rouge2: 0.13 | rougeL: 0.17 --> avg rouge: 0.25\n",
      "rouge1: 0.46 | rouge2: 0.21 | rougeL: 0.25 --> avg rouge: 0.31\n",
      "rouge1: 0.19 | rouge2: 0.08 | rougeL: 0.12 --> avg rouge: 0.13\n",
      "rouge1: 0.57 | rouge2: 0.3 | rougeL: 0.33 --> avg rouge: 0.4\n",
      "rouge1: 0.5 | rouge2: 0.2 | rougeL: 0.24 --> avg rouge: 0.31\n",
      "rouge1: 0.34 | rouge2: 0.11 | rougeL: 0.16 --> avg rouge: 0.2\n",
      "rouge1: 0.46 | rouge2: 0.24 | rougeL: 0.27 --> avg rouge: 0.32\n",
      "rouge1: 0.34 | rouge2: 0.15 | rougeL: 0.2 --> avg rouge: 0.23\n",
      "rouge1: 0.57 | rouge2: 0.31 | rougeL: 0.32 --> avg rouge: 0.4\n",
      "rouge1: 0.21 | rouge2: 0.09 | rougeL: 0.14 --> avg rouge: 0.15\n",
      "rouge1: 0.46 | rouge2: 0.2 | rougeL: 0.24 --> avg rouge: 0.3\n",
      "rouge1: 0.54 | rouge2: 0.18 | rougeL: 0.23 --> avg rouge: 0.32\n",
      "rouge1: 0.3 | rouge2: 0.09 | rougeL: 0.16 --> avg rouge: 0.18\n",
      "rouge1: 0.47 | rouge2: 0.21 | rougeL: 0.25 --> avg rouge: 0.31\n",
      "rouge1: 0.5 | rouge2: 0.21 | rougeL: 0.27 --> avg rouge: 0.33\n",
      "rouge1: 0.48 | rouge2: 0.18 | rougeL: 0.24 --> avg rouge: 0.3\n",
      "rouge1: 0.23 | rouge2: 0.08 | rougeL: 0.12 --> avg rouge: 0.14\n",
      "rouge1: 0.13 | rouge2: 0.03 | rougeL: 0.08 --> avg rouge: 0.08\n",
      "rouge1: 0.43 | rouge2: 0.17 | rougeL: 0.2 --> avg rouge: 0.27\n",
      "rouge1: 0.41 | rouge2: 0.11 | rougeL: 0.17 --> avg rouge: 0.23\n",
      "rouge1: 0.43 | rouge2: 0.18 | rougeL: 0.2 --> avg rouge: 0.27\n",
      "rouge1: 0.38 | rouge2: 0.15 | rougeL: 0.24 --> avg rouge: 0.26\n",
      "rouge1: 0.47 | rouge2: 0.23 | rougeL: 0.24 --> avg rouge: 0.31\n",
      "rouge1: 0.38 | rouge2: 0.14 | rougeL: 0.21 --> avg rouge: 0.24\n",
      "rouge1: 0.54 | rouge2: 0.22 | rougeL: 0.25 --> avg rouge: 0.34\n",
      "rouge1: 0.14 | rouge2: 0.08 | rougeL: 0.1 --> avg rouge: 0.11\n",
      "rouge1: 0.51 | rouge2: 0.19 | rougeL: 0.23 --> avg rouge: 0.31\n",
      "rouge1: 0.33 | rouge2: 0.14 | rougeL: 0.16 --> avg rouge: 0.21\n",
      "rouge1: 0.39 | rouge2: 0.09 | rougeL: 0.18 --> avg rouge: 0.22\n",
      "rouge1: 0.57 | rouge2: 0.26 | rougeL: 0.28 --> avg rouge: 0.37\n",
      "rouge1: 0.17 | rouge2: 0.05 | rougeL: 0.11 --> avg rouge: 0.11\n",
      "rouge1: 0.45 | rouge2: 0.19 | rougeL: 0.21 --> avg rouge: 0.28\n",
      "rouge1: 0.39 | rouge2: 0.14 | rougeL: 0.2 --> avg rouge: 0.24\n",
      "rouge1: 0.34 | rouge2: 0.14 | rougeL: 0.19 --> avg rouge: 0.22\n",
      "rouge1: 0.16 | rouge2: 0.05 | rougeL: 0.09 --> avg rouge: 0.1\n",
      "rouge1: 0.19 | rouge2: 0.06 | rougeL: 0.11 --> avg rouge: 0.12\n",
      "rouge1: 0.15 | rouge2: 0.06 | rougeL: 0.09 --> avg rouge: 0.1\n",
      "rouge1: 0.37 | rouge2: 0.12 | rougeL: 0.17 --> avg rouge: 0.22\n",
      "rouge1: 0.66 | rouge2: 0.37 | rougeL: 0.44 --> avg rouge: 0.49\n",
      "rouge1: 0.15 | rouge2: 0.07 | rougeL: 0.1 --> avg rouge: 0.11\n",
      "rouge1: 0.41 | rouge2: 0.14 | rougeL: 0.17 --> avg rouge: 0.24\n",
      "rouge1: 0.41 | rouge2: 0.09 | rougeL: 0.18 --> avg rouge: 0.23\n",
      "rouge1: 0.09 | rouge2: 0.03 | rougeL: 0.06 --> avg rouge: 0.06\n",
      "rouge1: 0.2 | rouge2: 0.06 | rougeL: 0.1 --> avg rouge: 0.12\n",
      "rouge1: 0.34 | rouge2: 0.08 | rougeL: 0.16 --> avg rouge: 0.19\n",
      "rouge1: 0.14 | rouge2: 0.06 | rougeL: 0.08 --> avg rouge: 0.09\n",
      "rouge1: 0.35 | rouge2: 0.12 | rougeL: 0.17 --> avg rouge: 0.21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score_1_averaged': 0.4,\n",
       " 'score_2_averaged': 0.17,\n",
       " 'score_3_averaged': 0.21,\n",
       " 'avg_rouge_averaged': 0.26}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_rank_evaluation = evaluate_extractive_methods(cleaned_abstract_list, TextRankSummarizer(stemmer))\n",
    "text_rank_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: 0.48 | rouge2: 0.2 | rougeL: 0.25 --> avg rouge: 0.31\n",
      "rouge1: 0.73 | rouge2: 0.47 | rougeL: 0.62 --> avg rouge: 0.61\n",
      "rouge1: 0.14 | rouge2: 0.07 | rougeL: 0.09 --> avg rouge: 0.1\n",
      "rouge1: 0.27 | rouge2: 0.17 | rougeL: 0.19 --> avg rouge: 0.21\n",
      "rouge1: 0.13 | rouge2: 0.05 | rougeL: 0.08 --> avg rouge: 0.09\n",
      "rouge1: 0.29 | rouge2: 0.12 | rougeL: 0.18 --> avg rouge: 0.2\n",
      "rouge1: 0.34 | rouge2: 0.1 | rougeL: 0.19 --> avg rouge: 0.21\n",
      "rouge1: 0.11 | rouge2: 0.06 | rougeL: 0.07 --> avg rouge: 0.08\n",
      "rouge1: 0.17 | rouge2: 0.07 | rougeL: 0.12 --> avg rouge: 0.12\n",
      "rouge1: 0.4 | rouge2: 0.14 | rougeL: 0.25 --> avg rouge: 0.26\n",
      "rouge1: 0.34 | rouge2: 0.13 | rougeL: 0.17 --> avg rouge: 0.21\n",
      "rouge1: 0.07 | rouge2: 0.03 | rougeL: 0.05 --> avg rouge: 0.05\n",
      "rouge1: 0.31 | rouge2: 0.14 | rougeL: 0.19 --> avg rouge: 0.21\n",
      "rouge1: 0.13 | rouge2: 0.03 | rougeL: 0.07 --> avg rouge: 0.08\n",
      "rouge1: 0.37 | rouge2: 0.12 | rougeL: 0.19 --> avg rouge: 0.23\n",
      "rouge1: 0.55 | rouge2: 0.32 | rougeL: 0.37 --> avg rouge: 0.41\n",
      "rouge1: 0.19 | rouge2: 0.1 | rougeL: 0.14 --> avg rouge: 0.14\n",
      "rouge1: 0.19 | rouge2: 0.07 | rougeL: 0.13 --> avg rouge: 0.13\n",
      "rouge1: 0.5 | rouge2: 0.29 | rougeL: 0.39 --> avg rouge: 0.39\n",
      "rouge1: 0.3 | rouge2: 0.06 | rougeL: 0.15 --> avg rouge: 0.17\n",
      "rouge1: 0.42 | rouge2: 0.13 | rougeL: 0.19 --> avg rouge: 0.25\n",
      "rouge1: 0.41 | rouge2: 0.12 | rougeL: 0.2 --> avg rouge: 0.24\n",
      "rouge1: 0.39 | rouge2: 0.1 | rougeL: 0.23 --> avg rouge: 0.24\n",
      "rouge1: 0.4 | rouge2: 0.11 | rougeL: 0.2 --> avg rouge: 0.24\n",
      "rouge1: 0.3 | rouge2: 0.12 | rougeL: 0.17 --> avg rouge: 0.2\n",
      "rouge1: 0.41 | rouge2: 0.19 | rougeL: 0.23 --> avg rouge: 0.28\n",
      "rouge1: 0.35 | rouge2: 0.12 | rougeL: 0.24 --> avg rouge: 0.24\n",
      "rouge1: 0.24 | rouge2: 0.1 | rougeL: 0.16 --> avg rouge: 0.17\n",
      "rouge1: 0.33 | rouge2: 0.11 | rougeL: 0.19 --> avg rouge: 0.21\n",
      "rouge1: 0.44 | rouge2: 0.15 | rougeL: 0.29 --> avg rouge: 0.29\n",
      "rouge1: 0.29 | rouge2: 0.12 | rougeL: 0.17 --> avg rouge: 0.19\n",
      "rouge1: 0.26 | rouge2: 0.09 | rougeL: 0.12 --> avg rouge: 0.16\n",
      "rouge1: 0.29 | rouge2: 0.11 | rougeL: 0.17 --> avg rouge: 0.19\n",
      "rouge1: 0.36 | rouge2: 0.16 | rougeL: 0.23 --> avg rouge: 0.25\n",
      "rouge1: 0.29 | rouge2: 0.11 | rougeL: 0.18 --> avg rouge: 0.19\n",
      "rouge1: 0.54 | rouge2: 0.16 | rougeL: 0.27 --> avg rouge: 0.32\n",
      "rouge1: 0.18 | rouge2: 0.07 | rougeL: 0.13 --> avg rouge: 0.13\n",
      "rouge1: 0.34 | rouge2: 0.14 | rougeL: 0.2 --> avg rouge: 0.23\n",
      "rouge1: 0.51 | rouge2: 0.21 | rougeL: 0.35 --> avg rouge: 0.36\n",
      "rouge1: 0.43 | rouge2: 0.13 | rougeL: 0.23 --> avg rouge: 0.26\n",
      "rouge1: 0.46 | rouge2: 0.12 | rougeL: 0.22 --> avg rouge: 0.27\n",
      "rouge1: 0.36 | rouge2: 0.11 | rougeL: 0.19 --> avg rouge: 0.22\n",
      "rouge1: 0.57 | rouge2: 0.21 | rougeL: 0.27 --> avg rouge: 0.35\n",
      "rouge1: 0.26 | rouge2: 0.11 | rougeL: 0.16 --> avg rouge: 0.18\n",
      "rouge1: 0.23 | rouge2: 0.08 | rougeL: 0.11 --> avg rouge: 0.14\n",
      "rouge1: 0.3 | rouge2: 0.1 | rougeL: 0.16 --> avg rouge: 0.19\n",
      "rouge1: 0.41 | rouge2: 0.12 | rougeL: 0.22 --> avg rouge: 0.25\n",
      "rouge1: 0.39 | rouge2: 0.11 | rougeL: 0.2 --> avg rouge: 0.23\n",
      "rouge1: 0.2 | rouge2: 0.06 | rougeL: 0.12 --> avg rouge: 0.13\n",
      "rouge1: 0.54 | rouge2: 0.27 | rougeL: 0.39 --> avg rouge: 0.4\n",
      "rouge1: 0.44 | rouge2: 0.14 | rougeL: 0.23 --> avg rouge: 0.27\n",
      "rouge1: 0.29 | rouge2: 0.09 | rougeL: 0.16 --> avg rouge: 0.18\n",
      "rouge1: 0.26 | rouge2: 0.06 | rougeL: 0.13 --> avg rouge: 0.15\n",
      "rouge1: 0.35 | rouge2: 0.14 | rougeL: 0.19 --> avg rouge: 0.23\n",
      "rouge1: 0.33 | rouge2: 0.11 | rougeL: 0.16 --> avg rouge: 0.2\n",
      "rouge1: 0.18 | rouge2: 0.07 | rougeL: 0.12 --> avg rouge: 0.12\n",
      "rouge1: 0.4 | rouge2: 0.13 | rougeL: 0.22 --> avg rouge: 0.25\n",
      "rouge1: 0.65 | rouge2: 0.35 | rougeL: 0.34 --> avg rouge: 0.45\n",
      "rouge1: 0.37 | rouge2: 0.09 | rougeL: 0.18 --> avg rouge: 0.21\n",
      "rouge1: 0.28 | rouge2: 0.1 | rougeL: 0.16 --> avg rouge: 0.18\n",
      "rouge1: 0.4 | rouge2: 0.14 | rougeL: 0.2 --> avg rouge: 0.25\n",
      "rouge1: 0.41 | rouge2: 0.2 | rougeL: 0.26 --> avg rouge: 0.29\n",
      "rouge1: 0.58 | rouge2: 0.28 | rougeL: 0.31 --> avg rouge: 0.39\n",
      "rouge1: 0.44 | rouge2: 0.11 | rougeL: 0.24 --> avg rouge: 0.26\n",
      "rouge1: 0.4 | rouge2: 0.1 | rougeL: 0.21 --> avg rouge: 0.24\n",
      "rouge1: 0.26 | rouge2: 0.09 | rougeL: 0.15 --> avg rouge: 0.17\n",
      "rouge1: 0.4 | rouge2: 0.09 | rougeL: 0.2 --> avg rouge: 0.23\n",
      "rouge1: 0.47 | rouge2: 0.19 | rougeL: 0.23 --> avg rouge: 0.3\n",
      "rouge1: 0.57 | rouge2: 0.24 | rougeL: 0.3 --> avg rouge: 0.37\n",
      "rouge1: 0.38 | rouge2: 0.14 | rougeL: 0.18 --> avg rouge: 0.23\n",
      "rouge1: 0.46 | rouge2: 0.12 | rougeL: 0.25 --> avg rouge: 0.28\n",
      "rouge1: 0.27 | rouge2: 0.04 | rougeL: 0.14 --> avg rouge: 0.15\n",
      "rouge1: 0.38 | rouge2: 0.1 | rougeL: 0.19 --> avg rouge: 0.22\n",
      "rouge1: 0.38 | rouge2: 0.1 | rougeL: 0.15 --> avg rouge: 0.21\n",
      "rouge1: 0.3 | rouge2: 0.12 | rougeL: 0.19 --> avg rouge: 0.2\n",
      "rouge1: 0.48 | rouge2: 0.19 | rougeL: 0.3 --> avg rouge: 0.32\n",
      "rouge1: 0.2 | rouge2: 0.09 | rougeL: 0.12 --> avg rouge: 0.14\n",
      "rouge1: 0.35 | rouge2: 0.08 | rougeL: 0.2 --> avg rouge: 0.21\n",
      "rouge1: 0.4 | rouge2: 0.15 | rougeL: 0.21 --> avg rouge: 0.25\n",
      "rouge1: 0.3 | rouge2: 0.12 | rougeL: 0.18 --> avg rouge: 0.2\n",
      "rouge1: 0.38 | rouge2: 0.12 | rougeL: 0.2 --> avg rouge: 0.23\n",
      "rouge1: 0.25 | rouge2: 0.08 | rougeL: 0.15 --> avg rouge: 0.16\n",
      "rouge1: 0.31 | rouge2: 0.1 | rougeL: 0.19 --> avg rouge: 0.2\n",
      "rouge1: 0.27 | rouge2: 0.1 | rougeL: 0.16 --> avg rouge: 0.18\n",
      "rouge1: 0.33 | rouge2: 0.06 | rougeL: 0.19 --> avg rouge: 0.19\n",
      "rouge1: 0.3 | rouge2: 0.11 | rougeL: 0.18 --> avg rouge: 0.2\n",
      "rouge1: 0.38 | rouge2: 0.1 | rougeL: 0.2 --> avg rouge: 0.23\n",
      "rouge1: 0.31 | rouge2: 0.13 | rougeL: 0.17 --> avg rouge: 0.2\n",
      "rouge1: 0.33 | rouge2: 0.09 | rougeL: 0.17 --> avg rouge: 0.2\n",
      "rouge1: 0.36 | rouge2: 0.06 | rougeL: 0.2 --> avg rouge: 0.21\n",
      "rouge1: 0.4 | rouge2: 0.09 | rougeL: 0.19 --> avg rouge: 0.23\n",
      "rouge1: 0.32 | rouge2: 0.1 | rougeL: 0.16 --> avg rouge: 0.19\n",
      "rouge1: 0.22 | rouge2: 0.1 | rougeL: 0.15 --> avg rouge: 0.16\n",
      "rouge1: 0.38 | rouge2: 0.11 | rougeL: 0.21 --> avg rouge: 0.23\n",
      "rouge1: 0.1 | rouge2: 0.04 | rougeL: 0.07 --> avg rouge: 0.07\n",
      "rouge1: 0.35 | rouge2: 0.07 | rougeL: 0.17 --> avg rouge: 0.2\n",
      "rouge1: 0.31 | rouge2: 0.04 | rougeL: 0.16 --> avg rouge: 0.17\n",
      "rouge1: 0.38 | rouge2: 0.08 | rougeL: 0.17 --> avg rouge: 0.21\n",
      "rouge1: 0.31 | rouge2: 0.04 | rougeL: 0.17 --> avg rouge: 0.17\n",
      "rouge1: 0.26 | rouge2: 0.07 | rougeL: 0.14 --> avg rouge: 0.16\n",
      "rouge1: 0.46 | rouge2: 0.13 | rougeL: 0.19 --> avg rouge: 0.26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score_1_averaged': 0.35,\n",
       " 'score_2_averaged': 0.12,\n",
       " 'score_3_averaged': 0.2,\n",
       " 'avg_rouge_averaged': 0.22}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_evaluation = evaluate_extractive_methods(cleaned_abstract_list, LsaSummarizer(stemmer))\n",
    "lsa_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_summarization(method, text):\n",
    "#     summarized = []\n",
    "#     summarizer = method(stemmer)\n",
    "#     summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "#     parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))\n",
    "\n",
    "#     for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "#         summarized.append(sentence)\n",
    "\n",
    "#     return ' '.join(str(v) for v in summarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_summary(\n",
    "    human_summary,\n",
    "    make_sum_basic_summarization()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_summary(\n",
    "    human_summary,\n",
    "    make_summarization(LuhnSummarizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_summary(\n",
    "    human_summary,\n",
    "    make_summarization(ReductionSummarizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_summary(\n",
    "    human_summary,\n",
    "    make_summarization(KLSummarizer)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_summary(\n",
    "    human_summary,\n",
    "    make_summarization(LexRankSummarizer)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_summary(\n",
    "    human_summary,\n",
    "    make_summarization(TextRankSummarizer)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_summary(\n",
    "    human_summary,\n",
    "    make_summarization(LsaSummarizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_edmundson_summarization():\n",
    "    summarized = []\n",
    "    summarizer = EdmundsonSummarizer(stemmer)\n",
    "    summarizer.bonus_words = ('foo')\n",
    "    summarizer.stigma_words = ('foo')\n",
    "    summarizer.null_words = ('foo')\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        summarized.append(sentence)\n",
    "\n",
    "    return ' '.join(str(v) for v in summarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_summary(\n",
    "    human_summary,\n",
    "    make_edmundson_summarization()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compare y_test and predicted\n",
    "# from IPython.core.display import display, HTML\n",
    "# match = display_string_matching(dtf_test[\"y\"][i], predicted, both=True, sentences=False,\n",
    "#                                 titles=[\"Real Summary\", \"Predicted Summary\"])\n",
    "\n",
    "# display(HTML(match))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Explainability\n",
    "# from IPython.core.display import display, HTML\n",
    "# match = display_string_matching(dtf_test[\"text\"][i], predicted , both=True, sentences=True,\n",
    "#                                 titles=[\"Full Text\", \"Predicted Summary\"])\n",
    "\n",
    "# display(HTML(match))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstractive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/1/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "t5_model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(t5_model_name)\n",
    "t5_summarizer = pipeline(\"summarization\", model=t5_model_name, tokenizer=t5_model_name, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "pegasus_model_name = \"google/pegasus-large\"\n",
    "pegasus_tokenizer = AutoTokenizer.from_pretrained(pegasus_model_name)\n",
    "pegasus_summarizer = pipeline(\"summarization\", model=pegasus_model_name, tokenizer=pegasus_model_name, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_model_name = \"facebook/bart-large-cnn\"\n",
    "bart_tokenizer = AutoTokenizer.from_pretrained(bart_model_name)\n",
    "bart_summarizer = pipeline(\"summarization\", model=bart_model_name, tokenizer=bart_model_name, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_para(para, summarizer):\n",
    "    try:\n",
    "        summarized_para = summarizer(para)[0][\"summary_text\"]\n",
    "        return summarized_para\n",
    "    except:\n",
    "        print(\"An error occurred, chunk text trying to summarize: \", para)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_long_para(para, tokenizer, summarizer, max_token_number):\n",
    "    para_list = para.split(\" \")\n",
    "    para_length = len(para_list)\n",
    "    chunks_number = math.ceil(para_length / max_token_number)\n",
    "    chunkified = np.array_split(para_list, chunks_number)\n",
    "    summarized_chunk_list = []\n",
    "    print(\"mega long sentence, chunks_number is: \", chunks_number)\n",
    "\n",
    "    for chunk in chunkified:\n",
    "      chunk_text = ' '.join(chunk)\n",
    "      summarized_chunk = summarize_para(chunk_text, summarizer)\n",
    "      summarized_chunk_list.append(summarized_chunk)\n",
    "\n",
    "    summarized_long_para = ' '.join(summarized_chunk_list)\n",
    "    summarized_para_length = len(tokenizer.tokenize(summarized_long_para))\n",
    "\n",
    "    if summarized_para_length > max_token_number:\n",
    "       return summarize_long_para(summarized_long_para, tokenizer, summarizer, max_token_number)\n",
    "\n",
    "    return summarized_long_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pegasus_summarized_by = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_summarized_by = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_summarized_by = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_summarized_by = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.88"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(np.mean(pegasus_summarized_by), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.3"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(np.mean(t5_summarized_by), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.45"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(np.mean(bart_summarized_by), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.44"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(np.mean(human_summarized_by), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_summarized_percentage(source, human_summary, generated_summary, tokenizer, model_name):\n",
    "    source_length = len(tokenizer.tokenize(source))\n",
    "\n",
    "    human_summary_length = len(tokenizer.tokenize(human_summary))\n",
    "    generated_summary_length = len(tokenizer.tokenize(generated_summary))\n",
    "\n",
    "    summarized_by_human_percentage = source_length / human_summary_length \n",
    "    auto_summarized_percentage = source_length / generated_summary_length \n",
    "\n",
    "    print(\"summarized by human percentage: \", summarized_by_human_percentage)\n",
    "    print(\"auto summarized percentage: \", auto_summarized_percentage)\n",
    "\n",
    "    human_summarized_by.append(summarized_by_human_percentage)\n",
    "\n",
    "    if model_name == pegasus_model_name:\n",
    "        pegasus_summarized_by.append(auto_summarized_percentage)\n",
    "    elif model_name == t5_model_name:\n",
    "        t5_summarized_by.append(auto_summarized_percentage)\n",
    "    elif model_name == bart_model_name:\n",
    "        bart_summarized_by.append(auto_summarized_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_abstractively(tokenizer, summarizer, law, human_summary, max_token_number):\n",
    "  chunk = []\n",
    "  summarized = []\n",
    "\n",
    "  for para in law:\n",
    "      chunk_length = len(tokenizer.tokenize(' '.join(chunk)))\n",
    "      para_length = len(tokenizer.tokenize(para))\n",
    "      chunk_and_para_length = chunk_length + para_length\n",
    "\n",
    "      if para_length > max_token_number:\n",
    "        para = summarize_long_para(para, tokenizer, summarizer, max_token_number)\n",
    "        summarized_long_para_length = len(tokenizer.tokenize(para))\n",
    "        chunk_and_para_length = chunk_length + summarized_long_para_length\n",
    "\n",
    "      if chunk_and_para_length > max_token_number: \n",
    "        summarized_chunk_text = summarize_para(' '.join(chunk), summarizer)\n",
    "        summarized.append(summarized_chunk_text)\n",
    "        print(chunk_length)\n",
    "        chunk.clear()\n",
    "\n",
    "      chunk.append(para)\n",
    "\n",
    "  if chunk:\n",
    "    chunk_text = ' '.join(chunk)\n",
    "    chunk_length = len(tokenizer.tokenize(chunk_text))\n",
    "    summarized_remaining_chunk_text = summarize_para(chunk_text, summarizer)\n",
    "    summarized.append(summarized_remaining_chunk_text)\n",
    "\n",
    "    print(\"summarized remaining chunk with length: \", chunk_length)\n",
    "    chunk.clear()\n",
    "\n",
    "  summarized_law_text = ' '.join(summarized)\n",
    "  law_text = ' '.join(law)\n",
    "  calculate_summarized_percentage(law_text, human_summary, summarized_law_text, tokenizer, summarizer.model.name_or_path)\n",
    "\n",
    "  return summarized_law_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting data for evaluating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_summary_of_on_ensuring_civil_rights_on_temporarily_occupied_territory = \"\"\"\n",
    "The Law determines a legal regime of occupied territories as well as sets forth special procedures for the operation of governmental authorities, bodies of local self-government, enterprises and organizations, under such legal regime, provides for protection of human and civil rights and freedoms as well as rights, freedoms and lawful interests of legal entities and indicates Ukrainian courts that will consider disputes arising in Crimea during the occupation.\n",
    "It defines \"occupied territory\" as including the land territory of the Autonomous Republic of Crimea and the city of Sevastopol as well as their domestic waters; domestic waters and territorial sea of Ukraine adjacent to the coast of the Crimean peninsula; the territory of an adjacent area, exclusive economic zone and continental shelf along the coast of the Crimean peninsula subject to jurisdiction of Ukrainian bodies of state power in accordance with provisions of international law, the Constitution and laws of Ukraine; underwater space within the territorial sea; air space above these territories.\n",
    "For as long as Crimea remains occupied, the \"occupied territory\" will be subjected to a special legal regime with respect to the border crossing with the \"occupied territory\", carrying out business activity, entering into transactions, holding elections and referenda, exercise of other human and civil rights and freedoms as well as a special procedure for ensuring human and civil rights and freedoms of Ukrainian citizens residing in the temporarily occupied territories of Ukraine.\n",
    "1) Activities of state and local self-governance bodies created in the occupied territories in breach of Ukrainian law shall be prohibited, and their regulations and acts shall be illegal and will not be recognized in Ukraine - e.g. permits, authorizations, allocation of land plots.\n",
    "2) The Law specifically deals with transfers of Crimea-based immovable property by setting forth that ownership rights to immovable property may be transferred and acquired in accordance with Ukrainian law. As a result, Crimea-based immovable property may be transferred outside Crimea and notarized by any Ukrainian notary.\n",
    "3) Disputes arising in Crimea during the occupation will be considered by respective civil, commercial and administrative courts in the city of Kyiv.\n",
    "4) If provisions of the Law are violated, Ukrainian authorities shall apply mechanisms provided for by Ukrainian law and provisions of international laws both in Ukrainian and international courts.\n",
    "Business activities in Crimea will be covered by a separate law intended to specifically address the \"doing business in Crimea\" aspect.\n",
    "Generally, the Law limits business activity in the territory of Crimea and, obviously, persons engaged in business activity in Crimea may be prosecuted by Ukraine and the international community (i.e., the countries that did not recognize the occupation of Crimea by the Russian Federation). Considering this, the Ministry of Justice of Ukraine introduced a simplified procedure for Crimea-registered businesses to change their registered addresses. Accordingly, Crimea-registered businesses may now register anywhere in Ukraine and continue with their businesses.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_summary_of_on_prevention_of_and_counteraction_to_laundering_of_proceeds = \"\"\"\n",
    "The Law considerably updates, inter alia, the requirements of the disclosure of information regarding beneficial owners of legal entities, trusts and other similar legal structures.\n",
    "In particular, the Law provides for the following:\n",
    "1. The legal entities incorporated before  April 28, 2020, shall be obliged to file data related to the beneficial owner and ownership structure within three months from the date of entry into force of the relevant regulation whereby the form and content of such a report disclosing the ownership structure are to be approved.\n",
    "2. During the state registration of a legal entity, or the state registration of changes to the data related to a legal entity, or transition of a legal entity to operations under the model charter, or vice versa, or registration of a separate unit of a legal entity, etc., the ultimate beneficial owner of the founder shall be obliged to submit a notarized copy of his/her passport to the registrar, except for the instances when his/her passport is issued using the Unified State Demographic Register (the so-called biometric passport).\n",
    "3. Legal entities are obliged to keep up to date the information regarding ultimate beneficial owner and ownership structure; to amend such information when necessary; to notify the state registrar of such amendments within 30 working days from the day of their occurrence; and to submit documents confirming these amendments to the state registrar.\n",
    "4. All legal entities are obliged to confirm, annually, within 14 calendar days, their information regarding the ultimate beneficial owner by submitting the documents stipulated by the Law. \n",
    "In consideration of the above, Ukrainian legal entities shall be obliged to bring information regarding their beneficial owners into compliance with the requirements determined by the new Law within a time limit set by it.    \n",
    "The failure to submit, or late submission of, the information regarding ultimate beneficial owners shall be punishable by a fine imposed upon the legal entity’s executive or person authorized to act on behalf of such a legal entity (executive body).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_summary_of_on_licensing_types_of_business_activities = \"\"\"\n",
    "The Law introduces a uniform licensing procedure and procedures for drafting and approving licensing conditions, clearly determines grounds for refusing to issue and revoke licenses, and envisages administrative responsibility for officials and officers of licensing authorities for not complying with its requirements.\n",
    "The number of licensable business activities is reduced from 54 types to 29. The Law establishes that licensing conditions must contain an exhaustive list of documents required to apply for a specific license.\n",
    "Once the Law is in effect, a construction license will be required only for construction of the 4th and 5th complexity categories. A number of other business activities will become non-licensable - such as supply of electrical power  at a non-regulated tariff, centralized water supply and sewage services at a non-regulated tariff, generation, transportation and supply of thermal energy at a non-regulated tariff, extraction of precious metals and precious stones, production of veterinary drugs and preparations; wholesale and retail trade in veterinary drugs and preparations; trade in pesticides and agrochemicals (plant growth regulators only), introduction, importation, exportation of holographic security elements; production of forms of securities, customs brokerage; production, export and import of disks for laser reading systems, matrixes; development, production, sale, repair, modernization and disposal of weapons, military equipment, military weapons and ammunition thereto; trade in liquid biofuels and biogas, digital signature services, industrial fishing in inland waters and rivers, transportation of freight (except hazardous goods) by river, sea, road, rail and air.\n",
    "At the same time, the Law requires a license to carry out such activities as, in particular, banking activities, financial services (except for professional activities in the securities market), professional activities in the securities market, television and radio broadcasting activities; activities in the field of electric power industry, production and sale of ethyl alcohol, cognac and fruit spirits, alcoholic beverages and tobacco, telecommunication activities, manufacturing of medicinal products, wholesale and retail trade in medicinal products, importation of medicinal products, fire suppression services and works, particularly hazardous substances and waste management, medical practice, tour operator activities, transportation of hazardous cargo by river, sea, road, rail and air transport, international carriage of passengers and goods by road; transportation of oil and oil products by main pipeline, transportation and distribution of natural gas.\n",
    "The Law aims to significantly simplify the procedure for obtaining licenses, improve business environment in Ukraine and bolster investments in the economy of Ukraine.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "English_law_pages_URL_list = [\"https://zakon.rada.gov.ua/laws/main/en/llenglaws/page\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await asession.get(English_law_pages_URL_list[0])\n",
    "await r.html.arender()\n",
    "page_link_list = r.html.find(\".page-link:not([title='current page'])\")\n",
    "\n",
    "for page_link in page_link_list:\n",
    "    link_url = list(page_link.absolute_links)[0]\n",
    "    English_law_pages_URL_list.append(link_url)\n",
    "\n",
    "English_law_pages_URL_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "law_list = []\n",
    "abstract_list = []\n",
    "URL_list = []\n",
    "SLEEP_TIME = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def retrieve_abstract(url):\n",
    "    r = await asession.get(url)\n",
    "    await r.html.arender(sleep=SLEEP_TIME)\n",
    "    element_list = r.html.find(\".rvts0 [align='justify'], .rvts0 ul\")\n",
    "    abstract_list.append(element_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def retrieve_law(url):\n",
    "    print(\"retrieving law: \", url)\n",
    "    r = await asession.get(url)\n",
    "    await r.html.arender(sleep=SLEEP_TIME)\n",
    "    abstract_icon = r.html.find(\"[title='Abstract']\", first=True)\n",
    "\n",
    "    if abstract_icon:\n",
    "        element_list = r.html.find(\".rvts0 > .rvps8, .rvts0 > .rvps2, .rvts0 .rvts15\")\n",
    "        \n",
    "        if not element_list:\n",
    "            print(\"bad law: \", url)\n",
    "            return \n",
    "\n",
    "        URL_list.append(url)   \n",
    "        law_list.append(element_list)\n",
    "\n",
    "        abstract_link_element = r.html.find(\"[href*='anot']\", first=True)\n",
    "        abstract_URL = list(abstract_link_element.absolute_links)[0]\n",
    "        asyncio.run(retrieve_abstract(abstract_URL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def retrieve_law_page(url):\n",
    "    r = await asession.get(url)\n",
    "    await r.html.arender()\n",
    "    law_link_list = r.html.find(\".valid\")\n",
    "    \n",
    "    for link in law_link_list:\n",
    "        link_url = list(link.absolute_links)[0]\n",
    "        asyncio.run(retrieve_law(link_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def retrieve_law_list_with_abstract_list(URL_list):\n",
    "    for page_URL in URL_list:\n",
    "        asyncio.run(retrieve_law_page(page_URL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asyncio.run(retrieve_law_list_with_abstract_list(English_law_pages_URL_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asyncio.run(retrieve_law_page(English_law_pages_URL_list[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sift(source_list, sifted_list):\n",
    "    for document in source_list:\n",
    "        sifted_document = []\n",
    "        \n",
    "        if type(document) == list:\n",
    "            for sentence in document:\n",
    "                sifted_sentence = {\n",
    "                \"text\": sentence.text,\n",
    "                \"tag\": sentence.tag,\n",
    "                \"url\": sentence.url\n",
    "                }\n",
    "                sifted_document.append(sifted_sentence)\n",
    "\n",
    "            sifted_list.append(sifted_document)\n",
    "        else:\n",
    "            sifted_document = { \"url\": document.url, \"text\": document.text, \"tag\": document.tag }\n",
    "            sifted_list.append(sifted_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sifted_abstract_list = []\n",
    "sift(abstract_list, sifted_abstract_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sifted_law_list = []\n",
    "sift(law_list, sifted_law_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(sifted_abstract_list, 'sifted_abstract_list.pkl')\n",
    "save_object(sifted_law_list, 'sifted_law_list.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sifted_abstract_list.pkl', 'rb') as inp:\n",
    "    sifted_abstract_list = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sifted_law_list.pkl', 'rb') as inp:\n",
    "    sifted_law_list = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sifted_abstract_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sifted_law_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_abstract(element_list):\n",
    "    cleaned_abstract = []\n",
    "\n",
    "    for element in element_list:\n",
    "        if element[\"text\"]:\n",
    "            if \"\\n\" in element[\"text\"]:\n",
    "              # prevText = cleaned_abstract[-1]\n",
    "              text = element[\"text\"].replace(\"\\n\", \" \")\n",
    "              # cleaned_abstract[-1] = prevText + \" \" + text\n",
    "              element[\"text\"] = text\n",
    "            \n",
    "            cleaned_abstract.append(element[\"text\"])\n",
    "    \n",
    "    return { \"url\": element_list[0][\"url\"], \"cleaned_abstract\": cleaned_abstract }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_abstract_list = []\n",
    "\n",
    "for abstract in sifted_abstract_list:\n",
    "  cleaned_abstract = clean_abstract(abstract)\n",
    "  cleaned_abstract_list.append(cleaned_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_law(element_list):\n",
    "    cleaned_law = []\n",
    "\n",
    "    for element in element_list:\n",
    "        if element[\"text\"]:\n",
    "            if \"\\n\" in element[\"text\"]:\n",
    "                text = element[\"text\"].replace(\"\\n\", \" \")\n",
    "                element[\"text\"] = text\n",
    "\n",
    "            cleaned_law.append(element[\"text\"])\n",
    "    \n",
    "    return { \"url\": element_list[0][\"url\"], \"cleaned_law\": cleaned_law }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_law_list = []\n",
    "\n",
    "for law in sifted_law_list:\n",
    "  cleaned_law = clean_law(law)\n",
    "  cleaned_law_list.append(cleaned_law)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pegasus_summarized = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_summarized = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_summarized = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"bart_default_max_tokens\"\n",
    "# save_auto_generated_summaries(model_name, bart_summarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"t5_default_max_tokens\"\n",
    "# save_auto_generated_summaries(model_name, t5_summarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"pegasus-large\"\n",
    "# save_auto_generated_summaries(model_name, pegasus_summarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_law_abstracts(law_list, tokenizer, summarizer, evaluation_storage, summarized_storage, max_token_number):\n",
    "    for index, law in enumerate(law_list):\n",
    "        print(index, summarizer.model.name_or_path, law[\"url\"])\n",
    "\n",
    "        human_summary = \" \".join(cleaned_abstract_list[index][\"cleaned_abstract\"])\n",
    "        summarized = summarize_abstractively(tokenizer, summarizer, law[\"cleaned_law\"], human_summary, max_token_number)\n",
    "        summarized_storage.append(summarized)\n",
    "\n",
    "        print(summarized)\n",
    "\n",
    "        evaluation_storage.append(evaluate_summary(human_summary, summarized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pegasus_evaluation = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_evaluation = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_evaluation = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pegasus_evaluation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[187], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# evaluate_law_abstracts([cleaned_law_list[2]], bart_tokenizer, bart_summarizer, bart_evaluation, bart_summarized, 1022)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m evaluate_law_abstracts(cleaned_law_list, pegasus_tokenizer, pegasus_summarizer, pegasus_evaluation, pegasus_summarized, \u001b[39m510\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pegasus_evaluation' is not defined"
     ]
    }
   ],
   "source": [
    "# evaluate_law_abstracts([cleaned_law_list[2]], bart_tokenizer, bart_summarizer, bart_evaluation, bart_summarized, 1022)\n",
    "evaluate_law_abstracts(cleaned_law_list, pegasus_tokenizer, pegasus_summarizer, pegasus_evaluation, pegasus_summarized, 510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_law_abstracts(cleaned_law_list, t5_tokenizer, t5_summarizer, t5_evaluation, t5_summarized, 510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_law_abstracts(cleaned_law_list, bart_tokenizer, bart_summarizer, bart_evaluation, bart_summarized, 510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_evaluation(evaluation_list):\n",
    "    score_1_list = []\n",
    "    score_2_list = []\n",
    "    score_L_list = []\n",
    "    avg_rouge_list = []\n",
    "\n",
    "    for evaluation in evaluation_list:\n",
    "        score_1_list.append(evaluation[\"score_1\"])\n",
    "        score_2_list.append(evaluation[\"score_2\"])\n",
    "        score_L_list.append(evaluation[\"score_L\"])\n",
    "        avg_rouge_list.append(evaluation[\"avg_rouge\"])\n",
    "\n",
    "    score_1_averaged = round(np.mean([score_1_list]), 2)\n",
    "    score_2_averaged = round(np.mean([score_2_list]), 2)\n",
    "    score_L_averaged = round(np.mean([score_L_list]), 2)\n",
    "    avg_rouge_averaged = round(np.mean([avg_rouge_list]), 2)\n",
    "\n",
    "    return { \"score_1_averaged\": score_1_averaged, \"score_2_averaged\": score_2_averaged, \"score_L_averaged\": score_L_averaged, \"avg_rouge_averaged\": avg_rouge_averaged }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_evaluation(pegasus_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_evaluation(t5_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_evaluation(bart_evaluation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_bertscore(model_name):\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for index, human_summary in enumerate(cleaned_abstract_list):\n",
    "        file_path = \"./\" + model_name + \"_summarized/\" + str(index) + \".txt\"\n",
    "        file = open(file_path, \"r\")\n",
    "        auto_summarized_text = file.read()\n",
    "        file.close()\n",
    "        human_summarized_text = \" \".join(human_summary[\"cleaned_abstract\"])\n",
    "\n",
    "        references.append(human_summarized_text)\n",
    "        predictions.append(auto_summarized_text)\n",
    "\n",
    "    result = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "    precision = result[\"precision\"]\n",
    "    recall = result[\"recall\"]\n",
    "    f1 = result[\"f1\"]\n",
    "    avg_precision = round(np.mean(precision), 2)\n",
    "    avg_recall = round(np.mean(recall), 2)\n",
    "    avg_f1 = round(np.mean(f1), 2)\n",
    "\n",
    "    return { \"avg_precision\": avg_precision, \"avg_recall\": avg_recall, \"avg_f1\": avg_f1, \"precision\": precision, \"recall\": recall, \"f1\": f1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_precision': 0.85, 'avg_recall': 0.84, 'avg_f1': 0.84}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edmundson_evaluated_with_bertscore = evaluate_with_bertscore(\"EdmundsonSummarizer\")\n",
    "edmundson_evaluated_with_bertscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_precision': 0.83, 'avg_recall': 0.83, 'avg_f1': 0.83}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_rank_evaluated_with_bertscore = evaluate_with_bertscore(\"TextRankSummarizer\")\n",
    "text_rank_evaluated_with_bertscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_precision': 0.83, 'avg_recall': 0.83, 'avg_f1': 0.83}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex_rank_evaluated_with_bertscore = evaluate_with_bertscore(\"LexRankSummarizer\")\n",
    "lex_rank_evaluated_with_bertscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_precision': 0.83, 'avg_recall': 0.84, 'avg_f1': 0.83}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_evaluated_with_bertscore = evaluate_with_bertscore(\"LsaSummarizer\")\n",
    "lsa_evaluated_with_bertscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_precision': 0.83, 'avg_recall': 0.84, 'avg_f1': 0.83}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luhn_evaluated_with_bertscore = evaluate_with_bertscore(\"LuhnSummarizer\")\n",
    "luhn_evaluated_with_bertscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_precision': 0.84,\n",
       " 'avg_recall': 0.82,\n",
       " 'avg_f1': 0.83,\n",
       " 'precision': [0.8295081853866577,\n",
       "  0.8676743507385254,\n",
       "  0.8232463598251343,\n",
       "  0.8307418823242188,\n",
       "  0.8289199471473694,\n",
       "  0.8230001330375671,\n",
       "  0.8449926376342773,\n",
       "  0.8487291932106018,\n",
       "  0.8596936464309692,\n",
       "  0.8414504528045654,\n",
       "  0.8280347585678101,\n",
       "  0.8213546276092529,\n",
       "  0.8441535234451294,\n",
       "  0.8147008419036865,\n",
       "  0.8204883337020874,\n",
       "  0.8631142377853394,\n",
       "  0.8353319764137268,\n",
       "  0.829002857208252,\n",
       "  0.8351359963417053,\n",
       "  0.7918592095375061,\n",
       "  0.8156771063804626,\n",
       "  0.865684986114502,\n",
       "  0.8334755897521973,\n",
       "  0.8376014232635498,\n",
       "  0.8217756152153015,\n",
       "  0.8496711254119873,\n",
       "  0.8344910144805908,\n",
       "  0.8719782829284668,\n",
       "  0.8565788865089417,\n",
       "  0.8554496765136719,\n",
       "  0.8122999668121338,\n",
       "  0.8436827659606934,\n",
       "  0.8374814987182617,\n",
       "  0.8173473477363586,\n",
       "  0.8637114763259888,\n",
       "  0.8276344537734985,\n",
       "  0.8229872584342957,\n",
       "  0.8796212077140808,\n",
       "  0.8555228114128113,\n",
       "  0.8590303063392639,\n",
       "  0.8566054105758667,\n",
       "  0.8205284476280212,\n",
       "  0.8391947150230408,\n",
       "  0.8366780281066895,\n",
       "  0.8155180811882019,\n",
       "  0.8142979145050049,\n",
       "  0.8247551918029785,\n",
       "  0.8277120590209961,\n",
       "  0.8214375972747803,\n",
       "  0.8559040427207947,\n",
       "  0.8748961091041565,\n",
       "  0.8009752035140991,\n",
       "  0.820644736289978,\n",
       "  0.8179245591163635,\n",
       "  0.8281444311141968,\n",
       "  0.8389113545417786,\n",
       "  0.8228318691253662,\n",
       "  0.8861783146858215,\n",
       "  0.8435851335525513,\n",
       "  0.8259570598602295,\n",
       "  0.8249205350875854,\n",
       "  0.8642665147781372,\n",
       "  0.8297582864761353,\n",
       "  0.8302943110466003,\n",
       "  0.8471970558166504,\n",
       "  0.8239543437957764,\n",
       "  0.8352449536323547,\n",
       "  0.8597587943077087,\n",
       "  0.8414099216461182,\n",
       "  0.8501336574554443,\n",
       "  0.8111207485198975,\n",
       "  0.809548020362854,\n",
       "  0.7955343723297119,\n",
       "  0.839895486831665,\n",
       "  0.8296228051185608,\n",
       "  0.8226183652877808,\n",
       "  0.8473460674285889,\n",
       "  0.8357159495353699,\n",
       "  0.8389082551002502,\n",
       "  0.8157378435134888,\n",
       "  0.8390536308288574,\n",
       "  0.8379846811294556,\n",
       "  0.8335039019584656,\n",
       "  0.88387531042099,\n",
       "  0.8183571100234985,\n",
       "  0.833242654800415,\n",
       "  0.8398615121841431,\n",
       "  0.8483439683914185,\n",
       "  0.821549117565155,\n",
       "  0.839336633682251,\n",
       "  0.8296061754226685,\n",
       "  0.8368171453475952,\n",
       "  0.8722678422927856,\n",
       "  0.7895197868347168,\n",
       "  0.8115044832229614,\n",
       "  0.8214223384857178,\n",
       "  0.8180884122848511,\n",
       "  0.8407176733016968,\n",
       "  0.8026009798049927,\n",
       "  0.8375394344329834,\n",
       "  0.8248789310455322],\n",
       " 'recall': [0.8045754432678223,\n",
       "  0.8786805868148804,\n",
       "  0.820130467414856,\n",
       "  0.8383949398994446,\n",
       "  0.8308779001235962,\n",
       "  0.8350590467453003,\n",
       "  0.8131503462791443,\n",
       "  0.8165603280067444,\n",
       "  0.7587524652481079,\n",
       "  0.8408452868461609,\n",
       "  0.8170244097709656,\n",
       "  0.812339186668396,\n",
       "  0.7898482084274292,\n",
       "  0.7911420464515686,\n",
       "  0.8114722967147827,\n",
       "  0.8327550888061523,\n",
       "  0.8285645246505737,\n",
       "  0.8212131857872009,\n",
       "  0.8696234226226807,\n",
       "  0.8088141083717346,\n",
       "  0.8177517652511597,\n",
       "  0.7438797354698181,\n",
       "  0.839272141456604,\n",
       "  0.8340526223182678,\n",
       "  0.8281688690185547,\n",
       "  0.8311454057693481,\n",
       "  0.7884695529937744,\n",
       "  0.8724700212478638,\n",
       "  0.8185775876045227,\n",
       "  0.8610376715660095,\n",
       "  0.7994334101676941,\n",
       "  0.8256151676177979,\n",
       "  0.8464097380638123,\n",
       "  0.8344387412071228,\n",
       "  0.856629490852356,\n",
       "  0.826358437538147,\n",
       "  0.8289598226547241,\n",
       "  0.821232795715332,\n",
       "  0.7415858507156372,\n",
       "  0.8240038752555847,\n",
       "  0.8358538150787354,\n",
       "  0.7825817465782166,\n",
       "  0.8480867147445679,\n",
       "  0.8312170505523682,\n",
       "  0.8034939169883728,\n",
       "  0.8270306587219238,\n",
       "  0.8198505640029907,\n",
       "  0.8267157077789307,\n",
       "  0.8036707043647766,\n",
       "  0.8675099015235901,\n",
       "  0.862189769744873,\n",
       "  0.8025936484336853,\n",
       "  0.7985574007034302,\n",
       "  0.8003042340278625,\n",
       "  0.8274645805358887,\n",
       "  0.8150047659873962,\n",
       "  0.8202589750289917,\n",
       "  0.8645181059837341,\n",
       "  0.8138422966003418,\n",
       "  0.8150984048843384,\n",
       "  0.8108000755310059,\n",
       "  0.8489927053451538,\n",
       "  0.8473005890846252,\n",
       "  0.8399626016616821,\n",
       "  0.8317136764526367,\n",
       "  0.8114075660705566,\n",
       "  0.8061138391494751,\n",
       "  0.8272011280059814,\n",
       "  0.8123843669891357,\n",
       "  0.8239337801933289,\n",
       "  0.8245853185653687,\n",
       "  0.8257552981376648,\n",
       "  0.8151000142097473,\n",
       "  0.8172473907470703,\n",
       "  0.8058428764343262,\n",
       "  0.7591411471366882,\n",
       "  0.8132297396659851,\n",
       "  0.8275927305221558,\n",
       "  0.834833025932312,\n",
       "  0.8506594896316528,\n",
       "  0.839993417263031,\n",
       "  0.8246147036552429,\n",
       "  0.7996059060096741,\n",
       "  0.8399744629859924,\n",
       "  0.8335357308387756,\n",
       "  0.8125340938568115,\n",
       "  0.8289734721183777,\n",
       "  0.8205960392951965,\n",
       "  0.8283199071884155,\n",
       "  0.8244073390960693,\n",
       "  0.836226224899292,\n",
       "  0.828723669052124,\n",
       "  0.8539351224899292,\n",
       "  0.8349912166595459,\n",
       "  0.8075562715530396,\n",
       "  0.7784695029258728,\n",
       "  0.8044865131378174,\n",
       "  0.8156511187553406,\n",
       "  0.8105757236480713,\n",
       "  0.8216532468795776,\n",
       "  0.8063960075378418],\n",
       " 'f1': [0.8168516159057617,\n",
       "  0.8731427788734436,\n",
       "  0.8216854333877563,\n",
       "  0.8345509171485901,\n",
       "  0.8298977017402649,\n",
       "  0.8289857506752014,\n",
       "  0.8287657499313354,\n",
       "  0.832334041595459,\n",
       "  0.8060752749443054,\n",
       "  0.841147780418396,\n",
       "  0.822492778301239,\n",
       "  0.8168220520019531,\n",
       "  0.8160985112190247,\n",
       "  0.8027486205101013,\n",
       "  0.815955400466919,\n",
       "  0.8476629257202148,\n",
       "  0.8319344520568848,\n",
       "  0.8250896334648132,\n",
       "  0.852030873298645,\n",
       "  0.8002468943595886,\n",
       "  0.8167131543159485,\n",
       "  0.8001735210418701,\n",
       "  0.8363637924194336,\n",
       "  0.8358232975006104,\n",
       "  0.8249598145484924,\n",
       "  0.8403061628341675,\n",
       "  0.8108277916908264,\n",
       "  0.8722240924835205,\n",
       "  0.8371472358703613,\n",
       "  0.8582345843315125,\n",
       "  0.8058152794837952,\n",
       "  0.8345511555671692,\n",
       "  0.841921865940094,\n",
       "  0.825804591178894,\n",
       "  0.8601559400558472,\n",
       "  0.8269959688186646,\n",
       "  0.8259627819061279,\n",
       "  0.8494247794151306,\n",
       "  0.7944902777671814,\n",
       "  0.8411526083946228,\n",
       "  0.8461024165153503,\n",
       "  0.8011060357093811,\n",
       "  0.84361732006073,\n",
       "  0.8339385986328125,\n",
       "  0.8094613552093506,\n",
       "  0.8206148743629456,\n",
       "  0.8222955465316772,\n",
       "  0.8272135853767395,\n",
       "  0.8124570250511169,\n",
       "  0.8616679310798645,\n",
       "  0.8684964776039124,\n",
       "  0.8017836213111877,\n",
       "  0.8094503879547119,\n",
       "  0.8090184330940247,\n",
       "  0.8278043866157532,\n",
       "  0.8267853260040283,\n",
       "  0.8215433955192566,\n",
       "  0.8752142190933228,\n",
       "  0.8284468054771423,\n",
       "  0.8204918503761292,\n",
       "  0.8177993893623352,\n",
       "  0.8565614819526672,\n",
       "  0.8384376168251038,\n",
       "  0.835100531578064,\n",
       "  0.8393839597702026,\n",
       "  0.817632794380188,\n",
       "  0.8204208612442017,\n",
       "  0.8431657552719116,\n",
       "  0.8266424536705017,\n",
       "  0.8368286490440369,\n",
       "  0.8177976012229919,\n",
       "  0.8175713419914246,\n",
       "  0.8051983714103699,\n",
       "  0.828416645526886,\n",
       "  0.8175599575042725,\n",
       "  0.7896060943603516,\n",
       "  0.8299374580383301,\n",
       "  0.831634521484375,\n",
       "  0.8368656635284424,\n",
       "  0.8328327536582947,\n",
       "  0.8395232558250427,\n",
       "  0.8312459588050842,\n",
       "  0.8162031173706055,\n",
       "  0.8613659143447876,\n",
       "  0.8258767127990723,\n",
       "  0.8227580785751343,\n",
       "  0.8343819975852966,\n",
       "  0.8342393040657043,\n",
       "  0.824920654296875,\n",
       "  0.8318049907684326,\n",
       "  0.8329030871391296,\n",
       "  0.8327507376670837,\n",
       "  0.8630041480064392,\n",
       "  0.8116191029548645,\n",
       "  0.8095255494117737,\n",
       "  0.7993692755699158,\n",
       "  0.8112304210662842,\n",
       "  0.8279947638511658,\n",
       "  0.8065686225891113,\n",
       "  0.8295202851295471,\n",
       "  0.8155327439308167]}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_evaluated_with_bertscore = evaluate_with_bertscore(\"RandomSummarizer\")\n",
    "random_evaluated_with_bertscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_precision': 0.83, 'avg_recall': 0.83, 'avg_f1': 0.83}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduction_evaluated_with_bertscore = evaluate_with_bertscore(\"ReductionSummarizer\")\n",
    "reduction_evaluated_with_bertscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_precision': 0.85,\n",
       " 'avg_recall': 0.85,\n",
       " 'avg_f1': 0.85,\n",
       " 'precision': [0.8469557762145996,\n",
       "  0.8510720133781433,\n",
       "  0.850729763507843,\n",
       "  0.8461459875106812,\n",
       "  0.8437168002128601,\n",
       "  0.8457304239273071,\n",
       "  0.8517477512359619,\n",
       "  0.850279688835144,\n",
       "  0.8569711446762085,\n",
       "  0.8368847966194153,\n",
       "  0.8653173446655273,\n",
       "  0.8478686809539795,\n",
       "  0.8484551906585693,\n",
       "  0.8208937644958496,\n",
       "  0.8523094058036804,\n",
       "  0.8439438343048096,\n",
       "  0.8400304317474365,\n",
       "  0.8483629822731018,\n",
       "  0.8142700791358948,\n",
       "  0.8523275256156921,\n",
       "  0.8453500866889954,\n",
       "  0.8429064750671387,\n",
       "  0.8494917750358582,\n",
       "  0.8372006416320801,\n",
       "  0.8488714694976807,\n",
       "  0.8541162014007568,\n",
       "  0.8379871845245361,\n",
       "  0.8442880511283875,\n",
       "  0.851631224155426,\n",
       "  0.8522617816925049,\n",
       "  0.8523801565170288,\n",
       "  0.8470505475997925,\n",
       "  0.8666777610778809,\n",
       "  0.8264190554618835,\n",
       "  0.8404381275177002,\n",
       "  0.8543692231178284,\n",
       "  0.8535748720169067,\n",
       "  0.8699327707290649,\n",
       "  0.8622990846633911,\n",
       "  0.8499889373779297,\n",
       "  0.85775226354599,\n",
       "  0.8326998353004456,\n",
       "  0.857637345790863,\n",
       "  0.8555981516838074,\n",
       "  0.8301084041595459,\n",
       "  0.8235864043235779,\n",
       "  0.8533546328544617,\n",
       "  0.8521424531936646,\n",
       "  0.8457186818122864,\n",
       "  0.8505609631538391,\n",
       "  0.8492921590805054,\n",
       "  0.8480836153030396,\n",
       "  0.8434593677520752,\n",
       "  0.8499385118484497,\n",
       "  0.8180633783340454,\n",
       "  0.8430774807929993,\n",
       "  0.8413782715797424,\n",
       "  0.8964356184005737,\n",
       "  0.8415059447288513,\n",
       "  0.8316766619682312,\n",
       "  0.8595371842384338,\n",
       "  0.858627438545227,\n",
       "  0.8532450795173645,\n",
       "  0.863142192363739,\n",
       "  0.8686630725860596,\n",
       "  0.8273227214813232,\n",
       "  0.8424770832061768,\n",
       "  0.8687137961387634,\n",
       "  0.8561710119247437,\n",
       "  0.8569568991661072,\n",
       "  0.831547737121582,\n",
       "  0.7996799945831299,\n",
       "  0.8332688212394714,\n",
       "  0.8382017016410828,\n",
       "  0.8593713045120239,\n",
       "  0.8580214381217957,\n",
       "  0.8676654696464539,\n",
       "  0.8287698030471802,\n",
       "  0.8598909378051758,\n",
       "  0.836263120174408,\n",
       "  0.8569571375846863,\n",
       "  0.8501476645469666,\n",
       "  0.8547490835189819,\n",
       "  0.8692281246185303,\n",
       "  0.8235443234443665,\n",
       "  0.8593076467514038,\n",
       "  0.8422576785087585,\n",
       "  0.8517045378684998,\n",
       "  0.8109652400016785,\n",
       "  0.807153046131134,\n",
       "  0.8118384480476379,\n",
       "  0.848699688911438,\n",
       "  0.8584808707237244,\n",
       "  0.8225060701370239,\n",
       "  0.8442074656486511,\n",
       "  0.8329593539237976,\n",
       "  0.8191221952438354,\n",
       "  0.8355726003646851,\n",
       "  0.847301185131073,\n",
       "  0.8233907222747803,\n",
       "  0.836396336555481],\n",
       " 'recall': [0.8553505539894104,\n",
       "  0.8414140343666077,\n",
       "  0.8685891628265381,\n",
       "  0.8533125519752502,\n",
       "  0.8457648158073425,\n",
       "  0.8487128019332886,\n",
       "  0.8532300591468811,\n",
       "  0.8609456419944763,\n",
       "  0.866989016532898,\n",
       "  0.8386469483375549,\n",
       "  0.8691009283065796,\n",
       "  0.8455585837364197,\n",
       "  0.8503072261810303,\n",
       "  0.8229234218597412,\n",
       "  0.8604896068572998,\n",
       "  0.8457185626029968,\n",
       "  0.8390781283378601,\n",
       "  0.8419137597084045,\n",
       "  0.8476340770721436,\n",
       "  0.843448281288147,\n",
       "  0.8468529582023621,\n",
       "  0.8462854027748108,\n",
       "  0.850435197353363,\n",
       "  0.8428740501403809,\n",
       "  0.848209023475647,\n",
       "  0.8476044535636902,\n",
       "  0.848748505115509,\n",
       "  0.8575058579444885,\n",
       "  0.8513944745063782,\n",
       "  0.8626942038536072,\n",
       "  0.8554434180259705,\n",
       "  0.854041576385498,\n",
       "  0.8672887086868286,\n",
       "  0.818918764591217,\n",
       "  0.8437191247940063,\n",
       "  0.8533741235733032,\n",
       "  0.8500249981880188,\n",
       "  0.8652083277702332,\n",
       "  0.8584601879119873,\n",
       "  0.841805636882782,\n",
       "  0.8606187701225281,\n",
       "  0.833338737487793,\n",
       "  0.8625518679618835,\n",
       "  0.8640540838241577,\n",
       "  0.8326313495635986,\n",
       "  0.8439227342605591,\n",
       "  0.8561767935752869,\n",
       "  0.8488922119140625,\n",
       "  0.8500944375991821,\n",
       "  0.8589379787445068,\n",
       "  0.8429245352745056,\n",
       "  0.8411596417427063,\n",
       "  0.8440799713134766,\n",
       "  0.8591411113739014,\n",
       "  0.8186001777648926,\n",
       "  0.8481298089027405,\n",
       "  0.8357276916503906,\n",
       "  0.8544966578483582,\n",
       "  0.845659077167511,\n",
       "  0.8304875493049622,\n",
       "  0.8542822599411011,\n",
       "  0.868750274181366,\n",
       "  0.8633012175559998,\n",
       "  0.8865429162979126,\n",
       "  0.8758438229560852,\n",
       "  0.8307640552520752,\n",
       "  0.8430073857307434,\n",
       "  0.8635914325714111,\n",
       "  0.8661348223686218,\n",
       "  0.8451299667358398,\n",
       "  0.8479358553886414,\n",
       "  0.8223458528518677,\n",
       "  0.8509664535522461,\n",
       "  0.8476702570915222,\n",
       "  0.8529263138771057,\n",
       "  0.8536421656608582,\n",
       "  0.8736995458602905,\n",
       "  0.8465657830238342,\n",
       "  0.866737961769104,\n",
       "  0.855964183807373,\n",
       "  0.8567295670509338,\n",
       "  0.8520575761795044,\n",
       "  0.8538243174552917,\n",
       "  0.873967707157135,\n",
       "  0.8427541851997375,\n",
       "  0.8610311150550842,\n",
       "  0.8458313345909119,\n",
       "  0.8533544540405273,\n",
       "  0.8207230567932129,\n",
       "  0.8224154114723206,\n",
       "  0.8290611505508423,\n",
       "  0.8426268100738525,\n",
       "  0.8610877990722656,\n",
       "  0.8536176085472107,\n",
       "  0.8315802812576294,\n",
       "  0.8263742327690125,\n",
       "  0.831037163734436,\n",
       "  0.833466649055481,\n",
       "  0.8498048782348633,\n",
       "  0.8222503662109375,\n",
       "  0.8303875923156738],\n",
       " 'f1': [0.8511325120925903,\n",
       "  0.8462154269218445,\n",
       "  0.8595666885375977,\n",
       "  0.8497141599655151,\n",
       "  0.844739556312561,\n",
       "  0.8472189903259277,\n",
       "  0.8524882793426514,\n",
       "  0.8555794358253479,\n",
       "  0.8619509935379028,\n",
       "  0.8377649784088135,\n",
       "  0.867205023765564,\n",
       "  0.846712052822113,\n",
       "  0.8493801951408386,\n",
       "  0.8219073414802551,\n",
       "  0.8563799262046814,\n",
       "  0.8448302745819092,\n",
       "  0.839553952217102,\n",
       "  0.8451260924339294,\n",
       "  0.8306171894073486,\n",
       "  0.8478646874427795,\n",
       "  0.8461008667945862,\n",
       "  0.8445925712585449,\n",
       "  0.8499631881713867,\n",
       "  0.8400277495384216,\n",
       "  0.8485400676727295,\n",
       "  0.8508478403091431,\n",
       "  0.8433335423469543,\n",
       "  0.8508456349372864,\n",
       "  0.8515128493309021,\n",
       "  0.8574462532997131,\n",
       "  0.8539090752601624,\n",
       "  0.8505316972732544,\n",
       "  0.8669831156730652,\n",
       "  0.8226518630981445,\n",
       "  0.8420754671096802,\n",
       "  0.8538713455200195,\n",
       "  0.8517961502075195,\n",
       "  0.8675641417503357,\n",
       "  0.8603754043579102,\n",
       "  0.8458775281906128,\n",
       "  0.859183132648468,\n",
       "  0.8330191969871521,\n",
       "  0.8600875735282898,\n",
       "  0.8598053455352783,\n",
       "  0.8313679695129395,\n",
       "  0.8336305618286133,\n",
       "  0.8547633290290833,\n",
       "  0.8505142331123352,\n",
       "  0.8479008674621582,\n",
       "  0.8547289967536926,\n",
       "  0.846096396446228,\n",
       "  0.844607412815094,\n",
       "  0.8437695503234863,\n",
       "  0.854515016078949,\n",
       "  0.8183317184448242,\n",
       "  0.8455960750579834,\n",
       "  0.8385435342788696,\n",
       "  0.8749638795852661,\n",
       "  0.8435773849487305,\n",
       "  0.8310816884040833,\n",
       "  0.8569016456604004,\n",
       "  0.8636592030525208,\n",
       "  0.8582436442375183,\n",
       "  0.8746861219406128,\n",
       "  0.8722386956214905,\n",
       "  0.8290398120880127,\n",
       "  0.8427421450614929,\n",
       "  0.8661450147628784,\n",
       "  0.8611240983009338,\n",
       "  0.8510022759437561,\n",
       "  0.8396618366241455,\n",
       "  0.8108545541763306,\n",
       "  0.8420246243476868,\n",
       "  0.8429093956947327,\n",
       "  0.8561366200447083,\n",
       "  0.855826199054718,\n",
       "  0.8706721067428589,\n",
       "  0.8375732898712158,\n",
       "  0.8633008599281311,\n",
       "  0.8459988832473755,\n",
       "  0.8568433523178101,\n",
       "  0.8511015772819519,\n",
       "  0.8542864322662354,\n",
       "  0.8715915083885193,\n",
       "  0.8330385088920593,\n",
       "  0.8601685166358948,\n",
       "  0.8440407514572144,\n",
       "  0.8525286912918091,\n",
       "  0.8158149719238281,\n",
       "  0.8147127032279968,\n",
       "  0.8203593492507935,\n",
       "  0.8456523418426514,\n",
       "  0.859782338142395,\n",
       "  0.8377730846405029,\n",
       "  0.8378463387489319,\n",
       "  0.8296537399291992,\n",
       "  0.8250366449356079,\n",
       "  0.834518313407898,\n",
       "  0.8485511541366577,\n",
       "  0.8228201270103455,\n",
       "  0.833381175994873]}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_evaluated_with_bertscore = evaluate_with_bertscore(\"bart\")\n",
    "bart_evaluated_with_bertscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_precision': 0.82, 'avg_recall': 0.82, 'avg_f1': 0.82}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pegasus_evaluated_with_bertscore = evaluate_with_bertscore(\"pegasus\")\n",
    "pegasus_evaluated_with_bertscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_precision': 0.83, 'avg_recall': 0.82, 'avg_f1': 0.82}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_evaluated_with_bertscore = evaluate_with_bertscore(\"t5\")\n",
    "t5_evaluated_with_bertscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams[\"xtick.major.size\"] = 0\n",
    "rcParams[\"xtick.minor.size\"] = 0\n",
    "rcParams[\"ytick.major.size\"] = 0\n",
    "rcParams[\"ytick.minor.size\"] = 0\n",
    "\n",
    "rcParams[\"axes.labelsize\"] = \"large\"\n",
    "rcParams[\"axes.axisbelow\"] = True\n",
    "rcParams[\"axes.grid\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(bart_evaluated_with_bertscore[\"f1\"], bins=20)\n",
    "plt.xlabel(\"score\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_abstractively_thesis(tokenizer, summarizer, law, max_token_number):\n",
    "  chunk = []\n",
    "  summarized = []\n",
    "\n",
    "  for para in law:\n",
    "      chunk_length = len(tokenizer.tokenize(' '.join(chunk)))\n",
    "      para_length = len(tokenizer.tokenize(para))\n",
    "      chunk_and_para_length = chunk_length + para_length\n",
    "\n",
    "      if para_length > max_token_number:\n",
    "        para = summarize_long_para(para, tokenizer, summarizer, max_token_number)\n",
    "        summarized_long_para_length = len(tokenizer.tokenize(para))\n",
    "        chunk_and_para_length = chunk_length + summarized_long_para_length\n",
    "\n",
    "      if chunk_and_para_length > max_token_number: \n",
    "        summarized_chunk_text = summarize_para(' '.join(chunk), summarizer)\n",
    "        summarized.append(summarized_chunk_text)\n",
    "        print(chunk_length)\n",
    "        chunk.clear()\n",
    "\n",
    "      chunk.append(para)\n",
    "\n",
    "  if chunk:\n",
    "    chunk_text = ' '.join(chunk)\n",
    "    chunk_length = len(tokenizer.tokenize(chunk_text))\n",
    "    summarized_remaining_chunk_text = summarize_para(chunk_text, summarizer)\n",
    "    summarized.append(summarized_remaining_chunk_text)\n",
    "\n",
    "    print(\"summarized remaining chunk with length: \", chunk_length)\n",
    "    chunk.clear()\n",
    "\n",
    "  summarized_law_text = ' '.join(summarized)\n",
    "\n",
    "  return summarized_law_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesis = \"\"\"\n",
    "Despite the abundance of solutions for text summarization, including extractive and abstractive methods, supervised and unsupervised techniques, etc., automatic text summarization remains one of the hardest tasks for a computer. Currently, no official model achieves high data-independent accuracy, with state-of-the-art models using deep learning typically reaching around 50% accuracy. This complexity arises from several key challenges:\n",
    "Fusion of reading comprehension and text generation: Effectively summarizing text requires a comprehensive understanding of the content coupled with the ability to generate concise summaries. \n",
    "Representation of sentence significance and terms: To create a coherent and informative summary, it is crucial to capture the significance of each sentence and the terms within it as well as understand the inter-word and inter-sentence dependencies.\n",
    "Limited focus on non-extractive methods: While human-generated summaries are typically abstractive in nature, most research in text summarization has predominantly focused on extractive methods. My prior research project also considered extractive algorithms. Extractive summarization refers to the process of choosing and reorganizing already existing sentences, whereas abstractive summarization focuses on creating fresh sentences that capture the main points of the text. Due to this bias, the performance of deep learning models, which have shown promise in other domains, may not be fully explored in the context of automatic text summarization.\n",
    "Domain-specific language challenges: Applying domain-general summarization techniques to domain-specific documents may not yield optimal results due to the unique characteristics of domain-specific language. Legal texts that were chosen for this research, often have specific terminology, complex sentence structures, and precise formulations, which require specialized approaches to capture their essence accurately. Although some legal domain-specific methods have been developed for case document summarization, there is still a lack of comprehensive analyses comparing the performance of different summarization models on legal documents.\n",
    "Limited exploration of transformer-based models: The field of document summarization has seen rapid advancements in recent years, largely driven by transformer-based models. However, to my knowledge, there has been limited exploration of how these state-of-the-art transformer-based summarization models perform on legal documents. Furthermore, no research has specifically applied summarization models to laws, highlighting a significant research gap in this area.\n",
    "Lack of large legal summarization datasets: Acquiring datasets for legal summarization tasks is often expensive and a challenge on its own. Hiring legal domain experts to write summaries can be cost-prohibitive, resulting in small or non-existent datasets. This limitation makes it difficult, if not impossible, to utilize supervised models effectively. Additionally, legal documents and their summaries tend to be much longer than other document types, posing further challenges in summarization. \n",
    "Input capacity limitations: Using existing abstractive summarizers on legal documents presents a key challenge due to their often lower input capacity compared to the length of legal texts. Legal documents can be extensive, exceeding the limitations of many summarization models. \n",
    "For this research, I curated a dataset consisting of only 102 law-summary pairs from publicly available Ukrainian legislation to mimic the real-world scenario of summarizing long domain-specific documents. The dataset I used is notably smaller than the well-known CNN/Daily Mail benchmark, yet it comprises significantly longer documents and summaries.\n",
    "I aim to contribute to the field of automatic text summarization, particularly in the context of laws and codes by exploring the effectiveness of state-of-the-art abstractive summarization models in this specific domain, addressing the challenges that arise.\n",
    "The primary contributions of this work are: \n",
    "An analysis of the role of zero-shot learning in the summarization of domain-specific laws. \n",
    "Introduction of a new summarization dataset that can be utilized by the research community. \n",
    "Abstractive summarization is an advanced technique in natural language processing that aims to generate summaries that are more human-like by comprehending and interpreting the crucial elements of a text. Unlike extractive summarization, which selects and rearranges existing sentences, abstractive summarization involves creating new sentences and inferring the overall meaning of the text. This makes it more intricate and computationally intensive.\n",
    "Summaries are frequently produced using abstractive summarization. Sequence-to-sequence models with attention mechanisms, copy mechanisms, content selection, pointer-generator methods, and reinforcement learning are common techniques used in abstractive summarization algorithms.\n",
    "These techniques have shown promising performance in high-resource large summarization datasets, typically consisting of short documents.\n",
    "However, real-world applications often face limitations in terms of resources. Obtaining domain-specific datasets, especially for lengthy documents containing thousands of tokens, which is also a more practical and relevant problem on its own, is a costly and time-consuming process. \n",
    "In contrast to the news summarization datasets like CNN/Daily Mail and XSUM, such texts as laws are significantly longer, adhere to specific structures, and incorporate more technical terms and complex concepts. \n",
    "As a result, researchers and industry professionals are exploring methods that require less annotated data. One such approach gaining attention is zero-shot learning, which leverages pre-trained models that have been trained on extensive amounts of data. This approach shows promise in eliminating the data annotation requirements in summarization tasks, offering a more practical and relevant solution.\n",
    "Zero-shot learning is a powerful technique that has found applications in various fields, including computer vision and natural language processing (NLP). It refers to a scenario where the objective is to train a classifier using a labelled dataset enabling it to classify objects from unseen classes. \n",
    "One of the applications of zero-shot learning in NLP is text summarization. Zero-shot text summarization is often a powerful and underrated task. Off-the-shelf models can predict the content of a given text without explicit training on specific classes. \n",
    "Zero-shot summarization involves the use of pre-trained language models, making it a form of transfer learning. As stated in the literature, “A major assumption in many machine learning and data mining algorithms is that the training and future data must have the same features and the same probability distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data labelling efforts.”\n",
    "The approach is particularly valuable when labelled data is scarce. In recent years, transfer learning from unsupervised models has enabled surpassing benchmarks on downstream supervised learning tasks. OpenAI's preprint on the GPT-3 model, boasting 175 billion parameters, demonstrates its competitive performance on downstream tasks with significantly less task-specific data compared to smaller models. Nevertheless, such large models remain impractical for real-world use.\n",
    "Smaller models like BERT have been shown to encode an immense amount of information in their weights, making them suitable for downstream tasks that benefit from this latent information. These models can generate high-quality summaries without requiring a large amount of task-specific annotated data. The Big Science Research Workgroup , recently released their T0pp model, specifically trained for zero-shot multitask learning, which has outperformed models six times larger on the BIG-bench  benchmark and even outperformed GPT-3 (which is 16 times larger) on various NLP benchmarks.\n",
    "Zero summarization is an emergent feature observed in large language models, typically appearing in models with 100 million parameters or more. The effectiveness of a model in zero-shot tasks tends to increase with model size, implying that larger models (with more trainable parameters or layers) generally perform better at this task.\n",
    "In this research, zero-shot models trained for text summarization on news articles and internet data are employed to evaluate and compare the performance of advanced neural network text summarization models against simple baselines, specifically in the context of laws. State-of-the-art NLP models are utilized for text summarization in the legal domain, where large, annotated training sets are not readily available. Although transfer learning may not result in optimal performance, it can be an effective approach when limited data is available for the target task. However, it is important to note that these models are primarily designed to summarize news articles and internet content effectively, which may present challenges when applied to the specific patterns and complexities inherent in laws. While fine-tuning a pre-trained model on laws and their summaries may be ideal, it may still be possible to generate summaries that capture the main ideas and important information contained within the documents.\n",
    "The field of natural language processing (NLP) has undergone a significant revolution with the introduction of Transformers.\n",
    "This journey from recurrent neural networks (RNNs) and convolutional neural networks (CNNs) to Transformers marks a major milestone that took more than three decades to accomplish. Since then, dedicated research teams pushed the boundaries even further, transitioning from task-specific Transformers to multi-task models requiring minimal or no fine-tuning. Google's research team further standardized Transformers, by introducing a common input format that incorporates a prefix indicating the specific NLP problem to be solved, which in itself is a remarkable achievement.\n",
    "The groundbreaking paper by Google, \"Attention is All You Need\", introduced Transformer models and demonstrated that attention mechanisms can effectively replace sequence models like LSTM leading to improved performance. Previous sequence processing models, such as RNNs, faced challenges in capturing long-term dependencies and suffered from computational inefficiency. Transformers address these issues by employing attention mechanisms instead of recurrence, enabling them to capture long-range dependencies and learn word relationships in a sentence while parallelizing computations. Consequently, Transformers can be trained on unprecedentedly large datasets that were previously unfeasible, offering a more accurate understanding of sentence meaning compared to traditional methods.\n",
    "The current state-of-the-art in NLP comprises a diverse range of Transformer-based models that have significantly advanced the field. Prominent models like BART, T5, and Pegasus exemplify the cutting-edge capabilities of Transformers, delivering impressive results and inspiring further research and development in the field.\n",
    "To fine-tune Transformer models for specific tasks, such as summarization, the Transformer architecture can be employed alongside different summarization datasets. These datasets typically include two essential features: the article itself and the highlights, which capture the key elements of the text and prove valuable for summarization.\n",
    "Working extensively with real-life data is of great significance as it reveals the limitations that Transformers and other NLP models encounter when processing texts such as laws. It is crucial to present not only the successful samples but to acknowledge, that despite the transformative impact of Transformers, they have not solved all the NLP challenges we face. aptly titled their approach to T5 as \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\", recognizing the ongoing exploration of the boundaries and potential of transfer learning in NLP.\n",
    "Most abstractive summarization models have a limited input token capacity, which is usually shorter than the length of laws and codes. Ongoing research aims to develop models that can process larger sequences while minimizing resource consumption. For example, some models are highly optimized to handle a high number of tokens:\n",
    "Big Bird attempts to address long document summarization by replacing the full self-attention of Transformer models with a sparse attention mechanism that can scale to longer inputs. However, this approach may struggle to scale to documents of arbitrary length and does not fully utilize the underlying structure of documents.\n",
    "Longformer and Reformer are models that have been developed to efficiently process long input sequences for tasks including long document summarization. Longformer uses a combination of local and global attention to capture both local and long-range dependencies, while Reformer utilizes a reversible architecture and locality-sensitive hashing to reduce memory consumption and computational complexity.\n",
    "Despite their advancements in handling long document summarization, Longformer’s sliding window attention still has a quadratic complexity with respect to the window size, which limits its ability to handle very long sequences. Additionally, Longformer requires pre-training on a large corpus of long documents, which is computationally expensive and may not be feasible for some domains. Reformer’s locality-sensitive hashing technique introduces the possibility of errors in the attention computation as it relies on random projections to group similar tokens. Furthermore, Reformer may face limitations in capturing fine-grained dependencies across very long documents, as it attends to only a fixed number of tokens per bucket. Both models might encounter difficulties when processing documents of arbitrary length. \n",
    "The \"Divide and Conquer\" approach is widely regarded as the best practice. This involves chunking the data and setting the maximum length as a reference, with the possibility of iterative chunking until the desired summary length is achieved.\n",
    "Moreover, OpenAI recently published impressive work demonstrating how they combined reinforcement learning with human feedback to summarize entire books. This innovative technique showcases the potential of leveraging both automated learning algorithms and human expertise in generating high-quality summaries.\n",
    "To address the low resource setting, I use pre-trained language models. Unlike previous approaches that rely on data augmentation techniques to handle data scarcity in summarization, my method needs no synthetic data augmentation. Moreover, I study a significantly more resource-constrained scenario — a complex law dataset with only 102 available (document, summary) pairs; assumes access to 90,000 pairs, uses 370 pairs.\n",
    "I employ T5, Pegasus and BART models, which are recognized as state-of-the-art abstractive methods. These models are pre-trained on large datasets from related domains and can generate high-quality summaries without requiring additional training data. I access these models through the HuggingFace  library.\n",
    "T5, also known as The Text-to-Text Transfer Transformer, is a type of Transformer that can be trained on various tasks using a unified architecture. It was developed by Google AI and introduced in the paper “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer“. For this research, I use an mT5 model which is a fine-tuned pre-trained multilingual T5 on the XL-SUM dataset. It was finetuned on the 45 languages included in the XL-Sum dataset, which comprises 1 million professionally annotated article-summary pairs from BBC.\n",
    "Pegasus is a transformer-based model for text generation, also developed by Google Research. It was introduced in the paper \"PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization\", published in 2020. Pegasus is specifically designed for abstractive text summarization and was trained on the C4 (Colossal and Cleaned version of Common Crawl) web archive and HugeNews corpus of the articles, which consists of 1.5 million documents.\n",
    "BART, developed by Facebook, stands for Bidirectional Auto-Regressive Transformer. It combines a standard Seq2Seq bidirectional encoder (similar to BERT) with a left-to-right autoregressive decoder (similar to GPT) into a single Seq2Seq model. In other words, BART = BERT + GPT. The BART-based Transformer was trained on the CNN/DailyMail dataset, which includes over 300,000 unique news articles, for finetuning it to text summarization.\n",
    "Since the models were trained on short documents, they truncate documents longer than 512 or 1024 subwords which is much smaller than the number of subwords in law documents from my dataset.\n",
    "To overcome this limitation, I adopt a chunking-based approach. Instead of feeding the entire source document as input, I split long laws and codes into smaller chunks, each consisting of no more than 510 tokens. I choose 510 tokens to leave room for possible [CLS] and [SEP] tokens. The BART model can take up to 1024 tokens, but I wanted to cut it to less than 1024 to be on the safe side. 512 should be better and it's what the original BERT uses, so it shouldn't be bad.\n",
    "During the iteration, I do not break the sentences but collect them in chunks to preserve the context better. For each sentence, I count the number of tokens using the corresponding tokenizer for the model being used. If, at a certain iteration, adding the current sentence to the chunk would potentially exceed the 510 token limit, I summarize the chunk, append it to the final summary, clear the chunk, and start a new chunk with the current sentence.\n",
    "There even happened to be sentences with several times more than 510 tokens. \n",
    "In these cases, I split such a long sentence into such number of parts so that each part does not exceed the 510 limit. I then summarize each part and check if the number of tokens of the already previously collected chunk and these generated summaries in total exceeds the 510 limit. If it exceeds, I summarize the previously collected chunk, append it to the final summary, and then append the summarized halves to the final summary. If it does not exceed, I simply add the two generated summaries from the halves to the chunk and continue the iteration. Ultimately, all the generated summaries are concatenated into a single final summary.\n",
    "For the abstraction stage, I employ black-box pre-trained abstractive summarizers trained on related domains. Leveraging pre-trained language models and models trained on related domains helps improve generalization to unseen but related domains, such as laws.\n",
    "This chunking-based approach has significant potential for broader applications in summarizing long documents.\n",
    "In this research, I aligned 102 original laws and codes with their corresponding expert abstracts and automatically generated summaries. To obtain the data, I utilized an online version of Ukrainian legislation translated into English. The Ukrainian Parliament has made efforts to provide an English version of Ukrainian laws to promote understanding among non-Ukrainian speakers . However, out of the thousands of laws, only 173 have been translated into English. Additionally, the Parliament has provided abstracts for some laws to make them more accessible. Manual summarization of legal documents is a time-consuming task that requires extensive effort and strong legal knowledge. Consequently, only 102 laws out of the translated ones have corresponding abstracts. Thus, the automation of law document summarization would be highly beneficial.\n",
    "I utilized a requests-HTML Python library  to fetch, navigate and extract the required data from the pages. The process involved the following steps:\n",
    "Navigating to the List of Official Translations of Laws in English  and using the .page-link:not([title='current page']) CSS selector to obtain the remaining links from the pagination.\n",
    "Making queries using all the pagination links to obtain links to the law pages through the .valid selector.\n",
    "Scraping and saving the HTML content of each standardized law. I set a sleep time of 10 seconds for each law webpage to allow the JavaScript to load content dynamically. To identify links to the abstract pages, I used the [title='Abstract'] selector. If a link was found, I saved it and made a request to scrape its HTML content. If no link was found, it indicated that the law did not have an abstract, rendering it unsuitable for this study. \n",
    "Although some laws exhibited inconsistencies in their presentation and structure, deviating from the majority, I mitigated this issue by leveraging the HTML structure of law pages, which accompany references and amendments with the em HTML tag. In particular, I used the .rvts0 > .rvps8, .rvps2, .rvts0 .rvts15 selector to retrieve only meaningful information from standardized laws. This information included the title of the act, main content, and additional contextual details such as the author and date of entry into force.\n",
    "For abstract webpages, I used the .rvts0 [align='justify'], .rvts0 ul selector to retrieve their content. Regarding the actual content of the abstracts, I concatenated the list items of the unordered lists into single sentences. This approach ensures greater lexical correctness when evaluating both human summaries and auto-generated summaries.\n",
    "Measuring the performance of a model is often straightforward in many ML projects, where there is little ambiguity in determining the correctness of the model's results. The labels in the dataset are usually binary (True/False, Yes/No) or categorical, allowing for easy comparison between the model's output and the labels.\n",
    "However, evaluating text generation becomes more challenging. The summaries provided in the dataset represent only one way to summarize the text, but there are numerous possibilities for summarization. Even if the model's output doesn't match a label exactly, it can still produce a valid and useful summary.\n",
    "The most straightforward way to evaluate the quality of a summary is through human evaluation. Human evaluation is widely considered the gold standard in text summarization. Experts in automatic text summarization emphasize that domain expertise is a crucial factor contributing to the evaluation.\n",
    "The evaluation of summaries by humans has the obvious drawback of being a time-consuming process. To address this challenge, Lin introduced Recall-Oriented Understudy for Gisting Evaluation (ROUGE) which became the most popular summarization evaluation metric. ROUGE compares the candidate summary to a set of human-produced reference summaries by computing the co-occurrences of n-grams between the candidate and each reference. In this research, I report the most prevalent in the literature.\n",
    "A common practice is to calculate the average of all the ROUGE metrics mentioned above.\n",
    "One drawback of ROUGE is that it is designed to compare a candidate against a set of reference summaries. However, in practice, it is rare to have multiple summaries for a single text.\n",
    "While ROUGE performs well when averaged over a set of references, it struggles to distinguish between good and bad summaries when there is a single reference.\n",
    "ROUGE relies on n-gram statistics, resulting in a surface-level comparison that penalizes lexical and compositional diversity, even if the output is semantically analogous to the reference. Supervised summarization methods tend to perform better on ROUGE scores compared to unsupervised approaches because they tend to have the same writing style as the reference summary. Recent approaches have investigated the use of word embeddings to analyze the semantic similarity between two texts. \n",
    "In this research, ROUGE is considered a necessary but not sufficient condition, where high scores do not necessarily indicate strong summaries, but low scores serve as a red flag.\n",
    "Another automatic evaluation metric for text generation is BERTScore, which computes a similarity score for each token in the candidate sentence compared to each token in the reference sentence. BERTScore leverages the pre-trained contextual embeddings from BERT to automatically evaluate two texts by comparing the weighted cosine similarities of their embedded representations. Additionally, BERTScore computes precision, recall, and F1 measure metrics, as shown in \n",
    "In a previous research project, I established a baseline using random and extractive methods without employing ML techniques. Establishing a baseline is a critical step in any ML project as it allows us to quantify the progress achieved through the use of AI and determine whether investing in AI technology is worthwhile.\n",
    "During the experimentation, I conducted tests with various output length limits and subjectively determined that 7 sentences for extractive methods and a maximum of 256 tokens for abstractive methods yielded the best results. These settings also resulted in compression rates approximately equal to those of human summaries. All other settings were kept as default.\n",
    "It is important to note that these numbers are quite impressive for such a simple approach, particularly the ROUGE-1 and ROUGE-1 scores. To gain further context, we can refer to Pegasus Models , which displays scores of state-of-the-art models for different datasets.\n",
    "It is important to note that certain contextual embedding models, such as RoBERTa, tend to produce BERTScores within a narrow range. In the provided example, the scores range approximately between 0.81 and 0.87. Although this may impact readability, it does not affect the ranking capability of BERTScore. If desired, the scores can be adjusted using baseline rescaling to achieve a wider spread.\n",
    "In order to further enhance the research, several potential avenues for future experiments can be explored:\n",
    "Investigating New Architectures: Exploring new architectures designed to handle longer sequences, such as Longformer, Reformer, Big Bird, and Linformer, could provide valuable insights into their effectiveness for summarizing laws and codes.\n",
    "Expanding Automatic Evaluation Metrics: Incorporating additional automatic evaluation metrics can offer a more comprehensive assessment of the summarization models' performance. This can provide a broader perspective on the quality and effectiveness of the generated summaries.\n",
    "Dataset Split for Fine-tuning: Splitting the existing dataset of 102 law and summary pairs into training and test sets would enable the opportunity to perform fine-tuning on this limited dataset. This could potentially improve the model's performance by training it specifically on the available data.\n",
    "Scraping Ukrainian Laws and Summaries: Considering the availability of a larger number of laws and summaries in Ukrainian, scraping laws and their corresponding summaries in the Ukrainian language can be valuable. This would allow for the fine-tuning of multilingual models using the expanded dataset, potentially enhancing the summarization capabilities across different languages.\n",
    "This research project demonstrated the potential of pre-trained zero-shot text summarization models in summarizing text within a narrow domain. By leveraging large-trained models from transformers, it was possible to generate informative and concise summaries of laws and codes.\n",
    "The findings highlight the importance of exploring and monitoring the advancements in zero-shot summarization techniques. These models provide a valuable tool for summarizing text in various domains, including legal texts. By utilizing the knowledge and capabilities encoded in the pre-trained models, researchers and practitioners can efficiently generate summaries without the need for extensive domain-specific training data.\n",
    "State-of-the-art models such as T5, Pegasus, and BART demonstrated promising results, achieving competitive ROUGE scores. These results highlight the potential of advanced neural architectures for law summarization and signify the progress made in the field. \n",
    "Furthermore, the findings underscore the importance of using appropriate evaluation metrics to assess summarization models. While ROUGE scores provide valuable insights, the inclusion of additional metrics such as BERTScore offers a more comprehensive evaluation, considering factors like semantic similarity.\n",
    "Notably, the BART model emerged as the top performer for summarizing laws and codes. Its consistent and impressive performance surpassed that of the extractive and abstractive methods, generating accurate and meaningful summaries. The correlation observed between the ROUGE scores and BERTScore further validates its reliability and effectiveness.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "888\n",
      "890\n",
      "882\n",
      "890\n",
      "886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but your input_length is only 162. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=81)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "888\n",
      "summarized remaining chunk with length:  160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No official model achieves high data-independent accuracy, with state-of-the-art models using deep learning typically reaching around 50% accuracy. Effectively summarizing text requires a comprehensive understanding of the content coupled with the ability to generate concise summaries. Legal texts often have specific terminology, complex sentence structures, and precise formulations, which require specialized approaches. Researchers and industry professionals are exploring methods that require less annotated data. One such approach gaining attention is zero-shot learning, which leverages pre-trained models that have been trained on extensive amounts of data. This approach shows promise in eliminating the data annotation requirements in summarization tasks, offering a more practical and relevant solution. Transformers can be trained on unprecedentedly large datasets that were previously unfeasible. The current state-of-the-art in NLP comprises a diverse range of Transformer-based models that have significantly advanced the field. Ongoing research aims to develop models that can process larger sequences while minimizing resource consumption. I employ T5, Pegasus and BART models, which are recognized as state-of-the-art abstractive methods. T5 is a type of Transformer that can be trained on various tasks using a unified architecture. Pegasus is specifically designed for abstractive text summarization and was trained on the C4 (Colossal and Cleaned version of Common Crawl) web archive and HugeNews corpus of the articles. BART, developed by Facebook, stands for Bidirectional Auto-Regressive Transformer. The Ukrainian Parliament has made efforts to provide an English version of Ukrainian laws to promote understanding among non-Ukrainian speakers. Out of the thousands of laws, only 173 have been translated into English. Manual summarization of legal documents is a time-consuming task that requires extensive effort and strong legal knowledge. ROUGE performs well when averaged over a set of references. It struggles to distinguish between good and bad summaries when there is a single reference. BERTScore leverages the pre-trained contextual embeddings from BERT to automatically evaluate two texts. By leveraging large-trained models from transformers, it was possible to generate informative and concise summaries of laws and codes. State-of-the-art models such as T5, Pegasus, and BART demonstrated promising results. BART model emerged as the top performer for summarizing laws and codes. Its consistent and impressive performance surpassed that of the extractive and abstractive methods. The correlation observed between the ROUGE scores and BERTScore further validates its reliability and effectiveness.'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumarized_thesis = summarize_abstractively_thesis(bart_tokenizer, bart_summarizer, sent_tokenize(thesis), 894)\n",
    "sumarized_thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarized remaining chunk with length:  378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No official model achieves high data-independent accuracy, with state-of-the-art models using deep learning typically reaching around 50% accuracy. Legal texts often have specific terminology, complex sentence structures, and precise formulations. By leveraging large-trained models from transformers, it was possible to generate informative and concise summaries of laws and code.'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_abstract = summarize_abstractively_thesis(bart_tokenizer, bart_summarizer, sent_tokenize(sumarized_thesis), 1024)\n",
    "final_abstract"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d569fefa4ff5660fa9c2f1dfa382b3700d307fde3813ab35bb4c9cb0d80f7a0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
