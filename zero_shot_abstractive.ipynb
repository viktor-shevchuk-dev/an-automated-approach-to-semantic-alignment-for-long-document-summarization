{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from transformers import pipeline, AutoTokenizer, set_seed\n",
    "\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_seed(1)\n",
    "# gpt2_model_name = \"gpt2-xl\"\n",
    "# summarizer = pipeline(\"text-generation\", model=gpt2_model_name, max_new_tokens=64)\n",
    "# gpt2_query = \"summarize:\\n\"+ sample_text\n",
    "# pipe_out = summarizer(gpt2_query, clean_up_tokenization_spaces=True)\n",
    "# test3 = sent_tokenize(pipe_out[0][\"generated_text\"][len(gpt2_query) :])\n",
    "# test3\n",
    "\n",
    "\n",
    "def summarize_para(para, summarizer):\n",
    "    try:\n",
    "        if summarizer.task == 'text-generation':\n",
    "            gpt2_query = \"summarize:\\n\" + para\n",
    "            summarized_para = summarizer(\n",
    "                gpt2_query, clean_up_tokenization_spaces=True)[0]['generated_text'][len(gpt2_query):]\n",
    "        else:\n",
    "            summarized_para = summarizer(para, max_length=256)[\n",
    "                0][\"summary_text\"]\n",
    "\n",
    "        return summarized_para\n",
    "    except:\n",
    "        print(\"An error occurred, chunk text trying to summarize: \", para)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_long_para(para, para_length, tokenizer, summarizer, max_token_number):\n",
    "    para_list = para.split(\" \")\n",
    "    chunks_number = math.ceil(para_length / max_token_number)\n",
    "    chunkified = np.array_split(para_list, chunks_number)\n",
    "    summarized_chunk_list = []\n",
    "    print(\"mega long sentence, chunks_number is: \", chunks_number)\n",
    "\n",
    "    if chunks_number == 1:\n",
    "        print(para_list)\n",
    "\n",
    "    for chunk in chunkified:\n",
    "        chunk_text = ' '.join(chunk)\n",
    "        summarized_chunk = summarize_para(chunk_text, summarizer)\n",
    "        summarized_chunk_list.append(summarized_chunk)\n",
    "\n",
    "    summarized_long_para = ' '.join(summarized_chunk_list)\n",
    "    summarized_para_length = len(tokenizer.tokenize(summarized_long_para))\n",
    "\n",
    "    if summarized_para_length > max_token_number:\n",
    "        return summarize_long_para(summarized_long_para, summarized_para_length, tokenizer, summarizer, max_token_number)\n",
    "\n",
    "    return summarized_long_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_abstractively(tokenizer, summarizer, max_token_number, para_list):\n",
    "    chunk = []\n",
    "    summarized = []\n",
    "\n",
    "    for para in para_list:\n",
    "        chunk_length = len(tokenizer.tokenize(' '.join(chunk)))\n",
    "        para_length = len(tokenizer.tokenize(para))\n",
    "        chunk_and_para_length = chunk_length + para_length\n",
    "\n",
    "        if para_length > max_token_number:\n",
    "            para = summarize_long_para(\n",
    "                para, para_length, tokenizer, summarizer, max_token_number)\n",
    "            summarized_long_para_length = len(tokenizer.tokenize(para))\n",
    "            chunk_and_para_length = chunk_length + summarized_long_para_length\n",
    "\n",
    "        if chunk_and_para_length > max_token_number:\n",
    "            summarized_chunk_text = summarize_para(' '.join(chunk), summarizer)\n",
    "            summarized.append(summarized_chunk_text)\n",
    "            print(chunk_length)\n",
    "            chunk.clear()\n",
    "\n",
    "        chunk.append(para)\n",
    "\n",
    "    if chunk:\n",
    "        chunk_text = ' '.join(chunk)\n",
    "        chunk_length = len(tokenizer.tokenize(chunk_text))\n",
    "        summarized_remaining_chunk_text = summarize_para(\n",
    "            chunk_text, summarizer)\n",
    "        summarized.append(summarized_remaining_chunk_text)\n",
    "\n",
    "        print(\"summarized remaining chunk with length: \", chunk_length)\n",
    "        chunk.clear()\n",
    "\n",
    "    summarized_law_text = ' '.join(summarized)\n",
    "\n",
    "    return summarized_law_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_abstractive_summaries(model_name, max_token_number, max_new_tokens):\n",
    "    path = os.path\n",
    "    storage_directory = path.join(path.curdir, \"english_laws_with_abstracts\")\n",
    "\n",
    "    method_folder = path.join(\n",
    "        storage_directory, \"summarized\", \"zero_shot_abstractive\")\n",
    "\n",
    "    create_folder_if_not_exists(method_folder)\n",
    "\n",
    "    model_folder_mame = model_name.replace('/', '_').replace('-', '_')\n",
    "    summarized_method_folder = path.join(method_folder, model_folder_mame)\n",
    "\n",
    "    create_folder_if_not_exists(summarized_method_folder)\n",
    "\n",
    "    laws_folder = path.join(storage_directory, \"laws\")\n",
    "    laws_listed = os.listdir(laws_folder)\n",
    "\n",
    "    if model_name == 'gpt2-xl':\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        summarizer = pipeline(\"text-generation\", model=model_name,max_new_tokens=max_new_tokens, tokenizer=tokenizer)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=max_token_number)\n",
    "        summarizer = pipeline(\"summarization\", model=model_name, tokenizer=tokenizer)\n",
    "\n",
    "    for index, _ in enumerate(laws_listed):\n",
    "        start_index = get_next_index(summarized_method_folder, split_by_dot=True)\n",
    "\n",
    "        if int(start_index) > index:\n",
    "            continue\n",
    "\n",
    "        law_folder = path.join(laws_folder, str(index))\n",
    "        paras_listed = os.listdir(law_folder)\n",
    "        para_list = []\n",
    "\n",
    "        summarized_law_file = path.join(\n",
    "            summarized_method_folder, f\"{start_index}.txt\")\n",
    "\n",
    "        for index, _ in enumerate(paras_listed):\n",
    "            para_file = path.join(law_folder, f\"{index}.txt\")\n",
    "\n",
    "            with open(para_file, 'r') as file:\n",
    "                para_text = file.read()\n",
    "                para_list.append(para_text)\n",
    "\n",
    "        summarized_text = summarize_abstractively(tokenizer, summarizer, max_token_number - 2, para_list)\n",
    "\n",
    "        with open(summarized_law_file, 'w') as f:\n",
    "            f.write(summarized_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
